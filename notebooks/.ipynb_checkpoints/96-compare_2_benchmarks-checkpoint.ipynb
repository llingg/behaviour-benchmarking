{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       " $('div.input').hide();\n",
       " } else {\n",
       " $('div.input').show();\n",
       " }\n",
       " code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the code.\"></form>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Hide the code by default from the web page:\n",
    "# # (from: http://stackoverflow.com/questions/27934885/how-to-hide-code-from-cells-in-ipython-notebook-visualized-with-nbviewer#28073228)\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the code.\"></form>\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import contracts\n",
    "contracts.disable_all()\n",
    "\n",
    "import geometry as geo\n",
    "import math \n",
    "import numpy as np\n",
    "from os import path, listdir\n",
    "from scipy import stats\n",
    "import yaml\n",
    "import datetime\n",
    "import pprint\n",
    "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
    "import matplotlib.pyplot as plt\n",
    "from math import ceil\n",
    "import collections\n",
    "\n",
    "import duckietown_world as dw\n",
    "from duckietown_world.svg_drawing.ipython_utils import ipython_draw_svg, ipython_draw_html\n",
    "from duckietown_world.world_duckietown.tile import get_lane_poses\n",
    "from duckietown_world import draw_static\n",
    "\n",
    "from os import path, listdir\n",
    "import json\n",
    "import yaml\n",
    "# from scipy import interpolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Below are some functions used throughout the script\n",
    "\n",
    "def relative_pose(q0, q1):\n",
    "    \"Computes the relative pose between two points in SE2\"\n",
    "    return geo.SE2.multiply(geo.SE2.inverse(q0), q1)\n",
    "\n",
    "class AFakeBar(dw.PlacedObject):\n",
    "    \"Ellipse object with a large ration between the radii\"\n",
    "\n",
    "    def __init__(self, len=0, fill_opacity=0.5, color='pink', *args, **kwargs):\n",
    "        self.len = len\n",
    "        self.fill_opacity = fill_opacity\n",
    "        self.color = color\n",
    "        dw.PlacedObject.__init__(self, *args, **kwargs)\n",
    "\n",
    "    def draw_svg(self, drawing, g):\n",
    "        # drawing is done using the library svgwrite\n",
    "        c = drawing.ellipse(center=(0, 0), r=(0.03,self.len), fill=self.color, fill_opacity=self.fill_opacity)\n",
    "        g.add(c)\n",
    "        # draws x,y axes\n",
    "        dw.draw_axes(drawing, g)\n",
    "        \n",
    "def find_nearest_2d(mid_line, point, theta):\n",
    "    \"\"\"Function to find the nearest point on the midle line to a specific point in 2d\"\"\"\n",
    "    \"\"\"It then calculates the relative x and y offset of the point to the nearest point on the center line\"\"\"\n",
    "    \"\"\" as well as the relative angle of the April Tag on your Duckiebot compared to the cener line\"\"\"\n",
    "#     print(value)\n",
    "    min_dist = 100000\n",
    "    rel_offset_cr_min = 10000\n",
    "#     print(type(mid_line))\n",
    "    start = True\n",
    "    indx = 0\n",
    "    for i in range(1, len(mid_line)):\n",
    "        xs_c = mid_line[i].p[0]\n",
    "        ys_c = mid_line[i].p[1]\n",
    "        xs_p = mid_line[i-1].p[0]\n",
    "        ys_p = mid_line[i-1].p[1]\n",
    "        p1 = np.array([xs_p,ys_p])\n",
    "        p2 = np.array([xs_c,ys_c])\n",
    "        p3 = np.array([point[0],point[1]])\n",
    "        rel_offset_cr = np.cross(p2-p1,p3-p1)/np.linalg.norm(p2-p1)\n",
    "        if rel_offset_cr < rel_offset_cr_min:\n",
    "            rel_offset_cr_min = rel_offset_cr\n",
    "            indx = i\n",
    "        dist = (point[0]-xs_c)**2 + (point[1]-ys_c)**2\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            \n",
    "        \n",
    "            \n",
    "    rel_x = point[0] - mid_line[indx].p[0] \n",
    "    rel_y = point[1] - mid_line[indx].p[1] \n",
    "    rel_angle = mid_line[indx].theta\n",
    "    theta_rel = np.arctan2(np.mean(np.sin(theta-rel_angle)),np.mean(np.cos(theta-rel_angle)))\n",
    "    \n",
    "#     indx = (mid_line.index(idx))    \n",
    "    return indx, rel_x, rel_y, theta_rel, rel_offset_cr_min\n",
    "\n",
    "class Circle(dw.PlacedObject):\n",
    "    \"Circle object.\"\n",
    "\n",
    "    def __init__(self, radius, color='pink', *args, **kwargs):\n",
    "        self.radius = radius\n",
    "        self.color = color\n",
    "        dw.PlacedObject.__init__(self, *args, **kwargs)\n",
    "\n",
    "    def draw_svg(self, drawing, g):\n",
    "        # drawing is done using the library svgwrite\n",
    "        c = drawing.circle(center=(0, 0), r=self.radius, fill=self.color)\n",
    "        g.add(c)\n",
    "        # draws x,y axes\n",
    "        dw.draw_axes(drawing, g)\n",
    "\n",
    "    def extent_points(self):\n",
    "        # set of points describing the boundary\n",
    "        L = self.radius\n",
    "        return [(-L, -L), (+L, +L)]\n",
    "    \n",
    "    \n",
    "def interpolate_custom(q0, q1, alpha):\n",
    "    \"Interpolates between two points in SE2, given a coefficient alpha.\"\n",
    "    q1_from_q0 = relative_pose(q0, q1)\n",
    "    vel = geo.SE2.algebra_from_group(q1_from_q0)\n",
    "    rel = geo.SE2.group_from_algebra(vel * alpha)\n",
    "    q = geo.SE2.multiply(q0, rel)\n",
    "    return q\n",
    "\n",
    "def get_global_center_line(map, used_lane_segs, global_segs_SE2, pts_per_segment):\n",
    "    \"Builds a center line for all the used lanes in the global coordinate frame.\"\n",
    "    center_line = []\n",
    "    center_line_global = []\n",
    "    center_line_global_tfs = []\n",
    "    \n",
    "    # The number of points genereated for the center line depends on the tile \n",
    "    # mid is the number of points for a straight tile\n",
    "    # long is the number of points for a left curve tile\n",
    "    # short is the number of points for a right curve tile\n",
    "    for i, lane_segment in enumerate(used_lane_segs):\n",
    "        if lane_segment[2] == 'straight':\n",
    "            n_inter = int(pts_per_segment['mid'])\n",
    "        elif lane_segment[-1] == 'lane2':\n",
    "            n_inter = int(pts_per_segment['long'])\n",
    "        elif lane_segment[-1] == 'lane1':\n",
    "            n_inter = int(pts_per_segment['short'])\n",
    "        lane = map[lane_segment]\n",
    "\n",
    "        # The end point is part of next tile\n",
    "        steps = np.linspace(0, len(lane.control_points) - 1, num=n_inter, endpoint=False)\n",
    "\n",
    "        for beta in steps:\n",
    "            center_point_local_SE2 = lane.center_point(beta)\n",
    "            center_line.append(center_point_local_SE2)\n",
    "\n",
    "            # get SE2 of the point in global coords\n",
    "            center_point_global_SE2 = geo.SE2.multiply(global_segs_SE2[lane_segment],\n",
    "                                                       center_point_local_SE2)\n",
    "\n",
    "            center_line_global.append(center_point_global_SE2)\n",
    "            center_line_global_tfs.append(dw.SE2Transform.from_SE2(center_point_global_SE2))\n",
    "\n",
    "    \n",
    "    \n",
    "    return center_line_global, center_line_global_tfs\n",
    "\n",
    "def get_used_lanes_mine(trajectories):\n",
    "    \"\"\"Returns a list with all used lanes and a dictionary containing the transform to each lane segment.\"\"\"\n",
    "    \"\"\"It also calculates the number of completed laps, the time needed per tile and it counts the number\"\"\"\n",
    "    \"\"\"of tiles covered (total as well as specific for different types)\"\"\"\n",
    "    \"\"\"Moreover it checks if the Duckiebot had a crash or drives too slow -> if the center of the April Tag\"\"\"\n",
    "    \"\"\"of the  Duckiebot takes more than 30 seconds to get across one tile the Benchmark is stoped there\"\"\"\n",
    "    \"\"\"The time when this happened is saved and the trajectories are shorten to that time\"\"\"\n",
    "    \n",
    "    # If in future for another Benchmark there are other tiles part of the loop just add a dictionary for them as well\n",
    "    used_lane_segs = set()\n",
    "    used_lane_segs_list = []\n",
    "    lane_segs_tfs = dict()\n",
    "    last_lane_seg = dict()\n",
    "    prev_lane_seg = ()\n",
    "    current_lane_seg = ()\n",
    "    start_tile = ()\n",
    "    \n",
    "    total_nb_of_tiles = 0\n",
    "    nb_straight_tiles = 0\n",
    "    nb_curve_left = 0\n",
    "    nb_curve_right = 0\n",
    "    nb_complete_laps = 0\n",
    "    \n",
    "    too_slow = False\n",
    "    \n",
    "    first_time_on_tile = 0.0\n",
    "    start = False\n",
    "    new_tile = False\n",
    "    count = 0\n",
    "    \n",
    "    \n",
    "    for traj in trajectories:\n",
    "        for pose in traj:\n",
    "            count += 1\n",
    "            try:\n",
    "                tl = list(get_lane_poses(m, pose))[0]\n",
    "                lane_segment_name = tl.lane_segment_fqn\n",
    "                if not start:\n",
    "                    # if other tiles are part of the loop, just add another if condition with the name of the tile\n",
    "                    total_nb_of_tiles += 1\n",
    "                    # checks what kind of tile that it is\n",
    "                    if lane_segment_name[2] == \"straight\":  \n",
    "                        nb_straight_tiles += 1  \n",
    "                    elif lane_segment_name[2] == \"curve_left\": \n",
    "                        nb_curve_left += 1;\n",
    "                    elif lane_segment_name[2] == \"curve_right\":\n",
    "                        nb_curve_right += 1\n",
    "                    \n",
    "                    start_tile = lane_segment_name\n",
    "                    current_lane_seg = lane_segment_name\n",
    "                    prev_lane_seg = lane_segment_name\n",
    "                    start = True\n",
    "                    \n",
    "                if lane_segment_name[1] == current_lane_seg[1]:\n",
    "                    new_tile = False\n",
    "                    # the following condoition checks if the Duckiebot drives too slow or not\n",
    "\n",
    "                elif lane_segment_name[1] != current_lane_seg[1]:\n",
    "                    new_tile = True\n",
    "                    # if other tiles are part of the loop, just add another if condition with the name of the tile\n",
    "                    total_nb_of_tiles += 1\n",
    "                    # checks what kind of tile that it is\n",
    "                    if lane_segment_name[2] == \"straight\":  \n",
    "                        nb_straight_tiles += 1  \n",
    "                    elif lane_segment_name[2] == \"curve_left\": \n",
    "                        nb_curve_left += 1;\n",
    "                    elif lane_segment_name[2] == \"curve_right\":\n",
    "                        nb_curve_right += 1\n",
    "                        \n",
    "                    current_lane_seg = lane_segment_name\n",
    "                    \n",
    "                    if lane_segment_name[1] == start_tile[1]:\n",
    "                        print(\"new round\")\n",
    "                        nb_complete_laps +=1\n",
    "                \n",
    "                #checks if the lane segment appears for the first time or not\n",
    "                #if it appears for the first time the new lane segment is added to the list of used lane segments\n",
    "                if lane_segment_name not in used_lane_segs:\n",
    "                    used_lane_segs.add(lane_segment_name)\n",
    "                    used_lane_segs_list.append(lane_segment_name)\n",
    "                    lane_segs_tfs[lane_segment_name] = tl.lane_segment_transform.asmatrix2d().m\n",
    "            except IndexError:\n",
    "                pass\n",
    "\n",
    "    return used_lane_segs_list, lane_segs_tfs, nb_complete_laps, total_nb_of_tiles, nb_straight_tiles, \\\n",
    "nb_curve_left, nb_curve_right\n",
    "\n",
    "def get_used_lanes(trajectories):\n",
    "    \"\"\"Returns a list with all used lanes and a dictionary containing the transform to each lane segment.\"\"\"\n",
    "    used_lane_segs = set()\n",
    "    used_lane_segs_list = []\n",
    "    lane_segs_tfs = dict()\n",
    "\n",
    "    for traj in trajectories:\n",
    "        for pose in traj:\n",
    "            try:\n",
    "                tl = list(get_lane_poses(m, pose))[0]\n",
    "                lane_segment_name = tl.lane_segment_fqn\n",
    "\n",
    "                if lane_segment_name not in used_lane_segs:\n",
    "                    used_lane_segs.add(lane_segment_name)\n",
    "                    used_lane_segs_list.append(lane_segment_name)\n",
    "                    lane_segs_tfs[lane_segment_name] = tl.lane_segment_transform.asmatrix2d().m\n",
    "            except IndexError:\n",
    "                pass\n",
    "    return used_lane_segs_list, lane_segs_tfs\n",
    "\n",
    "def get_interpolated_points(center_line, trajectories):\n",
    "    \"\"\"Generates an interpolated point for each point on the center line, for each trajectory as long as the point\n",
    "    lies between two trajectory points.\"\"\"\n",
    "    closest_behind = [None] * len(trajectories)\n",
    "    interpolated_trajectories = []\n",
    "    for center_point in center_line:\n",
    "        interpolated_points = []\n",
    "        for idx_t, traj in enumerate(trajectories):\n",
    "            interpolated_point_traj = None\n",
    "            begin_t = closest_behind[idx_t] if closest_behind[idx_t] else 0\n",
    "            for idx_point in range(begin_t, len(traj)):\n",
    "                if a_behind_b(a=traj[idx_point], b=center_point):\n",
    "                    closest_behind[idx_t] = idx_point\n",
    "                    continue\n",
    "\n",
    "                if closest_behind[idx_t] is None:\n",
    "                    # If there is no point behind we cannot compute the interpolation\n",
    "                    interpolated_point_traj = None\n",
    "                    break\n",
    "                else:\n",
    "                    try:\n",
    "                        interpolated_point_traj = interpolate_magic(center_point,\n",
    "                                                                    traj[closest_behind[idx_t]],\n",
    "                                                                    traj[closest_behind[idx_t] + 1])\n",
    "                        break\n",
    "\n",
    "                    except IndexError:\n",
    "                        print('The index is outside the list!')\n",
    "                        interpolated_point_traj = None\n",
    "                        break\n",
    "            interpolated_points.append(interpolated_point_traj)\n",
    "        interpolated_trajectories.append(interpolated_points)\n",
    "    return interpolated_trajectories\n",
    "\n",
    "\n",
    "def a_behind_b(a=None, b=None):\n",
    "    \"\"\"Check if a is behind b wrt the heading direction of a.\"\"\"\n",
    "    if a is None or b is None:\n",
    "        return False\n",
    "    rel_pose = relative_pose(b, a)\n",
    "    return dw.SE2Transform.from_SE2(rel_pose).p[0] < 0\n",
    "\n",
    "\n",
    "def interpolate_magic(center_pt, previous_pt, next_pt):\n",
    "    \"\"\"Returns an interpolated point between previoust_pt and next_pt at the height of center_pt\"\"\"\n",
    "    tf_prev = relative_pose(center_pt, previous_pt)\n",
    "    d_prev = dw.SE2Transform.from_SE2(tf_prev).p[0]\n",
    "\n",
    "    tf_next = relative_pose(center_pt, next_pt)\n",
    "    d_next = dw.SE2Transform.from_SE2(tf_next).p[0]\n",
    "\n",
    "    alpha = np.abs(d_prev) / (np.abs(d_prev) + d_next)\n",
    "    interpolated_pt = interpolate_custom(previous_pt, next_pt, alpha)\n",
    "    return interpolated_pt\n",
    "\n",
    "\n",
    "def get_trajectories_statistics(trajectories,center_line):\n",
    "    \"\"\"Computes mean trajectory and std deviations for y and angle given a list of trajectories sampled at the same x\"\"\"\n",
    "    mean_tfs = []\n",
    "    std_y = []\n",
    "    mean_offset = []\n",
    "    cv_y = []\n",
    "    cv_heading = []\n",
    "    std_heading = []\n",
    "    mean_heading = []\n",
    "\n",
    "    start_idx = None\n",
    "    end_idx = None\n",
    "    # We need to find the first amd last index for which all trajectories have a point\n",
    "    for idx, trajs_points in enumerate(trajectories):\n",
    "        if all(trajs_points) and start_idx is None:\n",
    "            start_idx = idx\n",
    "        elif not all(trajs_points) and start_idx is not None:\n",
    "            end_idx = idx\n",
    "            break\n",
    "    end_idx = -1 if end_idx is None else end_idx\n",
    "    complete_trajectories = trajectories[start_idx:end_idx]\n",
    "    \n",
    "    for tfs in complete_trajectories:\n",
    "        xs = [tf.p[0] for tf in tfs]\n",
    "        ys = [tf.p[1] for tf in tfs]\n",
    "        headings = [tf.theta for tf in tfs]\n",
    "        mean_x = np.mean(xs)\n",
    "        mean_y = np.mean(ys)\n",
    "        point = [mean_x , mean_y]\n",
    "        # To compute mean angles we need to pay attention\n",
    "        mean_angle = np.arctan2(np.mean(np.sin(headings)),np.mean(np.cos(headings)))\n",
    "        mean_tfs.append(dw.SE2Transform.from_SE2(geo.SE2_from_translation_angle([mean_x, mean_y], mean_angle)))\n",
    "        \n",
    "        cur_ind = []\n",
    "        cur_offset_mine = []\n",
    "        cur_heading_mine = []\n",
    "        \n",
    "        #find closest point on center_line to the mean of all of the points at this specific x\n",
    "        indx, x_rel, y_rel, theta_rel, rel_offset_cr_min = find_nearest_2d(center_line,point, mean_angle)\n",
    "        for i in range(0, len(xs)):\n",
    "            #transform the point to SE2\n",
    "            point_cur_tf = geo.SE2_from_translation_angle([xs[i], ys[i]], headings[i])\n",
    "            #for each point get relative position to the center line\n",
    "            relative_tf_mine = dw.SE2Transform.from_SE2(relative_pose(point_cur_tf, center_line[indx].as_SE2()))\n",
    "            #extract the offset and the heading angle deviation of the point relative to the center line\n",
    "            cur_offset_mine.append(relative_tf_mine.p[1].item())\n",
    "            cur_heading_mine.append(relative_tf_mine.theta)\n",
    "\n",
    "\n",
    "        # Compute all transforms wrt to the mean trajectory to compute the standard deviations\n",
    "        #lateral_deviation = [(mean_x-t.p[0])*np.sin(t.theta)+(mean_y-t.p[1])*np.cos(t.theta) for t in tfs]\n",
    "        lateral_deviation = []\n",
    "        mean_point = geo.SE2_from_translation_angle([mean_x, mean_y], mean_angle)\n",
    "        for t in tfs:\n",
    "            relative_tf = dw.SE2Transform.from_SE2(relative_pose(mean_point, t.as_SE2()))\n",
    "            lateral_deviation.append(relative_tf.p[1])\n",
    "        \n",
    "        relative_tf_mine = dw.SE2Transform.from_SE2(relative_pose(mean_point, center_line[indx].as_SE2()))\n",
    "\n",
    "        #calculate the avg and std lateral offset of the points (that are at a specific x position) of all the  \n",
    "        #different trajectories to the center line\n",
    "        std_y_cur = float(np.round(np.std(cur_offset_mine),6))\n",
    "        mean_y_cur = float(np.round(np.mean(cur_offset_mine),6))\n",
    "\n",
    "        mean_offset.append(float(np.round(np.mean(cur_offset_mine),6)))\n",
    "        std_y.append(float(np.round(np.std(cur_offset_mine),6)))\n",
    "        if mean_y_cur != 0.0:\n",
    "            cv_y.append(float(np.round(abs(std_y_cur/mean_y_cur),6)))\n",
    "        else:\n",
    "            if std_y_cur == 0.0:\n",
    "                cv_y.append(0.0)\n",
    "            else:\n",
    "                cv_y.append(10.0)\n",
    "                \n",
    "        #calculate the avg and std heading deviation of the points (that are at a specific x position) of all the  \n",
    "        #different trajectories to the center line\n",
    "        std_angle_cur = float(np.round(stats.circstd(cur_heading_mine, low=-math.pi, high=math.pi),6))\n",
    "        mean_angle_cur = float(np.round(stats.circmean(cur_heading_mine, low=-math.pi, high=math.pi),6))\n",
    "        mean_heading.append(mean_angle_cur)\n",
    "        std_heading.append(float(np.round(stats.circstd(cur_heading_mine, low=-math.pi, high=math.pi),6)))\n",
    "        if mean_angle_cur != 0.0:\n",
    "            cv_heading.append(float(np.round(abs(std_angle_cur/mean_angle_cur),6)))\n",
    "        else:\n",
    "            if std_angle_cur == 0.0:\n",
    "                cv_heading.append(0.0)\n",
    "            else:\n",
    "                cv_heading.append(10.0)\n",
    "        \n",
    "    return mean_tfs, std_y, std_heading, start_idx, end_idx,cv_y, cv_heading, mean_offset, mean_heading\n",
    "\n",
    "def get_trajectories_statistics_mean_traj(trajectories, center_line):\n",
    "    \"\"\"For each point on the trajectory of the Duckiebot, the relative offset as well as its angle of the center of \"\"\"\n",
    "    \"\"\"the April Tag of your Duckiebot is calculated\"\"\"\n",
    "    mean_tfs = []\n",
    "    std_y = []\n",
    "    std_heading = []\n",
    "\n",
    "    complete_trajectories = trajectories[:]\n",
    "    lateral_deviation_tes = []\n",
    "    rel_offset_cr = []\n",
    "    theta_rel_cr = []\n",
    "    \n",
    "    for tfs in complete_trajectories:\n",
    "        xs = tfs.p[0]\n",
    "        ys = tfs.p[1]      \n",
    "        headings = tfs.theta\n",
    "        mean_x = np.mean(xs)\n",
    "        mean_y = np.mean(ys)\n",
    "        point = [mean_x , mean_y]\n",
    "        mean_angle = np.arctan2(np.mean(np.sin(headings)),np.mean(np.cos(headings)))\n",
    "        \n",
    "        mean_point = geo.SE2_from_translation_angle([mean_x, mean_y], mean_angle)\n",
    "        \n",
    "        \n",
    "        indx, x_rel, y_rel, theta_rel, rel_offset_cr_min = find_nearest_2d(center_line,point, mean_angle)\n",
    "        \n",
    "        relative_tf = dw.SE2Transform.from_SE2(relative_pose(mean_point, center_line[indx].as_SE2()))\n",
    "        \n",
    "        rel_offset_cr.append(rel_offset_cr_min)\n",
    "        theta_rel_cr.append(theta_rel)\n",
    "        \n",
    "        # Compute all transforms wrt to the mean trajectory to compute the standard deviations\n",
    "        #lateral_deviation = [(mean_x-t.p[0])*np.sin(t.theta)+(mean_y-t.p[1])*np.cos(t.theta) for t in tfs]\n",
    "        lateral_deviation_tes.append((x_rel)*np.sin(theta_rel)+(y_rel)*np.cos(theta_rel))\n",
    "#         print((x_rel)*np.sin(theta_rel)+(y_rel)*np.cos(theta_rel))\n",
    "        offset_wt_non_interp_all.append(relative_tf.p[1].item())\n",
    "        angle_wt_non_interp_all.append(relative_tf.theta)\n",
    "        \n",
    "    return lateral_deviation_tes\n",
    "\n",
    "def Average(lst): \n",
    "    \"\"\"Calculates the average of a list\"\"\"\n",
    "    lst_abs = [abs(x) for x in lst]\n",
    "    return sum(lst_abs) / len(lst) \n",
    "\n",
    "def get_unit(info):\n",
    "    \"\"\"Function returning the unit of the different informations\"\"\" \n",
    "    if info == 'Number_of_completed_laps':\n",
    "        unit = 'laps'\n",
    "    elif info == 'Number_of_tiles_covered':\n",
    "        unit = 'tiles'\n",
    "    elif info == 'Avg_time_needed_per_tile':\n",
    "        unit = 'seconds per tile'\n",
    "    elif info == 'Time_needed_per_straight_tile_sec':\n",
    "        unit = 'seconds per tile'\n",
    "    elif info == 'Time_needed_per_curved_tile':\n",
    "        unit = 'seconds per tile'\n",
    "    elif info == 'Length_of_recorded_bag':\n",
    "        unit = 'seconds' \n",
    "    elif info == 'Actual_length_of_benchmark':\n",
    "        unit = 'seconds'   \n",
    "    elif info == 'Theoretical_length_of_benchmark':\n",
    "        unit = 'seconds'\n",
    "    elif info == 'Out_of_sight':\n",
    "        unit = ' '  \n",
    "    elif info == 'Tolerance_out_of_sight':\n",
    "        unit = 'seconds' \n",
    "    elif info == 'Time_out_of_sight':\n",
    "        unit = 'seconds' \n",
    "    elif info == 'Too_slow':\n",
    "        unit = ' '  \n",
    "    elif info == 'Time_too_slow':\n",
    "        unit = 'seconds' \n",
    "    elif info == 'Tolerance_too_slow_sec':\n",
    "        unit = 'seconds' \n",
    "    elif info == 'Position_too_slow':\n",
    "        unit = ' '\n",
    "    elif info == 'Abs_Ground_truth_wt_std_offset_non_interp':\n",
    "        unit = 'meters' \n",
    "    elif info == 'Abs_Ground_truth_wt_std_angle_non_interp':\n",
    "        unit = 'degree' \n",
    "    elif info == 'Abs_Ground_truth_wt_mean_offset_non_interp':\n",
    "        unit = 'meters' \n",
    "    elif info == 'Abs_Ground_truth_wt_mean_angle_non_interp':\n",
    "        unit = 'degree' \n",
    "    elif info == 'Abs_Ground_truth_wt_median_offset_non_interp':\n",
    "        unit = 'meters' \n",
    "    elif info == 'Abs_Ground_truth_wt_median_angle_non_interp':\n",
    "        unit = 'degree' \n",
    "    elif info == 'Abs_Ground_truth_wt_std_offset_interp':\n",
    "        unit = 'meters' \n",
    "    elif info == 'Abs_Ground_truth_wt_std_angle_interp':\n",
    "        unit = 'degree' \n",
    "    elif info == 'Abs_Ground_truth_wt_mean_offset_interp':\n",
    "        unit = 'meters' \n",
    "    elif info == 'Abs_Ground_truth_wt_mean_angle_interp':\n",
    "        unit = 'degree' \n",
    "    elif info == 'Abs_Ground_truth_wt_median_offset_interp':\n",
    "        unit = 'meters' \n",
    "    elif info == 'Abs_Ground_truth_wt_median_angle_interp':\n",
    "        unit = 'degree'\n",
    "    elif info == 'Abs_Measurements_db_std_offset':\n",
    "        unit = 'meters'\n",
    "    elif info == 'Abs_Measurements_db_std_angle':\n",
    "        unit = 'degree' \n",
    "    elif info == 'Abs_Measurements_db_mean_offset':\n",
    "        unit = 'meters' \n",
    "    elif info == 'Abs_Measurements_db_mean_angle':\n",
    "        unit = 'degree' \n",
    "    elif info == 'Abs_Measurements_db_median_offset':\n",
    "        unit = 'meters' \n",
    "    elif info == 'Abs_Measurements_db_median_angle':\n",
    "        unit = 'degree' \n",
    "    elif info == 'std_diff_btw_estimation_and_ground_truth_offset':\n",
    "        unit = 'meters' \n",
    "    elif info == 'std_diff_btw_estimation_and_ground_truth_angle':\n",
    "        unit = 'degree'\n",
    "    elif info == 'mean_diff_btw_estimation_and_ground_truth_offset':\n",
    "        unit = 'meters' \n",
    "    elif info == 'mean_diff_btw_estimation_and_ground_truth_angle':\n",
    "        unit = 'degree'\n",
    "    elif info == 'median_diff_btw_estimation_and_ground_truth_offset':\n",
    "        unit = 'meters' \n",
    "    elif info == 'median_diff_btw_estimation_and_ground_truth_angle':\n",
    "        unit = 'degree'\n",
    "    elif info == 'offset_db':\n",
    "        unit = 'meters' \n",
    "    elif info == 'angle_db':\n",
    "        unit = 'degree'\n",
    "    elif info == 'offset_wt_non_interp':\n",
    "        unit = 'meters' \n",
    "    elif info == 'angle_wt_non_interp':\n",
    "        unit = 'degree'\n",
    "    else:\n",
    "        unit = 'Uuuups' \n",
    "    \n",
    "    return unit\n",
    "\n",
    "def count_true(results):\n",
    "    \"\"\"Function that calculates how many times the boolean True is within the list results\"\"\"\n",
    "    counter = 0\n",
    "    for i in range(0,len(results)):\n",
    "        if results[i] == True:\n",
    "            counter += 1\n",
    "    return counter\n",
    "\n",
    "def comp_low_bet(ra, rb):\n",
    "    \"\"\"Compares ra and rb and returns the lower one, B relative to A, difference as well as the sign.\"\"\"\n",
    "    if ra != 0:\n",
    "        brela = 100.0/abs(float(ra)) * abs(float(rb))\n",
    "    else:\n",
    "        brela = 100.0\n",
    "    \n",
    "    diff = rb - ra\n",
    "    \n",
    "    if np.sign(diff) == 1:\n",
    "        sign = '+'\n",
    "    elif np.sign(diff) == -1:\n",
    "        sign = '-'\n",
    "    elif np.sign(diff) == 0:\n",
    "        sign = ' '\n",
    "    else:\n",
    "        sign = 'sth weird happend'\n",
    "        \n",
    "    if ra < rb:\n",
    "        win = 'A'\n",
    "    elif ra > rb:\n",
    "        win = 'B'\n",
    "    else:\n",
    "        win = 'equal'\n",
    "    return win,brela,diff,sign\n",
    "\n",
    "def comp_high_bet(ra, rb):\n",
    "    \"\"\"Compares ra and rb and returns the higher one, B relative to A, difference as well as the sign.\"\"\"\n",
    "    if ra != 0:\n",
    "        brela = 100.0/abs(float(ra)) * abs(float(rb))\n",
    "    else:\n",
    "        brela = 100.0\n",
    "        \n",
    "    diff = rb - ra\n",
    "    \n",
    "    if np.sign(diff) == 1:\n",
    "        sign = '+'\n",
    "    elif np.sign(diff) == -1:\n",
    "        sign = '-'\n",
    "    elif np.sign(diff) == 0:\n",
    "        sign = ' '\n",
    "    else:\n",
    "        sign = 'sth weird happend'\n",
    "        \n",
    "    if ra < rb:\n",
    "        win = 'B'\n",
    "    elif ra > rb:\n",
    "        win = 'A'\n",
    "    else:\n",
    "        win = 'equal'\n",
    "    \n",
    "    return win,brela,diff,sign\n",
    "\n",
    "def autolabel(rects):\n",
    "    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
    "    for rect in rects:\n",
    "        height = float(np.round(rect.get_height(),2))\n",
    "        ax.annotate('{}'.format(height),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your comparison/result\n",
    "\n",
    "Congratulations, you successfully complteded the Lane Following Benchmark. You are now about to compare it to another Lane Following Benchmark. To do so please run all the cells below.\n",
    "As a result you will get a nice overview about your performance in the different fields. If you want to save those results in a nice PDF file, please toggle out all the code pressing the button created in the first cell and then save the notebook as a PDF.\n",
    "\n",
    "To start, please place the `BAGNAME_benchmark_final_results.yaml` files (normaly found in the folder `~/behaviour-benchmarking/data/BenchmarkXY/benchmarks/final`) of the two Benchmark results you want to compare within the folder `/data/compare_bm` of your `behaviour-benchmarking` repository.\n",
    "Please adapt their name to `BAGNAME_benchmark_final_results_01.yaml` and `BAGNAME_benchmark_final_results_02.yaml` respectively.\n",
    "\n",
    "### Map\n",
    "Here is the Map your benchmark and the one you compare was performed on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Please change the Map_Name variable if you ran the Benchmark on a different map\n",
    "Map_Name = 'linus_loop'\n",
    "m = dw.load_map(Map_Name);\n",
    "\n",
    "print(\"The Map name is: \\n\" + Map_Name);\n",
    "ipython_draw_svg(m, 'name');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "experiment_dir = ''\n",
    "logs_path = path.join(experiment_dir, '../data/compare_bm')\n",
    "#loads the the yaml files found under logs_path\n",
    "localization_logs = [path.join(logs_path, f) for f in listdir(logs_path) if path.isfile(path.join(logs_path, f))]\n",
    "print(f'Logs found: {localization_logs}')\n",
    "\n",
    "advantages = {}\n",
    "\n",
    "compare_a = []\n",
    "compare_b = []\n",
    "enough_data_traj_a = True\n",
    "enough_data_res_a = True\n",
    "enough_data_traj_b = True\n",
    "enough_data_res_b = True\n",
    "\n",
    "i = 0\n",
    "for filename in localization_logs:\n",
    "    with open(filename, 'r') as file:\n",
    "        if i == 0:\n",
    "            name_a = filename\n",
    "            compare_a.append(yaml.safe_load(file))\n",
    "        if i == 1:\n",
    "            name_b = filename\n",
    "            compare_b.append(yaml.safe_load(file))\n",
    "    i += 1\n",
    "#extract the names and check how many files that were uploaded\n",
    "name_a = path.basename(name_a)    \n",
    "if i == 2:\n",
    "    do_comparison = True\n",
    "    name_b = path.basename(name_b)\n",
    "elif i == 1:\n",
    "    do_comparison = False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Check if the two Benchmarks are of the same type\n",
    "if do_comparison:\n",
    "    if compare_a[0]['Benchmark_Type'] != compare_b[0]['Benchmark_Type']:\n",
    "        print(\"This comparison won't work as the results come from two different Benchmarks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Benchmark type you are comparing is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(compare_a[0]['Benchmark_Type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "db_data_a = compare_a[0]['db_data']\n",
    "diag_toolbox_data_a = compare_a[0]['diag_toolbox_data']\n",
    "if do_comparison:\n",
    "    db_data_b = compare_b[0]['db_data']\n",
    "    diag_toolbox_data_b = compare_b[0]['diag_toolbox_data']\n",
    "    print(\"The two Benchmark results you are about to compare are called:\")\n",
    "    print(name_a + ' which will be referred to as Benchmark A and ' + name_b + ' which will be referred to as Benchmark B')\n",
    "    if db_data_a:\n",
    "        if diag_toolbox_data_a:\n",
    "            print(\"Benchmark A has data recorded of the diagnostic toolbox and of the Duckiebot directly.\")\n",
    "        elif not diag_toolbox_data_a:\n",
    "            print(\"Benchmark A has no data recorded of the diagnostic toolbox but he has data recorded of the \\\n",
    "            Duckiebot directly.\")\n",
    "    elif not db_data_a:\n",
    "        if diag_toolbox_data_a:\n",
    "            print(\"Benchmark A has data recorded of the diagnostic toolbox bun none of the Duckiebot directly.\")\n",
    "        elif not diag_toolbox_data_a:\n",
    "            print(\"Benchmark A has no data recorded of the diagnostic toolbox and no data of the Duckiebot directly.\")\n",
    "    \n",
    "    if db_data_b:\n",
    "        if diag_toolbox_data_b:\n",
    "            print(\"Benchmark B has data recorded of the diagnostic toolbox and of the Duckiebot directly.\")\n",
    "        elif not diag_toolbox_data_b:\n",
    "            print(\"Benchmark B has no data recorded of the diagnostic toolbox but he has data recorded of the \\\n",
    "            Duckiebot directly\")\n",
    "    elif not db_data_b:\n",
    "        if diag_toolbox_data_b:\n",
    "            print(\"Benchmark B has data recorded of the diagnostic toolbox bun none of the Duckiebot directly.\")\n",
    "        elif not diag_toolbox_data_b:\n",
    "            print(\"Benchmark B has no data recorded of the diagnostic toolbox and no data of the Duckiebot directly.\")\n",
    "else:\n",
    "    print(\"The Benchmark you are about to score is called:\")\n",
    "    print(name_a)\n",
    "    if db_data_a:\n",
    "        if diag_toolbox_data_a:\n",
    "            print(\"Your Benchmark has data recorded of the diagnostic toolbox and of the Duckiebot directly.\")\n",
    "        elif not diag_toolbox_data_a:\n",
    "            print(\"Your Benchmark has no data recorded of the diagnostic toolbox but he has data recorded of the Duckiebot directly.\")\n",
    "    elif not db_data_a:\n",
    "        if diag_toolbox_data_a:\n",
    "            print(\"Your Benchmark has data recorded of the diagnostic toolbox bun none of the Duckiebot directly.\")\n",
    "        elif not diag_toolbox_data_a:\n",
    "            print(\"Your Benchmark has no data recorded of the diagnostic toolbox and no data of the Duckiebot directly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if do_comparison:\n",
    "    if compare_a[0]['Enough data traj wise']:\n",
    "        if compare_a[0]['Enough data results wise']:\n",
    "            enough_data_traj_a = True\n",
    "            enough_data_res_a = True\n",
    "            print(\"Benchmark A has ran enough experiments for an actual comparison\")\n",
    "        elif not compare_a[0]['Enough data results wise']:\n",
    "            enough_data_traj_a = True\n",
    "            enough_data_res_a = False\n",
    "            print(\"Benchmark A has ran not enough experiments for an actual comparison of the results\")\n",
    "    elif not compare_a[0]['Enough data traj wise']:\n",
    "        if compare_a[0]['Enough data results wise']:\n",
    "            enough_data_traj_a = False\n",
    "            enough_data_res_a = True\n",
    "            print(\"Benchmark A has ran not enough experiments for an actual comparison of the trajectory\")\n",
    "        elif not compare_a[0]['Enough data results wise']:\n",
    "            enough_data_traj_a = False \n",
    "            enough_data_res_a = False\n",
    "            print(\"Benchmark A has ran not enough experiments for an actual comparison of the results nor trajectory\")\n",
    "  \n",
    "    if compare_b[0]['Enough data traj wise']:\n",
    "        if compare_b[0]['Enough data results wise']:\n",
    "            enough_data_traj_b = True\n",
    "            enough_data_res_b = True\n",
    "            print(\"Benchmark B has ran enough experiments for an actual comparison\")\n",
    "        elif not compare_b[0]['Enough data results wise']:\n",
    "            enough_data_traj_b = True\n",
    "            enough_data_res_b = False\n",
    "            print(\"Benchmark B has ran not enough experiments for an actual comparison of the results\")\n",
    "    elif not compare_b[0]['Enough data traj wise']:\n",
    "        if compare_b[0]['Enough data results wise']:\n",
    "            enough_data_traj_b = False\n",
    "            enough_data_res_b = True\n",
    "            print(\"Benchmark B has ran not enough experiments for an actual comparison of the trajectory\")\n",
    "        elif not compare_a[0]['Enough data results wise']:\n",
    "            enough_data_traj_b = False\n",
    "            enough_data_res_b = False\n",
    "            print(\"Benchmark B has ran not enough experiments for an actual comparison of the results nor trajectory\")\n",
    "                  \n",
    "else:\n",
    "    if compare_a[0]['Enough data traj wise']:\n",
    "        if compare_a[0]['Enough data results wise']:\n",
    "            enough_data_traj_a = True\n",
    "            enough_data_res_a = True\n",
    "            print(\"Your Benchmark has ran enough experiments for an actual comparison\")\n",
    "        elif not compare_a[0]['Enough data results wise']:\n",
    "            enough_data_traj_a = True\n",
    "            enough_data_res_a = False\n",
    "            print(\"Your Benchmark has ran not enough experiments for an actual comparison of the results\")\n",
    "    elif not compare_a[0]['Enough data traj wise']:\n",
    "        if compare_a[0]['Enough data results wise']:\n",
    "            enough_data_traj_a = False\n",
    "            enough_data_res_a = True\n",
    "            print(\"Your Benchmark has ran not enough experiments for an actual comparison of the trajectory\")\n",
    "        elif not compare_a[0]['Enough data results wise']:\n",
    "            enough_data_traj_a = False \n",
    "            enough_data_res_a = False\n",
    "            print(\"Your Benchmark has ran not enough experiments for an actual comparison of the results nor trajectory\")\n",
    "  \n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_comparison:\n",
    "    print(\"The actual performance of Benchmark A is based on {0} experiments whilst the performance of Benchmark B is based on {1}\\\n",
    "    experiments\".format(compare_a[0]['Number of tests ran'],compare_b[0]['Number of tests ran']))\n",
    "    print(\"\\n Also Benchmark A in average ran for {0} seconds where as Benchmark A ran in avg for {1} seconds\"\\\n",
    "          .format(float(np.round(compare_a[0]['Results']['Actual_length_of_benchmark']['Mean'],1)),\\\n",
    "                  float(np.round(compare_b[0]['Results']['Actual_length_of_benchmark']['Mean'],1))))\n",
    "else:\n",
    "    print(\"The actual performance of your is based on {0} experiments\".format(compare_a[0]['Number of tests ran']))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if do_comparison:\n",
    "    print(\"Lets now have a look at the performance of Benchmark A compared to B\")\n",
    "else:\n",
    "    print(\"Let's now have a look at the results of your Benchmark\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Benchmark Information\n",
    "\n",
    "### Experiment definition\n",
    "\n",
    "* Run lane following for 50 seconds\n",
    "* Termination criteria:\n",
    "    * Out of sight: 5 sec\n",
    "    * Crash / Too slow: 15 sec per tile\n",
    "* Repetition criteria:\n",
    "    * Consistency in data\n",
    "* Metrics:\n",
    "    * Behaviour\n",
    "    * Engineering data\n",
    "\n",
    "### Environment Definition\n",
    "\n",
    "* 1 Duckiebot \n",
    "* No natural light, just be white light coming from the ceiling\n",
    "* Map: 'linus_loop'\n",
    "\n",
    "### Metrics used\n",
    "\n",
    "In the Benchmark we consider on one hand the actual performance of the code, which means the actual behaving, and on the other hand we consider the engineering data performance. The engineering data that was recorded during the Benchmark gives you an insight on the CPU usage, update frequency of the different nodes, the latency etc.\n",
    "\n",
    "The metrics used here to generate a score are the following (please not that in brackets the priorities are noted, H = High priority, M = Medium priority and L = low priority):\n",
    "1. Behaviour performance\n",
    "    * Mean absolute distance of the Duckiebot to the center of the lane [m] (H)\n",
    "    * Mean absolute heading offset of the Duckiebot compared to the reference heading [deg] (H)\n",
    "    * Mean absoulte difference between the calculated/estimated offset by the Duckiebot and the actual offset calculated/measured by the Watchtowers [m] (M)\n",
    "    * Mean absolute difference between the calculated heading error by the Duckiebot and the actual heading error calculated by the Watchtowers. [deg] (M)\n",
    "    * Number of crashes (number of early stops due to slow driving or crashes devided by the number of experiments ran) (H)\n",
    "    * Mean time until termination [seconds] (L)\n",
    "    * Mean time needed per tile [seconds/tile] (L)\n",
    "    \n",
    "2. Engineering data performance:\n",
    "    * Mean latency (lag up to and including the detector node) [ms] (H)\n",
    "    * Mean of the update frequency of the different nodes [Hz] (H)\n",
    "    * Mean of the CPU usage of the different nodes in the dt-core container [%] (H)\n",
    "    * Mean of the Memory usage of the different nodes in the dt-core container [%] (L)\n",
    "    * Mean of the nr of Threads of the different nodes in the dt-core container (L)\n",
    "    * Overall CPU usage [%] (H)\n",
    "    * Overall Memory usage [%] (M)\n",
    "    * Overall SWAP usage [%] (M)\n",
    "\n",
    "The score then is calculated seperately for the Behaviour performance and the Enginieering data performane, where the score is increased by `+5` if the property has high priority, `+3` if the property has medium priority and `+1` if the property has low priority.\n",
    "The overall score is then simply the sum of the behaviour score and the engineering data score.\n",
    "\n",
    "Please note that the localization of the Duckiebot that is measured by the watchtowers is with respect to the center of the April Tag that is placed on your Duckiebot.\n",
    "This means that all kind of measurements and results that talk about the position of the Duckiebot are refering to the center of the April Tag on top of the Duckiebot. \n",
    "\n",
    "Also note that during the report you will see many different kind of results, also some of it which is not taken into account for the actual scoring.\n",
    "You can easily add a property to the scoring condition or change the priority of the property if you want to focus your score on something specific. This is simply done by changing the lists called: `high_prio_beh`, `low_prio_beh`, `medium_prio_beh`, `high_prio_eng`, `low_prio_eng` respectively  `medium_prio_eng`.\n",
    "\n",
    "If you want to further understand what is happening please toggle on the code and read through it, it is all commented in detail so it should be very simple to follow. Also the notebooks used to prepare everything are commented and explained. So to understand how for example the relative lane pose is calculated or how the mean is found of the heading offset of the Duckiebot compared to the center lane, just scorll through the Notebooks.\n",
    "\n",
    "Also you find in the [Documentation](https://github.com/llingg/docs-opmanual_developer) under the Chapter 3 [Benchmarking](https://github.com/llingg/docs-opmanual_developer/tree/daffy/book/opmanual_developer/30_benchmarking) all the needed information to further understand what is happening.\n",
    "If you find a mistake or a contribution please feel free to contact Linus Lingg on Slack. \n",
    "\n",
    "#### Formulas\n",
    "Arithmetic mean:\n",
    "$$\\bar{x}=\\frac{1}{n} \\sum_{i=1}^n x_i $$\n",
    "\n",
    "The standard deviation:\n",
    "$$s=\\sqrt{\\frac{1}{N}\\sum_{i=1}^N(x_i-\\bar{x})^2}$$\n",
    "\n",
    "Coefficient of variation (CV):\n",
    "$$CV = \\frac{std}{mean} *100 [\\%]$$\n",
    "\n",
    "<br /><br />\n",
    "\n",
    "But now I wish you good Duck and hope your code performs well and gets a great score :) \n",
    "\n",
    "<br /><br />\n",
    "\n",
    "First we have a look at the Hardware that was used for the two different benchmarks. If this Hardware has some major differences please not that your results might vary because of the Hardware difference. To check this please run a Hardware Benchmark.\n",
    "Here it is assumed that you followed precisely the Behaviour Benchmarking instructions which will result in two identical and therefor well comparable Hardware set ups.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_comparison:\n",
    "    print(\"Below you see the Hardware Information received from the Hardware check of the two Benchmarks.\\n\\\n",
    "    Please note that for a meaningful comparison they should be equal in all the keypoints. This should hold if you carefully followed the instructions of the Benchmark\\n\\\n",
    "    The keypoints are: verdict, db_version, platform, hat_version, actuation and camera.\")\n",
    "else:\n",
    "    print(\"Below you see the Hardware Information received from the Hardware check of your Benchmark.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_comparison:\n",
    "    print('Item\\t\\t\\tBenchmark A\\t\\t\\t\\tBenchmark B')\n",
    "    print('--------------------------------------------------------------------------------------')\n",
    "    for item, doc in compare_a[0]['Engineering Data']['Performance']['HW_info'].items():\n",
    "        res_a = str(doc).strip()\n",
    "        res_b = str(compare_b[0]['Engineering Data']['Performance']['HW_info'][item]).strip()\n",
    "        \n",
    "        if item == 'item':\n",
    "            continue\n",
    "        elif item in ('verdict','country'):\n",
    "            print(item+'\\t\\t\\t'+res_a+'\\t\\t\\t\\t\\t'+res_b)\n",
    "        elif item in ('hostname','hat_version'):\n",
    "            print(item+'\\t\\t'+res_a+'\\t\\t\\t\\t'+res_b)\n",
    "        elif item in ('mac-adress','usb-memory'):\n",
    "            print(item+'\\t\\t'+res_a+'\\t\\t\\t'+res_b)\n",
    "        elif item in ('date'):\n",
    "            print(item+'\\t\\t\\t'+res_a+'\\t\\t\\t\\t'+res_b)\n",
    "        elif item in ('battery'):\n",
    "            print(item+'\\t\\t\\t'+res_a+'\\t\\t\\t'+res_b)\n",
    "        elif item in ('actuation'):\n",
    "            print(item+'\\t\\t'+res_a+'\\t'+res_b)\n",
    "        elif item in ('camera'):\n",
    "            print(item+'\\t\\t\\t'+res_a+'\\t\\t'+res_b)\n",
    "        else:\n",
    "            print(item+'\\t\\t'+res_a+'\\t\\t\\t\\t\\t'+res_b)\n",
    "#         print(item + ':\\t\\t', doc, '\\t\\t\\t\\t\\t', res_b)\n",
    "\n",
    "\n",
    "else:\n",
    "    print(\"Your Benchmark:\")\n",
    "    for item, doc in compare_a[0]['Engineering Data']['Performance']['HW_info'].items():\n",
    "        print(item, \":\", doc)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Dashboard Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "comp_diag = True\n",
    "comp_db_dat = True\n",
    "\n",
    "if do_comparison:    \n",
    "    if diag_toolbox_data_a and diag_toolbox_data_b:\n",
    "        comp_diag = True\n",
    "        print(\"Luckely both Benchmarks have recorded data from the diagnostic toolbox which means the two Benchmarks can\\\n",
    "        be compared based on this measured engineering/performance data.\")\n",
    "    else:\n",
    "        comp_diag = False\n",
    "        print(\"Unfortunately, one of the Benchmarks does not have data recorded from the Diagnostic Toolbox,\\\n",
    "        therefor there won't be a comparison in this field. If your new Benchmark does have data recorded from the\\\n",
    "        diagnostic toolbox and the one you are comparing it with does not, please just upload your Benchmark\\\n",
    "        results and run through this Notebook to get some idea about the score of you performance.\")\n",
    "\n",
    "    if db_data_a and db_data_b:\n",
    "        comp_db_dat = True\n",
    "        print(\"Luckely both Benchmarks have recorded data from the Duckiebot directly which means the two Benchmarks can \\\n",
    "        be compared based those measurements.\")\n",
    "    else:\n",
    "        comp_db_dat = False\n",
    "        print(\"Unfortunately, one of the Benchmarks does not have data recorded from the Duckiebot directly,\\\n",
    "        therefor there won't be a comparison in this field. If your new Benchmark does have data recorded from the\\\n",
    "        diagnostic toolbox and the one you are comparing it with does not, please just upload your Benchmark\\\n",
    "        results and run through this Notebook to get some idea about the score of you performance.\")\n",
    "        \n",
    "else:\n",
    "    if diag_toolbox_data_a:\n",
    "        comp_diag = True\n",
    "        print(\"Luckely your Benchmark has recorded data from the diagnostic toolbox which means it can be scored based on this measured engineering/performance data.\")\n",
    "    else:\n",
    "        comp_diag = False\n",
    "        print(\"Unfortunately, your Benchmark does not have data recorded from the Diagnostic Toolbox,\\\n",
    "        therefor there won't be a scoring in this field.\")\n",
    "\n",
    "    if db_data_a:\n",
    "        comp_db_dat = True\n",
    "        print(\"Luckely your Benchmark has recorded data from the Duckiebot directly which means it can be scored based on those measurements.\")\n",
    "\n",
    "    else:\n",
    "        comp_db_dat = False\n",
    "        print(\"Unfortunately, your Benchmark does not have data recorded from the Dcukiebot directly,\\\n",
    "        therefor there won't be a scoring in this field.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we look at the Software that was used for the two Benchmarks, below you see the Containers that were running on the Duckiebots during the Benchmarks as well as their image names and tags, their base image, module type as well as their template name.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if do_comparison and comp_diag:\n",
    "    print(\"Benchmark A:\")\n",
    "    for cont,inf in compare_a[0]['Engineering Data']['Static']['Dashboard Info'].items():\n",
    "        if cont!= 'Container Name':\n",
    "            print(cont+':')\n",
    "            for infn in inf:\n",
    "                if infn in ('Image Name', 'Image Tag','DT label base image','DT label module type','DT label template name'):\n",
    "                    print('\\t'+infn+': '+inf[infn])\n",
    "    print('--------------------------------------------------------------------------------------------------------------')\n",
    "    print(\"Benchmark B:\")\n",
    "    for cont,inf in compare_b[0]['Engineering Data']['Static']['Dashboard Info'].items():\n",
    "        if cont!= 'Container Name':\n",
    "            print(cont+':')\n",
    "            for infn in inf:\n",
    "                if infn in ('Image Name', 'Image Tag','DT label base image','DT label module type','DT label template name'):\n",
    "                    print('\\t'+infn+': '+inf[infn])\n",
    "elif (not do_comparison) and comp_diag:\n",
    "    print(\"Your Benchmark:\")\n",
    "    for cont,inf in compare_a[0]['Engineering Data']['Static']['Dashboard Info'].items():\n",
    "        if cont!= 'Container Name':\n",
    "            print(cont+':')\n",
    "            for infn in inf:\n",
    "                if infn in ('Image Name', 'Image Tag','DT label base image','DT label module type','DT label template name'):\n",
    "                    print('\\t'+infn+': '+inf[infn])\n",
    "else:\n",
    "    print(\"unfortunately this comparison can not be made as one of the Benchmakrks does not have this information\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets have a look at the constats that were set when the experiments were ran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if do_comparison and comp_diag:\n",
    "    print('Constant\\tBenchmark A\\t\\tBenchmark B')\n",
    "    print('--------------------------------------------------------------------------------------')\n",
    "    for item, doc in compare_a[0]['Engineering Data']['Static']['Constants'].items():\n",
    "        res_a = str(doc).strip()\n",
    "        res_b = str(compare_b[0]['Engineering Data']['Static']['Constants'][item]).strip()\n",
    "        \n",
    "        if item in ('omega_max','baseline'):\n",
    "            print(item + ':\\t', res_a, '\\t\\t', res_b)\n",
    "        else:\n",
    "            print(item + ':\\t\\t', res_a, '\\t\\t', res_b)\n",
    "        \n",
    "\n",
    "\n",
    "elif (not do_comparison) and comp_diag:\n",
    "    print(\"Your Benchmark:\")\n",
    "    for item, doc in compare_a[0]['Engineering Data']['Performance']['HW_info'].items():\n",
    "        print(item, \":\", doc)\n",
    "        \n",
    "else:\n",
    "    print(\"unfortunately this comparison can not be made as one of the Benchmakrks does not have this information\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if do_comparison and comp_diag:\n",
    "    print(\"Below you can see the overall CPU, SWAP and Memory usage of the two benchmarks compared to each other. \\\n",
    "    Please note that the mean is of the corresponding measurements are shown as the usages don't change \\\n",
    "    significantly over the time of the Benchmark.\\n\\\n",
    "    The Benchmark that performed better will have a green bar and the one performing worse a red bar. If they both\\\n",
    "    performed equally they will both be colored blue\\n\\n\")\n",
    "    fil_used_diag_a = compare_a[0]['Engineering Data']['Performance']['Total engineering data container']\\\n",
    "    ['Nb of files']\n",
    "    fil_used_diag_b = compare_b[0]['Engineering Data']['Performance']['Total engineering data container']\\\n",
    "    ['Nb of files']\n",
    "    print(\"For the Benchmark A the Diagnostic Toolbox ran {0} times whilst doing the experiments\\\n",
    "    compared to Benchmark B where the Diagnostic Toolbox ran {1} times.\".format(fil_used_diag_a,fil_used_diag_b))\n",
    "\n",
    "    plt.rcParams.update({'font.size': 15})\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3)\n",
    "    fig.subplots_adjust(hspace=0.5)\n",
    "    fig.suptitle('Resources of Benchmark A vs. Benchmark B', y=1.1)\n",
    "    objects = ('Benchmark A', 'Benchmark B')\n",
    "    \n",
    "    x_pos = np.arange(len(objects))\n",
    "    count = 0\n",
    "    for cont,inf in compare_a[0]['Engineering Data']['Performance']['Total engineering data container']\\\n",
    "    ['Measurements']['Overall engineering data container'].items():\n",
    "        performance = [inf['Mean'],\\\n",
    "                       compare_b[0]['Engineering Data']['Performance']['Total engineering data container']\\\n",
    "                       ['Measurements']['Overall engineering data container'][cont]['Mean']]\n",
    "        #checks which Benchmark performed better, adds it to the corresponding advange list and colors the plots \n",
    "        #accordingly (green for the one with better performance in this property and red for the one that performed\n",
    "        #worse. If they performed equally the plots are blue.)\n",
    "        if performance[0] < performance[1]:\n",
    "            advantages.update({cont: 'A'})\n",
    "            winner_color = ['green', 'red']\n",
    "        elif performance[0] > performance[1]:\n",
    "            advantages.update({cont: 'B'})\n",
    "            winner_color = ['red', 'green']\n",
    "        elif performance[0] == performance[1]:\n",
    "            advantages.update({cont: 'equal'})\n",
    "            winner_color = ['blue', 'blue']   \n",
    "        else:\n",
    "            winner_color = ['black', 'black']\n",
    "        ax = axes.flatten()[count]\n",
    "        rec = ax.bar(x_pos, performance, align='center', alpha=0.5, color=winner_color)\n",
    "        ax.set_xticks(x_pos)\n",
    "        ax.set_xticklabels(objects)\n",
    "        ax.set_ylabel('%')\n",
    "        ax.set_title(cont)\n",
    "        \n",
    "        bottom, top = ax.get_ylim()\n",
    "        ax.set_ylim(bottom, top+5)  \n",
    "        \n",
    "        autolabel(rec)\n",
    "        count+=1\n",
    "\n",
    "    cpu_std_a = compare_a[0]['Engineering Data']['Performance']['Total engineering data container']['Measurements']\\\n",
    "    ['Overall engineering data container']['Overall CPU (CPU used in %)']['Standard Deviation']\n",
    "    cpu_std_b = compare_b[0]['Engineering Data']['Performance']['Total engineering data container']['Measurements']\\\n",
    "    ['Overall engineering data container']['Overall CPU (CPU used in %)']['Standard Deviation']\n",
    "    cv_a = compare_a[0]['Engineering Data']['Performance']['Total engineering data container']['Measurements']\\\n",
    "    ['Overall engineering data container']['Overall CPU (CPU used in %)']['coefficient of variation']\n",
    "    cv_b = compare_b[0]['Engineering Data']['Performance']['Total engineering data container']['Measurements']\\\n",
    "    ['Overall engineering data container']['Overall CPU (CPU used in %)']['coefficient of variation']\n",
    "    cpu_txt= 'The means over time of the CPU usage are compared as \\n the std of A is {0} and the std of B is {1} which gives \\n a CV of {2}\\\n",
    "    resp. {3}. This means that the mean \\n comparison is reasonable'.format(cpu_std_a,cpu_std_b,cv_a,cv_b)\n",
    "    fig.text(.23, -.25, cpu_txt, ha='center')    \n",
    "\n",
    "    swaps_std_a = compare_a[0]['Engineering Data']['Performance']['Total engineering data container']['Measurements']\\\n",
    "    ['Overall engineering data container']['Overall Swaps (Swaps used in %)']['Standard Deviation']\n",
    "    swaps_std_b = compare_b[0]['Engineering Data']['Performance']['Total engineering data container']['Measurements']\\\n",
    "    ['Overall engineering data container']['Overall Swaps (Swaps used in %)']['Standard Deviation']\n",
    "    cv_a = compare_a[0]['Engineering Data']['Performance']['Total engineering data container']['Measurements']\\\n",
    "    ['Overall engineering data container']['Overall Swaps (Swaps used in %)']['coefficient of variation']\n",
    "    cv_b = compare_b[0]['Engineering Data']['Performance']['Total engineering data container']['Measurements']\\\n",
    "    ['Overall engineering data container']['Overall Swaps (Swaps used in %)']['coefficient of variation']\n",
    "    swaps_max_a = compare_a[0]['Engineering Data']['Performance']['Total engineering data container']['Measurements']\\\n",
    "    ['Overall engineering data container']['Overall Swaps (Swaps used in %)']['Swaps max']\n",
    "    swaps_max_b = compare_b[0]['Engineering Data']['Performance']['Total engineering data container']['Measurements']\\\n",
    "    ['Overall engineering data container']['Overall Swaps (Swaps used in %)']['Swaps max']\n",
    "    swaps_txt= 'The means over time of the SWAPS usage are compared as \\n the std of A is {0} and the std of B is {1} which gives \\n a CV of {2}\\\n",
    "    resp. {3}. This means that the mean \\n comparison is reasonable.\\n\\\n",
    "    The maximum SWAPS for A is: {4} \\n and for B: {5}'.format(swaps_std_a,swaps_std_b,cv_a,cv_b,swaps_max_a,swaps_max_b)\n",
    "    fig.text(.5, -.25, swaps_txt, ha='center')    \n",
    "\n",
    "    mem_std_a = compare_a[0]['Engineering Data']['Performance']['Total engineering data container']['Measurements']\\\n",
    "    ['Overall engineering data container']['Overall Mem (memory used in %)']['Standard Deviation']\n",
    "    mem_std_b = compare_b[0]['Engineering Data']['Performance']['Total engineering data container']['Measurements']\\\n",
    "    ['Overall engineering data container']['Overall Mem (memory used in %)']['Standard Deviation']\n",
    "    cv_a = compare_a[0]['Engineering Data']['Performance']['Total engineering data container']['Measurements']\\\n",
    "    ['Overall engineering data container']['Overall Mem (memory used in %)']['coefficient of variation']\n",
    "    cv_b = compare_b[0]['Engineering Data']['Performance']['Total engineering data container']['Measurements']\\\n",
    "    ['Overall engineering data container']['Overall Mem (memory used in %)']['coefficient of variation']\n",
    "    mem_max_a = compare_a[0]['Engineering Data']['Performance']['Total engineering data container']['Measurements']\\\n",
    "    ['Overall engineering data container']['Overall Mem (memory used in %)']['Memory Max (bytes)']\n",
    "    mem_max_b = compare_b[0]['Engineering Data']['Performance']['Total engineering data container']['Measurements']\\\n",
    "    ['Overall engineering data container']['Overall Mem (memory used in %)']['Memory Max (bytes)']\n",
    "    mem_txt= 'The means over time of the Memory usage are compared as \\n the std of A is {0} and the std of B is {1} which gives \\n a CV of {2}\\\n",
    "    resp. {3}. This means that the mean \\n comparison is reasonable.\\n\\\n",
    "    The maximum Memory for A is: {4} bytes \\n and for B: {5} bytes.'.format(mem_std_a,mem_std_b,cv_a,cv_b,mem_max_a,mem_max_b)\n",
    "    fig.text(.78, -.25, mem_txt, ha='center')    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    fig.set_figheight(5)\n",
    "    fig.set_figwidth(25)\n",
    "\n",
    "elif (not do_comparison) and comp_diag:\n",
    "    print(\"Below you can see the overall CPU, SWAP and Memory usage of your Benchmark. \\\n",
    "    Please note that the mean is of the corresponding measurements are shown as the usages don't change \\\n",
    "    significantly over the time of the Benchmark.\")\n",
    "    fil_used_diag = compare_a[0]['Engineering Data']['Performance']['Total engineering data container']['Nb of files']\n",
    "    print(\"For the your Benchmark the Diagnostic Toolbox ran {0} times whilst doing the experiments.\".format(fil_used_diag))\n",
    "\n",
    "    plt.rcParams.update({'font.size': 15})\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3)\n",
    "    fig.subplots_adjust(hspace=0.5)\n",
    "    fig.suptitle('Resources of your Benchmark', y=1.1)\n",
    "    objects = ('Your Benchmark')\n",
    "    x_pos = np.arange(1)\n",
    "    count = 0\n",
    "    winner_color = ['green', 'red']\n",
    "    for cont,inf in compare_a[0]['Engineering Data']['Performance']['Total engineering data container']\\\n",
    "    ['Measurements']['Overall engineering data'].items():\n",
    "        ax = axes.flatten()[count]\n",
    "        ax.bar(x_pos, inf['Mean'], align='center', alpha=0.5, color=winner_color)\n",
    "        ax.set_xticks(x_pos)\n",
    "        ax.set_xticklabels(objects)\n",
    "        ax.set_ylabel('%')\n",
    "        ax.set_title(cont)\n",
    "        bottom, top = ax.get_ylim()\n",
    "        ax.set_ylim(bottom, top+5) \n",
    "        autolabel(rec)\n",
    "        count+=1\n",
    "\n",
    "    cpu_std_a = compare_a[0]['Engineering Data']['Performance']['Total engineering data container']['Measurements']\\\n",
    "    ['Overall engineering data container']['Overall CPU (CPU used in %)']['Standard Deviation']\n",
    "    cv_a = compare_a[0]['Engineering Data']['Performance']['Total engineering data container']['Measurements']\\\n",
    "    ['Overall engineering data container']['Overall CPU (CPU used in %)']['coefficient of variation']\n",
    "    cpu_txt= 'The means  of the CPU usage are compared as the std \\n of your BM is {0} which gives \\n a CV of {1}\\\n",
    "    . This means that the mean \\n comparison is reasonable'.format(cpu_std_a,cv_a)\n",
    "    fig.text(.23, -.2, cpu_txt, ha='center')    \n",
    "\n",
    "    swaps_std_a = compare_a[0]['Engineering Data']['Performance']['Total engineering data container']['Measurements']\\\n",
    "    ['Overall engineering data container']['Overall Swaps (Swaps used in %)']['Standard Deviation']\n",
    "    cv_a = compare_a[0]['Engineering Data']['Performance']['Total engineering data container']['Measurements']\\\n",
    "    ['Overall engineering data container']['Overall Swaps (Swaps used in %)']['coefficient of variation']\n",
    "    swaps_max = compare_b[0]['Engineering Data']['Performance']['Total engineering data container']['Measurements']\\\n",
    "    ['Overall engineering data container']['Overall Swaps (Swaps used in %)']['Swaps max']\n",
    "    swaps_txt= 'The means  of the SWPAS usage are compared as the std \\n of your BM is {0} which gives \\n a CV of {1}\\\n",
    "    . This means that the mean \\n comparison is reasonable.\\n\\\n",
    "     The maximum SWAPS is: {2} bytes'.format(swaps_std_a,cv_a,swaps_max)\n",
    "    fig.text(.5, -.2, swaps_txt, ha='center')    \n",
    "\n",
    "    mem_std_a = compare_a[0]['Engineering Data']['Performance']['Total engineering data container']['Measurements']\\\n",
    "    ['Overall engineering data container']['Overall Mem (memory used in %)']['Standard Deviation']\n",
    "    cv_a = compare_a[0]['Engineering Data']['Performance']['Total engineering data container']['Measurements']\\\n",
    "    ['Overall engineering data container']['Overall Mem (memory used in %)']['coefficient of variation']\n",
    "    mem_max = compare_b[0]['Engineering Data']['Performance']['Total engineering data container']['Measurements']\\\n",
    "    ['Overall engineering data container']['Overall Mem (memory used in %)']['Memory Max (bytes)']\n",
    "    mem_txt= 'The means  of the Memory usage are compared as the std \\n of your BM is {0} which gives \\n a CV of {1}\\\n",
    "    . This means that the mean \\n comparison is reasonable.\\n\\\n",
    "     The maximum Memory is: {2} bytes'.format(mem_std_a,cv_a,mem_max)\n",
    "    \n",
    "    fig.text(.78, -.2, mem_txt, ha='center')    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    fig.set_figheight(5)\n",
    "    fig.set_figwidth(25)\n",
    "    \n",
    "else:\n",
    "    print(\"unfortunately this comparison can not be made as one of the Benchmakrks does not have this information\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nb_nodes = 0\n",
    "if do_comparison:\n",
    "    print(\"Now lets have a close look at the different nodes of the dt-core that were observed by the \\\n",
    "    Diagnostic Toolbox, for each node the two Benchmarks are compared based on the CPU usage, the number of \\\n",
    "    threads used as well as the Memory usage.\\n\\\n",
    "    We compare the means as the neither of the above mentioned resources show a significant change over the \\\n",
    "    time in which the experiments were running.\\n\\\n",
    "    The Benchmark that performed better will have a green bar and the one performing worse a red bar. If they both\\\n",
    "    performed equally they will both be colored blue\\n\\n\"\")\n",
    "else:\n",
    "    print(\"Now lets have a close look at the different nodes of the dt-core that were observed by the \\\n",
    "    Diagnostic Toolbox, for each node the CPU usage, the number of threads used as well as the Memory usage of\\\n",
    "    your Benchmark is shown.\\n\\n\")\n",
    "for node, meas in compare_a[0]['Engineering Data']['Performance']['Total engineering data node']['Measurements']['Overall engineering data node'].items():\n",
    "    if do_comparison and comp_diag:\n",
    "        plt.rcParams.update({'font.size': 15})\n",
    "        fig, axes = plt.subplots(nrows=1, ncols=3)\n",
    "        fig.subplots_adjust(hspace=0.5)\n",
    "        fig.suptitle(node, y=1.1)\n",
    "        objects = ('Benchmark A', 'Benchmark B')\n",
    "        x_pos = np.arange(len(objects))\n",
    "        count = 0\n",
    "\n",
    "\n",
    "        for cont,inf in compare_a[0]['Engineering Data']['Performance']['Total engineering data node']\\\n",
    "        ['Measurements']['Overall engineering data node'][node].items():\n",
    "            performance = [inf['Mean'],\\\n",
    "                           compare_b[0]['Engineering Data']['Performance']['Total engineering data node']\\\n",
    "                           ['Measurements']['Overall engineering data node'][node][cont]['Mean']]\n",
    "            #checks which Benchmark performed better, adds it to the corresponding advange list and colors the plots \n",
    "            #accordingly (green for the one with better performance in this property and red for the one that performed\n",
    "            #worse. If they performed equally the plots are blue.)\n",
    "            if performance[0] < performance[1]:\n",
    "                advantages.update({node+cont: 'A'})\n",
    "                advantages.update({'Node {0} '.format(nb_nodes)+cont: 'A'})\n",
    "                \n",
    "                winner_color = ['green', 'red']\n",
    "            elif performance[0] > performance[1]:\n",
    "                advantages.update({node+cont: 'B'})\n",
    "                advantages.update({'Node {0} '.format(nb_nodes)+cont: 'B'})\n",
    "                winner_color = ['red', 'green']\n",
    "            elif performance[0] == performance[1]:\n",
    "                advantages.update({node+cont: 'equal'})\n",
    "                advantages.update({'Node {0} '.format(nb_nodes)+cont: 'equal'})\n",
    "                winner_color = ['blue', 'blue']   \n",
    "            else:\n",
    "                winner_color = ['black', 'black']\n",
    "            ax = axes.flatten()[count]\n",
    "            rec = ax.bar(x_pos, performance, align='center', alpha=0.5, color=winner_color)\n",
    "            ax.set_xticks(x_pos)\n",
    "            ax.set_xticklabels(objects)\n",
    "            ax.set_ylabel('%')\n",
    "            ax.set_title(cont)\n",
    "\n",
    "            bottom, top = ax.get_ylim()\n",
    "            ax.set_ylim(bottom, top+5) \n",
    "            autolabel(rec)\n",
    "            count+=1\n",
    "\n",
    "        cpu_std_a = compare_a[0]['Engineering Data']['Performance']['Total engineering data node']['Measurements']\\\n",
    "        ['Overall engineering data node'][node]['Overall CPU (CPU used in %)']['Standard Deviation']\n",
    "        cpu_std_b = compare_b[0]['Engineering Data']['Performance']['Total engineering data node']['Measurements']\\\n",
    "        ['Overall engineering data node'][node]['Overall CPU (CPU used in %)']['Standard Deviation']\n",
    "        cv_a = compare_a[0]['Engineering Data']['Performance']['Total engineering data node']['Measurements']\\\n",
    "        ['Overall engineering data node'][node]['Overall CPU (CPU used in %)']['coefficient of variation']\n",
    "        cv_b = compare_b[0]['Engineering Data']['Performance']['Total engineering data node']['Measurements']\\\n",
    "        ['Overall engineering data node'][node]['Overall CPU (CPU used in %)']['coefficient of variation']\n",
    "        cpu_txt= 'The means over time of the CPU usage are compared as \\n the std of A is {0} and the std of B is {1} which gives \\n\\\n",
    "        a CV of {2} resp. {3}. This means that the mean \\n comparison is reasonable'.format(cpu_std_a,cpu_std_b,cv_a,cv_b)\n",
    "        fig.text(.23, -.25, cpu_txt, ha='center')\n",
    "\n",
    "        nthreads_std_a = compare_a[0]['Engineering Data']['Performance']['Total engineering data node']['Measurements']\\\n",
    "        ['Overall engineering data node'][node]['Overall NThreads']['Standard Deviation']\n",
    "        nthreads_std_b = compare_b[0]['Engineering Data']['Performance']['Total engineering data node']['Measurements']\\\n",
    "        ['Overall engineering data node'][node]['Overall NThreads']['Standard Deviation']\n",
    "        cv_a = compare_a[0]['Engineering Data']['Performance']['Total engineering data node']['Measurements']\\\n",
    "        ['Overall engineering data node'][node]['Overall NThreads']['coefficient of variation']\n",
    "        cv_b = compare_b[0]['Engineering Data']['Performance']['Total engineering data node']['Measurements']\\\n",
    "        ['Overall engineering data node'][node]['Overall NThreads']['coefficient of variation']\n",
    "        nthreads_txt= 'The means over time of the nthreads usage are compared as \\n the std of A is {0} and the std of B is {1} which gives \\n\\\n",
    "        a CV of {2} resp. {3}. This means that the mean \\n comparison is reasonable.'.format(nthreads_std_a,nthreads_std_b,cv_a,cv_b)\n",
    "        fig.text(.5, -.25, nthreads_txt, ha='center')\n",
    "\n",
    "        mem_std_a = compare_a[0]['Engineering Data']['Performance']['Total engineering data node']['Measurements']\\\n",
    "        ['Overall engineering data node'][node]['Overall PMem (memory used in %)']['Standard Deviation']\n",
    "        mem_std_b = compare_b[0]['Engineering Data']['Performance']['Total engineering data node']['Measurements']\\\n",
    "        ['Overall engineering data node'][node]['Overall PMem (memory used in %)']['Standard Deviation']\n",
    "        cv_a = compare_a[0]['Engineering Data']['Performance']['Total engineering data node']['Measurements']\\\n",
    "        ['Overall engineering data node'][node]['Overall PMem (memory used in %)']['coefficient of variation']\n",
    "        cv_b = compare_b[0]['Engineering Data']['Performance']['Total engineering data node']['Measurements']\\\n",
    "        ['Overall engineering data node'][node]['Overall PMem (memory used in %)']['coefficient of variation']\n",
    "        mem_max_a = compare_a[0]['Engineering Data']['Performance']['Total engineering data node']['Measurements']\\\n",
    "        ['Overall engineering data node'][node]['Overall PMem (memory used in %)']['Memory Max (bytes)']\n",
    "        mem_max_b = compare_b[0]['Engineering Data']['Performance']['Total engineering data node']['Measurements']\\\n",
    "        ['Overall engineering data node'][node]['Overall PMem (memory used in %)']['Memory Max (bytes)']\n",
    "        mem_txt= 'The means over time of the Memory usage are compared as \\n the std of A is {0} and the std of B is {1} which gives \\n\\\n",
    "        a CV of {2} resp. {3}. This means that the mean \\n comparison is reasonable.\\n\\\n",
    "        The maximum Memory for A is: {4} bytes \\n and for B: {5} bytes.'.format(mem_std_a,mem_std_b,cv_a,cv_b,mem_max_a,mem_max_b)\n",
    "        fig.text(.78, -.25, mem_txt, ha='center')\n",
    "\n",
    "\n",
    "        fig.set_figheight(5)\n",
    "        fig.set_figwidth(25)\n",
    "\n",
    "    elif (not do_comparison) and comp_diag:\n",
    "        plt.rcParams.update({'font.size': 15})\n",
    "        fig, axes = plt.subplots(nrows=1, ncols=3)\n",
    "        fig.subplots_adjust(hspace=0.5)\n",
    "        fig.suptitle(node, y=1.1)\n",
    "        objects = ('Your Benchmark')\n",
    "        x_pos = np.arange(1)\n",
    "        count = 0\n",
    "        winner_color = ['blue']\n",
    "        for cont,inf in compare_a[0]['Engineering Data']['Performance']['Total engineering data node']\\\n",
    "        ['Measurements']['Overall engineering data node'][node].items():\n",
    "            ax = axes.flatten()[count]\n",
    "            rec = ax.bar(x_pos, inf['Mean'], align='center', alpha=0.5, color=winner_color)\n",
    "            ax.set_xticks(x_pos)\n",
    "            ax.set_xticklabels(objects)\n",
    "            ax.set_ylabel('%')\n",
    "            ax.set_title(cont)\n",
    "            bottom, top = ax.get_ylim()\n",
    "            ax.set_ylim(bottom, top+5) \n",
    "            autolabel(rec)\n",
    "\n",
    "            count+=1\n",
    "\n",
    "        \n",
    "\n",
    "        cpu_std_a = compare_a[0]['Engineering Data']['Performance']['Total engineering data node']['Measurements']\\\n",
    "        ['Overall engineering data node'][node]['Overall CPU (CPU used in %)']['Standard Deviation']\n",
    "        cv_a = compare_a[0]['Engineering Data']['Performance']['Total engineering data node']['Measurements']\\\n",
    "        ['Overall engineering data node'][node]['Overall CPU (CPU used in %)']['coefficient of variation']\n",
    "        cpu_txt= 'The means  of the CPU usage are compared as the std \\n of your BM is {0} which gives \\n a CV of {1}\\\n",
    "        . This means that the mean \\n comparison is reasonable'.format(cpu_std_a,cv_a)\n",
    "        fig.text(.23, -.2, cpu_txt, ha='center')\n",
    "\n",
    "        nthreads_std_a = compare_a[0]['Engineering Data']['Performance']['Total engineering data node']['Measurements']\\\n",
    "        ['Overall engineering data node'][node]['Overall NThreads']['Standard Deviation']\n",
    "        cv_a = compare_a[0]['Engineering Data']['Performance']['Total engineering data node']['Measurements']\\\n",
    "        ['Overall engineering data node'][node]['Overall NThreads']['coefficient of variation']\n",
    "        nthreads_txt= 'The means  of the SWPAS usage are compared as the std \\n of your BM is {0} which gives \\n a CV of {1}\\\n",
    "        . This means that the mean \\n comparison is reasonable.'.format(nthreads_std_a,cv_a)\n",
    "        fig.text(.5, -.2, nthreads_txt, ha='center')\n",
    "\n",
    "        mem_std_a = compare_a[0]['Engineering Data']['Performance']['Total engineering data node']['Measurements']\\\n",
    "        ['Overall engineering data node'][node]['Overall PMem (memory used in %)']['Standard Deviation']\n",
    "        cv_a = compare_a[0]['Engineering Data']['Performance']['Total engineering data node']['Measurements']\\\n",
    "        ['Overall engineering data node'][node]['Overall PMem (memory used in %)']['coefficient of variation']\n",
    "        mem_max = compare_b[0]['Engineering Data']['Performance']['Total engineering data node']['Measurements']\\\n",
    "        ['Overall engineering data node'][node]['Overall PMem (memory used in %)']['Memory Max (bytes)']\n",
    "        mem_txt= 'The means  of the Memory usage are compared as the std \\n of your BM is {0} which gives \\n a CV of {1}\\\n",
    "        . This means that the mean \\n comparison is reasonable.\\n\\\n",
    "         The maximum Memory is: {2} bytes'.format(mem_std_a,cv_a,mem_max)\n",
    "\n",
    "        fig.text(.78, -.2, mem_txt, ha='center')   \n",
    "\n",
    "        \n",
    "\n",
    "        fig.set_figheight(5)\n",
    "        fig.set_figwidth(25)\n",
    "        \n",
    "    else:\n",
    "        print(\"unfortunately this comparison can not be made as one of the Benchmakrks does not have this information\")\n",
    "        \n",
    "    nb_nodes += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you see the cpu usage, nthrads respectively the memory usage of all the nodes observed in one plot to get an idea of the comparison of the resources needed by the different nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_comparison and comp_diag:\n",
    "    dif_nodes = []\n",
    "    for node, meas in compare_a[0]['Engineering Data']['Performance']['Total engineering data node']['Measurements']\\\n",
    "    ['Overall engineering data node'].items():\n",
    "        dif_nodes.append(node)\n",
    "    cpu_a = []\n",
    "    cpu_b = []\n",
    "    threads_a = []\n",
    "    threads_b = []\n",
    "    mem_a = []\n",
    "    mem_b = []\n",
    "    for nod in dif_nodes:\n",
    "        cpu_a.append(compare_a[0]['Engineering Data']['Performance']['Total engineering data node']\\\n",
    "        ['Measurements']['Overall engineering data node'][nod]['Overall CPU (CPU used in %)']['Mean'])\n",
    "        cpu_b.append(compare_b[0]['Engineering Data']['Performance']['Total engineering data node']\\\n",
    "        ['Measurements']['Overall engineering data node'][nod]['Overall CPU (CPU used in %)']['Mean'])   \n",
    "        threads_a.append(compare_a[0]['Engineering Data']['Performance']['Total engineering data node']\\\n",
    "        ['Measurements']['Overall engineering data node'][nod]['Overall NThreads']['Mean'])\n",
    "        threads_b.append(compare_b[0]['Engineering Data']['Performance']['Total engineering data node']\\\n",
    "        ['Measurements']['Overall engineering data node'][nod]['Overall NThreads']['Mean'])\n",
    "        mem_a.append(compare_a[0]['Engineering Data']['Performance']['Total engineering data node']\\\n",
    "        ['Measurements']['Overall engineering data node'][nod]['Overall PMem (memory used in %)']['Mean'])\n",
    "        mem_b.append(compare_b[0]['Engineering Data']['Performance']['Total engineering data node']\\\n",
    "        ['Measurements']['Overall engineering data node'][nod]['Overall PMem (memory used in %)']['Mean'])\n",
    "\n",
    "    tot_a = [cpu_a,threads_a,mem_a]\n",
    "    tot_b = [cpu_b,threads_b,mem_b]\n",
    "\n",
    "    dif_meas = ('Overall CPU (CPU used in %)','Overall NThreads','Overall PMem (memory used in %)')    \n",
    "    unit = ['%', 'amount', '%']\n",
    "    labels = []\n",
    "    for i in range(0,len(dif_nodes)):\n",
    "        labels.append('Node{0}'.format(i))\n",
    "\n",
    "    x = np.arange(len(labels))  # the label locations\n",
    "    width = 0.4  # the width of the bars\n",
    "\n",
    "    count = 0\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=1)\n",
    "    for i in range(0,len(dif_meas)):\n",
    "        ax = axes.flatten()[i]\n",
    "        rects1 = ax.bar(x - width/2, tot_a[i], width, label='Benchmark A')\n",
    "        rects2 = ax.bar(x + width/2, tot_b[i], width, label='Benchmark B')\n",
    "        autolabel(rects1)\n",
    "        autolabel(rects2)\n",
    "        ax.set_ylabel(unit[i])\n",
    "        ax.set_title(dif_meas[i])\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(labels)\n",
    "        ax.legend()\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    fig.set_figheight(30)\n",
    "    fig.set_figwidth(30)\n",
    "\n",
    "    plt.subplots_adjust(wspace=0, hspace=0.2)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    print(\"where:\")\n",
    "    for nod in dif_nodes:\n",
    "        print('Node 0 = '+nod)\n",
    "\n",
    "elif (not do_comparison) and comp_diag:\n",
    "    dif_nodes = []\n",
    "    for node, meas in compare_a[0]['Engineering Data']['Performance']['Total engineering data node']['Measurements']\\\n",
    "    ['Overall engineering data node'].items():\n",
    "        dif_nodes.append(node)\n",
    "    cpu_a = []\n",
    "    threads_a = []\n",
    "    mem_a = []\n",
    "    for nod in dif_nodes:\n",
    "        cpu_a.append(compare_a[0]['Engineering Data']['Performance']['Total engineering data node']\\\n",
    "        ['Measurements']['Overall engineering data node'][nod]['Overall CPU (CPU used in %)']['Mean'])\n",
    "        threads_a.append(compare_a[0]['Engineering Data']['Performance']['Total engineering data node']\\\n",
    "        ['Measurements']['Overall engineering data node'][nod]['Overall NThreads']['Mean'])\n",
    "        mem_a.append(compare_a[0]['Engineering Data']['Performance']['Total engineering data node']\\\n",
    "        ['Measurements']['Overall engineering data node'][nod]['Overall PMem (memory used in %)']['Mean'])\n",
    "\n",
    "    tot_a = [cpu_a,threads_a,mem_a]\n",
    "\n",
    "    dif_meas = ('Overall CPU (CPU used in %)','Overall NThreads','Overall PMem (memory used in %)')    \n",
    "    unit = ['%', 'amount', '%']\n",
    "    labels = []\n",
    "    for i in range(0,len(dif_nodes)):\n",
    "        labels.append('Node{0}'.format(i))\n",
    "\n",
    "    x = np.arange(len(labels))  # the label locations\n",
    "    width = 0.4  # the width of the bars\n",
    "\n",
    "    count = 0\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=1)\n",
    "    for i in range(0,len(dif_meas)):\n",
    "        ax = axes.flatten()[i]\n",
    "        rects1 = ax.bar(x, tot_a[i], width, label='Your Benchmark')\n",
    "        autolabel(rects1)\n",
    "        ax.set_ylabel(unit[i])\n",
    "        ax.set_title(dif_meas[i])\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(labels)\n",
    "        ax.legend()\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    fig.set_figheight(30)\n",
    "    fig.set_figwidth(30)\n",
    "\n",
    "    plt.subplots_adjust(wspace=0, hspace=0.2)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    print(\"where:\")\n",
    "    for i, nod in enumerate(dif_nodes):\n",
    "        print('Node ' + i + ' = '+nod)\n",
    "else:\n",
    "    print(\"unfortunately this comparison can not be made as one of the Benchmakrks does not have this information\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duckiebot Bag Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if do_comparison and comp_db_dat:\n",
    "    print(\"Below you see a comparison between the update frequency, the messages counted as well as the \\\n",
    "    number of connections of the nodes that were recorded on the Duckiebots.\\n\\\n",
    "    The Benchmark that performed better will have a green bar and the one performing worse a red bar. If they both\\\n",
    "    performed equally they will both be colored blue\\n\\n\")\n",
    "    nb_of_files_a = compare_a[0]['Engineering Data']['Performance']['Node Info']['Nb of files']\n",
    "    nb_of_nodes_a = compare_a[0]['Engineering Data']['Performance']['Node Info']['Nb of nodes']\n",
    "    nb_of_files_b = compare_b[0]['Engineering Data']['Performance']['Node Info']['Nb of files']\n",
    "    nb_of_nodes_b = compare_b[0]['Engineering Data']['Performance']['Node Info']['Nb of nodes']\n",
    "    \n",
    "    print(\"For the Benchmark A there was {0} times a bag recorded directly on the Duckiebot whilst running\\\n",
    "    the experiments compared to Benchmark B where {1} bags were recorded directly on the Duckiebot over the entire Benchmark.\".format(nb_of_files_a,nb_of_files_b))\n",
    "    \n",
    "    print(\"In the bags recorded on the Duckiebot during the Benchmark A, there were {0} nodes recorded,\\\n",
    "    whilst during the Benchmark B {1} nodes were recorded\".format(nb_of_nodes_a,nb_of_nodes_b))\n",
    "    \n",
    "    for nod,inf in compare_a[0]['Engineering Data']['Performance']['Node Info']['Measurements'].items():\n",
    "        width = 0.35\n",
    "        plt.rcParams.update({'font.size': 15})\n",
    "        fig, axes = plt.subplots(nrows=1, ncols=3)\n",
    "        fig.subplots_adjust(hspace=0.5)\n",
    "        fig.suptitle(nod, y=1.1)\n",
    "        objects = ('Benchmark A', 'Benchmark B')\n",
    "        x_pos = np.arange(len(objects))\n",
    "        count = 0\n",
    "        \n",
    "        measurements = ('Mean frequency (Hz)','Mean message_count','Mean connections')\n",
    "        \n",
    "        for mes in measurements:\n",
    "            performance = [inf[mes],\\\n",
    "                           compare_b[0]['Engineering Data']['Performance']['Node Info']['Measurements'][nod][mes]]\n",
    "            \n",
    "            if mes == 'Mean frequency (Hz)':\n",
    "                if performance[0] > performance[1]:\n",
    "                    advantages.update({nod: 'A'})\n",
    "                    advantages.update({'node {0}'.format(count): 'A'})\n",
    "                    winner_color = ['green', 'red']\n",
    "                elif performance[0] < performance[1]:\n",
    "                    advantages.update({nod: 'B'})\n",
    "                    advantages.update({'node {0}'.format(count): 'B'})\n",
    "                    winner_color = ['red', 'green']\n",
    "                elif performance[0] == performance[1]:\n",
    "                    advantages.update({nod: 'equal'})\n",
    "                    advantages.update({'node {0}'.format(count): 'equal'})\n",
    "                    winner_color = ['blue', 'blue']   \n",
    "                else:\n",
    "                    winner_color = ['black', 'black']\n",
    "            else:\n",
    "                if performance[0] > performance[1]:\n",
    "                    winner_color = ['green', 'red']\n",
    "                elif performance[0] < performance[1]:\n",
    "                    winner_color = ['red', 'green']\n",
    "                elif performance[0] == performance[1]:\n",
    "                    winner_color = ['blue', 'blue']   \n",
    "                else:\n",
    "                    winner_color = ['black', 'black']\n",
    "                           \n",
    "            ax = axes.flatten()[count]\n",
    "            rec = ax.bar(x_pos, performance, align='center', alpha=0.5, color=winner_color)\n",
    "            ax.set_xticks(x_pos)\n",
    "            ax.set_xticklabels(objects)\n",
    "            if mes == 'Mean frequency (Hz)':\n",
    "                ax.set_ylabel('Hz')\n",
    "            else:\n",
    "                ax.set_ylabel('Amount')\n",
    "            ax.set_title(mes)\n",
    "            bottom, top = ax.get_ylim()\n",
    "            ax.set_ylim(bottom, top+5) \n",
    "            autolabel(rec)\n",
    "\n",
    "\n",
    "            count+=1\n",
    "        \n",
    "        \n",
    "        fig.set_figheight(5)\n",
    "        fig.set_figwidth(25)\n",
    "        \n",
    "        \n",
    "elif (not do_comparison) and comp_db_dat:\n",
    "    print(\"Below you see the update frequency, the messages counted as well as the number of connections of \\\n",
    "    the nodes that were recorded on the Duckiebots.\\n\\n\")\n",
    "    width= 0.35\n",
    "    nb_of_files_a = compare_a[0]['Engineering Data']['Performance']['Node Info']['Nb of files']\n",
    "    nb_of_nodes_a = compare_a[0]['Engineering Data']['Performance']['Node Info']['Nb of nodes']\n",
    "    \n",
    "    for nod,inf in compare_a[0]['Engineering Data']['Performance']['Node Info']['Measurements'].items():\n",
    "        plt.rcParams.update({'font.size': 15})\n",
    "        fig, axes = plt.subplots(nrows=1, ncols=3)\n",
    "        fig.subplots_adjust(hspace=0.5)\n",
    "        fig.suptitle(nod, y=1.1)\n",
    "        objects = ('Benchmark A')\n",
    "        x_pos = np.arange(1)\n",
    "        count = 0\n",
    "        \n",
    "        measurements = ('Mean frequency (Hz)','Mean message_count','Mean connections')\n",
    "        \n",
    "        for mes in measurements:\n",
    "                           \n",
    "            ax = axes.flatten()[count]\n",
    "            rec = ax.bar(x_pos, inf[mes],width, align='center', alpha=0.5, color=winner_color)\n",
    "            ax.set_xticks(x_pos)\n",
    "            ax.set_xticklabels(objects)\n",
    "            if mes == 'Mean frequency (Hz)':\n",
    "                ax.set_ylabel('Hz')\n",
    "            else:\n",
    "                ax.set_ylabel('Amount')\n",
    "            ax.set_title(mes)\n",
    "            bottom, top = ax.get_ylim()\n",
    "            ax.set_ylim(bottom, top+5) \n",
    "            autolabel(rec)\n",
    "\n",
    "\n",
    "            count+=1\n",
    "        \n",
    "        \n",
    "        fig.set_figheight(5)\n",
    "        fig.set_figwidth(25)\n",
    "        \n",
    "else:\n",
    "    print(\"unfortunately this comparison can not be made as one of the Benchmakrks does not have this information\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you see update frequency, the messages counted as well as the number of connections of the nodes recorded in one plot to get an idea of the comparison of the performance by the different nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_comparison and comp_db_dat:\n",
    "    dif_nodes = []\n",
    "    for node, meas in  compare_a[0]['Engineering Data']['Performance']['Node Info']['Measurements'].items():\n",
    "        dif_nodes.append(node)\n",
    "    mean_freq_a = []\n",
    "    mean_freq_b = []\n",
    "    mean_mes_cnt_a = []\n",
    "    mean_mes_cnt_b = []\n",
    "    mean_con_a = []\n",
    "    mean_con_b = []\n",
    "    \n",
    "    measurements = ('Mean frequency (Hz)','Mean message_count','Mean connections')\n",
    "    unit = ['Hz', 'amount', 'amount']\n",
    "\n",
    "    \n",
    "    for nod in dif_nodes:\n",
    "        mean_freq_a.append(compare_a[0]['Engineering Data']['Performance']['Node Info']['Measurements'][nod]['Mean frequency (Hz)'])\n",
    "        mean_freq_b.append(compare_b[0]['Engineering Data']['Performance']['Node Info']['Measurements'][nod]['Mean frequency (Hz)'])   \n",
    "        mean_mes_cnt_a.append(compare_a[0]['Engineering Data']['Performance']['Node Info']['Measurements'][nod]['Mean message_count'])\n",
    "        mean_mes_cnt_b.append(compare_b[0]['Engineering Data']['Performance']['Node Info']['Measurements'][nod]['Mean message_count'])\n",
    "        mean_con_a.append(compare_a[0]['Engineering Data']['Performance']['Node Info']['Measurements'][nod]['Mean connections'])\n",
    "        mean_con_b.append(compare_b[0]['Engineering Data']['Performance']['Node Info']['Measurements'][nod]['Mean connections'])\n",
    "\n",
    "    tot_a = [mean_freq_a,mean_mes_cnt_a,mean_con_a]\n",
    "    tot_b = [mean_freq_b,mean_mes_cnt_b,mean_con_b]\n",
    "\n",
    "    labels = []\n",
    "    for i in range(0,len(dif_nodes)):\n",
    "        labels.append('Node{0}'.format(i))\n",
    "\n",
    "    x = np.arange(len(labels))  # the label locations\n",
    "    width = 0.4  # the width of the bars\n",
    "\n",
    "    count = 0\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=1)\n",
    "    for i in range(0,len(measurements)):\n",
    "        ax = axes.flatten()[i]\n",
    "        rects1 = ax.bar(x - width/2, tot_a[i], width, label='Benchmark A')\n",
    "        rects2 = ax.bar(x + width/2, tot_b[i], width, label='Benchmark B')\n",
    "        autolabel(rects1)\n",
    "        autolabel(rects2)\n",
    "        ax.set_ylabel(unit[i])\n",
    "        ax.set_title(measurements[i])\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(labels)\n",
    "        ax.legend()\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    fig.set_figheight(20)\n",
    "    fig.set_figwidth(20)\n",
    "\n",
    "    plt.subplots_adjust(wspace=0, hspace=0.2)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    print(\"where:\")\n",
    "    for nod in dif_nodes:\n",
    "        print('Node 0 = '+nod)\n",
    "\n",
    "elif (not do_comparison) and comp_db_dat:\n",
    "    dif_nodes = []\n",
    "    for node, meas in  compare_a[0]['Engineering Data']['Performance']['Node Info']['Measurements'].items():\n",
    "        dif_nodes.append(node)\n",
    "    mean_freq_a = []\n",
    "    mean_mes_cnt_a = []\n",
    "    mean_con_a = []\n",
    "    \n",
    "    measurements = ('Mean frequency (Hz)','Mean message_count','Mean connections')\n",
    "    unit = ['Hz', 'amount', 'amount']\n",
    "\n",
    "    \n",
    "    for nod in dif_nodes:\n",
    "        mean_freq_a.append(compare_a[0]['Engineering Data']['Performance']['Node Info']['Measurements'][nod]['Mean frequency (Hz)'])\n",
    "        mean_mes_cnt_a.append(compare_a[0]['Engineering Data']['Performance']['Node Info']['Measurements'][nod]['Mean message_count'])\n",
    "        mean_con_a.append(compare_a[0]['Engineering Data']['Performance']['Node Info']['Measurements'][nod]['Mean connections'])\n",
    "        \n",
    "    tot_a = [mean_freq_a,mean_mes_cnt_a,mean_con_a]\n",
    "    \n",
    "    labels = []\n",
    "    for i in range(0,len(dif_nodes)):\n",
    "        labels.append('Node{0}'.format(i))\n",
    "\n",
    "    x = np.arange(len(labels))  # the label locations\n",
    "    width = 0.4  # the width of the bars\n",
    "\n",
    "    count = 0\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=1)\n",
    "    for i in range(0,len(measurements)):\n",
    "        ax = axes.flatten()[i]\n",
    "        rects1 = ax.bar(x - width/2, tot_a[i], width, label='Your Benchmark')\n",
    "        autolabel(rects1)\n",
    "        ax.set_ylabel(unit[i])\n",
    "        ax.set_title(measurements[i])\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(labels)\n",
    "        ax.legend()\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    fig.set_figheight(20)\n",
    "    fig.set_figwidth(20)\n",
    "\n",
    "    plt.subplots_adjust(wspace=0, hspace=0.2)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    print(\"where:\")\n",
    "    for nod in dif_nodes:\n",
    "        print('Node 0 = '+nod)\n",
    "        \n",
    "else:\n",
    "    print(\"unfortunately this comparison can not be made as one of the Benchmakrks does not have this information\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if do_comparison and comp_db_dat:\n",
    "    print(\"Below the Latencies and number of segments counted of both the Benchmarks are compared to each other.\\n\\\n",
    "    The Benchmark that performed better will have a green bar and the one performing worse a red bar. If they both\\\n",
    "    performed equally they will both be colored blue\\n\\n\")\n",
    "    nb_of_lat_a = compare_a[0]['Engineering Data']['Performance']['Latency']['Nb of latency meas. per file']\n",
    "    nb_of_lat_b = compare_b[0]['Engineering Data']['Performance']['Latency']['Nb of latency meas. per file']\n",
    "    nb_of_seg_cnt_a = compare_a[0]['Engineering Data']['Performance']['Segment Count']['Nb of segment count meas. per file']\n",
    "    nb_of_seg_cnt_b = compare_b[0]['Engineering Data']['Performance']['Segment Count']['Nb of segment count meas. per file']\n",
    "    \n",
    "    \n",
    "    plt.rcParams.update({'font.size': 15})\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2)\n",
    "    fig.subplots_adjust(hspace=0.5)\n",
    "    fig.suptitle('Latency and segment count', y=1.1)\n",
    "    objects = ('Benchmark A', 'Benchmark B')\n",
    "    x_pos = np.arange(len(objects))\n",
    "    count = 0\n",
    "\n",
    "    measurements = ('Latency','Segment Count')\n",
    "    for prop in measurements:\n",
    "        if prop == 'Latency':\n",
    "            print(\"Latency:\")\n",
    "            print(\"In the bags recorded on the Duckiebot during the Benchmark A, the latency was measured {0} times  ,\\\n",
    "            whilst during the Benchmark B the latency was measured {1} times.\\n\".format(nb_of_lat_a,nb_of_lat_b))\n",
    "            performance = [compare_a[0]['Engineering Data']['Performance'][prop]['Measurements']['Overall mean latency (ms)'],\\\n",
    "                           compare_b[0]['Engineering Data']['Performance'][prop]['Measurements']['Overall mean latency (ms)']]\n",
    "            if performance[0] < performance[1]:\n",
    "                advantages.update({prop: 'A'})\n",
    "                winner_color = ['green', 'red']\n",
    "            elif performance[0] > performance[1]:\n",
    "                advantages.update({prop: 'B'})\n",
    "                winner_color = ['red', 'green']\n",
    "            elif performance[0] == performance[1]:\n",
    "                advantages.update({prop: 'equal'})\n",
    "                winner_color = ['blue', 'blue']   \n",
    "            else:\n",
    "                winner_color = ['black', 'black']\n",
    "            \n",
    "            std_a = compare_a[0]['Engineering Data']['Performance'][prop]['Measurements']['Overall std latency (ms)']\n",
    "            std_b = compare_b[0]['Engineering Data']['Performance'][prop]['Measurements']['Overall std latency (ms)']\n",
    "            cv_a = compare_a[0]['Engineering Data']['Performance'][prop]['Measurements']['Overall CV latency']\n",
    "            cv_b = compare_b[0]['Engineering Data']['Performance'][prop]['Measurements']['Overall CV latency']\n",
    "            min_a = compare_a[0]['Engineering Data']['Performance'][prop]['Measurements']['Overall min latency (ms)']\n",
    "            max_a = compare_a[0]['Engineering Data']['Performance'][prop]['Measurements']['Overall max latency(ms)']\n",
    "            min_b = compare_b[0]['Engineering Data']['Performance'][prop]['Measurements']['Overall min latency (ms)']\n",
    "            max_b = compare_b[0]['Engineering Data']['Performance'][prop]['Measurements']['Overall max latency(ms)']\n",
    "            \n",
    "            txt_a = \"The standard deviation of the latency measurde in the Benchmark A is {0} ms, this leads to a coefficient of variation of {1}.\\n The minimal latency ever measured was {2} ms where as the maximal latency was {3} ms\".format(std_a,cv_a,min_a,max_a)\n",
    "            txt_b = \"The standard deviation of the latency measurde in the Benchmark B is {0} ms, this leads to a coefficient of variation of {1}.\\n The minimal latency ever measured was {2} ms where as the maximal latency was {3} ms\".format(std_b,cv_b,min_b,max_b)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            ax = axes.flatten()[count]\n",
    "            rec = ax.bar(x_pos, performance, align='center', alpha=0.5, color=winner_color)\n",
    "            ax.set_xticks(x_pos)\n",
    "            ax.set_xticklabels(objects)\n",
    "            if prop == 'Latency':\n",
    "                ax.set_ylabel('ms')\n",
    "            else:\n",
    "                ax.set_ylabel('amount')\n",
    "            ax.set_title(prop)\n",
    "            bottom, top = ax.get_ylim()\n",
    "            ax.set_ylim(bottom, top+5) \n",
    "            autolabel(rec)\n",
    "\n",
    "\n",
    "            count+=1\n",
    "\n",
    "\n",
    "            fig.set_figheight(5)\n",
    "            fig.set_figwidth(25)\n",
    "            \n",
    "            print(txt_a+'\\n')\n",
    "            print(txt_b+'\\n')\n",
    "            print('\\n')\n",
    "            \n",
    "            \n",
    "            print('Latency[ms]\\tBenchmark A\\tBenchmark B\\tdifference\\tB relative to A\\t\\tWinner')\n",
    "            print('-----------------------------------------------------------------------------------------------')\n",
    "            win,brela,diff,sign = comp_low_bet(performance[0],performance[1])\n",
    "            print('Mean' + ':\\t\\t', float(np.round(performance[0],1)), '\\t\\t', float(np.round(performance[1],1)), '\\t\\t',float(np.round(diff,1)), '\\t\\t',float(np.round(brela,1)),'\\t\\t\\t',win)\n",
    "            win,brela,diff,sign = comp_low_bet(std_a,std_b)\n",
    "            print('Std' + ':\\t\\t', float(np.round(std_a,1)), '\\t\\t', float(np.round(std_b,1)), '\\t\\t',float(np.round(diff,1)), '\\t\\t',float(np.round(brela,1)),'\\t\\t\\t',win)\n",
    "            win,brela,diff,sign = comp_low_bet(min_a,min_b)\n",
    "            print('Min' + ':\\t\\t', float(np.round(min_a,1)), '\\t\\t', float(np.round(min_b,1)), '\\t\\t',float(np.round(diff,1)), '\\t\\t',float(np.round(brela,1)),'\\t\\t\\t',win)\n",
    "            win,brela,diff,sign = comp_low_bet(max_a,max_b)\n",
    "            print('Max' + ':\\t\\t', float(np.round(max_a,1)), '\\t\\t', float(np.round(max_b,1)), '\\t\\t',float(np.round(diff,1)), '\\t\\t',float(np.round(brela,1)),'\\t\\t\\t',win)\n",
    "\n",
    "            \n",
    "            \n",
    "        # if not latency it is the Segment Count    \n",
    "        else:\n",
    "            print('\\n')\n",
    "            print('\\n')\n",
    "            print(\"Segments counted:\")\n",
    "            print(\"In the bags recorded on the Duckiebot during the Benchmark A, there was {0} times a segment count measured, whilst during the Benchmark B the segment count was measured {1} times\".format(nb_of_seg_cnt_a,nb_of_seg_cnt_b))\n",
    "            performance = [compare_a[0]['Engineering Data']['Performance'][prop]['Measurements']['Overall mean segment count'],\\\n",
    "                           compare_b[0]['Engineering Data']['Performance'][prop]['Measurements']['Overall mean segment count']]\n",
    "        \n",
    "            if performance[0] > performance[1]:\n",
    "                advantages.update({prop: 'A'})\n",
    "                winner_color = ['green', 'red']\n",
    "            elif performance[0] < performance[1]:\n",
    "                advantages.update({prop: 'B'})\n",
    "                winner_color = ['red', 'green']\n",
    "            elif performance[0] == performance[1]:\n",
    "                advantages.update({prop: 'equal'})\n",
    "                winner_color = ['blue', 'blue']   \n",
    "            else:\n",
    "                winner_color = ['black', 'black']\n",
    "                \n",
    "            std_a = compare_a[0]['Engineering Data']['Performance'][prop]['Measurements']['Overall std segment count']\n",
    "            std_b = compare_b[0]['Engineering Data']['Performance'][prop]['Measurements']['Overall std segment count']\n",
    "            cv_a = compare_a[0]['Engineering Data']['Performance'][prop]['Measurements']['Overall CV segment count']\n",
    "            cv_b = compare_b[0]['Engineering Data']['Performance'][prop]['Measurements']['Overall CV segment count']\n",
    "            min_a = compare_a[0]['Engineering Data']['Performance'][prop]['Measurements']['Overall min segment count']\n",
    "            max_a = compare_a[0]['Engineering Data']['Performance'][prop]['Measurements']['Overall max segment count']\n",
    "            min_b = compare_b[0]['Engineering Data']['Performance'][prop]['Measurements']['Overall min segment count']\n",
    "            max_b = compare_b[0]['Engineering Data']['Performance'][prop]['Measurements']['Overall max segment count']\n",
    "            \n",
    "            txt_a = \"The standard deviation of the segments counted in the Benchmark A is {0}, this leads to a coefficient of variation of {1}.\\n The minimal number of counted ever counted was {2} where as the maximal number counted was {3}\".format(std_a,cv_a,min_a,max_a)\n",
    "            txt_b = \"The standard deviation of the segments counted in the Benchmark B is {0}, this leads to a coefficient of variation of {1}.\\n The minimal number of counted ever counted was {2} where as the maximal number counted was {3}\".format(std_b,cv_b,min_b,max_b)\n",
    "            \n",
    "\n",
    "            ax = axes.flatten()[count]\n",
    "            rec = ax.bar(x_pos, performance, align='center', alpha=0.5, color=winner_color)\n",
    "            ax.set_xticks(x_pos)\n",
    "            ax.set_xticklabels(objects)\n",
    "            if prop == 'Latency':\n",
    "                ax.set_ylabel('ms')\n",
    "            else:\n",
    "                ax.set_ylabel('amount')\n",
    "            ax.set_title(prop)\n",
    "            bottom, top = ax.get_ylim()\n",
    "            ax.set_ylim(bottom, top+5) \n",
    "            autolabel(rec)\n",
    "            \n",
    "            count+=1\n",
    "\n",
    "\n",
    "            fig.set_figheight(5)\n",
    "            fig.set_figwidth(25)\n",
    "            \n",
    "            print('\\n')\n",
    "            print(txt_a+'\\n')\n",
    "            print(txt_b+'\\n')\n",
    "            print('\\n')\n",
    "            \n",
    "            \n",
    "            print('Segments Counted\\tBenchmark A\\tBenchmark B\\tdifference\\tB relative to A\\t\\tWinner')\n",
    "            print('-----------------------------------------------------------------------------------------------------')\n",
    "            win,brela,diff,sign = comp_high_bet(performance[0],performance[1])\n",
    "            print('Mean' + ':\\t\\t\\t', float(np.round(performance[0],1)), '\\t\\t', float(np.round(performance[1],1)), '\\t\\t',float(np.round(diff,1)), '\\t\\t',float(np.round(brela,1)),'\\t\\t\\t',win)\n",
    "            win,brela,diff,sign = comp_low_bet(std_a,std_b)\n",
    "            print('Std' + ':\\t\\t\\t', float(np.round(std_a,1)), '\\t\\t', float(np.round(std_b,1)), '\\t\\t',float(np.round(diff,1)), '\\t\\t',float(np.round(brela,1)),'\\t\\t\\t',win)\n",
    "            win,brela,diff,sign = comp_high_bet(min_a,min_b)\n",
    "            print('Min' + ':\\t\\t\\t', float(np.round(min_a,1)), '\\t\\t', float(np.round(min_b,1)), '\\t\\t',float(np.round(diff,1)), '\\t\\t',float(np.round(brela,1)),'\\t\\t\\t',win)\n",
    "            win,brela,diff,sign = comp_high_bet(max_a,max_b)\n",
    "            print('Max' + ':\\t\\t\\t', float(np.round(max_a,1)), '\\t\\t', float(np.round(max_b,1)), '\\t\\t',float(np.round(diff,1)), '\\t\\t',float(np.round(brela,1)),'\\t\\t\\t',win)\n",
    "            \n",
    "            print('\\n')\n",
    "            print('\\n')\n",
    "            \n",
    "\n",
    "\n",
    "elif (not do_comparison) and comp_db_dat:\n",
    "    print(\"Below the Latencies and number of segments counted of your Benchmark is shown.\\n\\n\")\n",
    "    nb_of_lat_a = compare_a[0]['Engineering Data']['Performance']['Latency']['Nb of latency meas. per file']\n",
    "    nb_of_seg_cnt_a = compare_a[0]['Engineering Data']['Performance']['Segment Count']['Nb of segment count meas. per file']\n",
    "    \n",
    "    plt.rcParams.update({'font.size': 15})\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2)\n",
    "    fig.subplots_adjust(hspace=0.5)\n",
    "    fig.suptitle('Latency and segment count', y=1.1)\n",
    "    objects = ('Benchmark A')\n",
    "    x_pos = np.arange(1)\n",
    "    count = 0\n",
    "    width = 0.35\n",
    "    measurements = ('Latency','Segment Count')\n",
    "    for prop in measurements:\n",
    "        if prop == 'Latency':\n",
    "            print(\"Latency:\")\n",
    "            print(\"In the bags recorded on the Duckiebot during your Benchmark, the latency was measured {0} times.\\n\".format(nb_of_lat_a))\n",
    "            mean_a = compare_a[0]['Engineering Data']['Performance'][prop]['Measurements']['Overall mean latency (ms)']\n",
    "           \n",
    "            \n",
    "            std_a = compare_a[0]['Engineering Data']['Performance'][prop]['Measurements']['Overall std latency (ms)']\n",
    "            cv_a = compare_a[0]['Engineering Data']['Performance'][prop]['Measurements']['Overall CV latency']\n",
    "            min_a = compare_a[0]['Engineering Data']['Performance'][prop]['Measurements']['Overall min latency (ms)']\n",
    "            max_a = compare_a[0]['Engineering Data']['Performance'][prop]['Measurements']['Overall max latency(ms)']\n",
    "            \n",
    "            txt_a = \"The standard deviation of the latency measurde in your Benchmark is {0} ms, this leads to a coefficient of variation of {1}.\\n The minimal latency ever measured was {2} ms where as the maximal latency was {3} ms\".format(std_a,cv_a,min_a,max_a)\n",
    "            \n",
    "            ax = axes.flatten()[count]\n",
    "            rec = ax.bar(x_pos, mean_a,width, align='center', alpha=0.5, color=winner_color)\n",
    "            ax.set_xticks(x_pos)\n",
    "            ax.set_xticklabels(objects)\n",
    "            if prop == 'Latency':\n",
    "                ax.set_ylabel('ms')\n",
    "            else:\n",
    "                ax.set_ylabel('amount')\n",
    "            ax.set_title(prop)\n",
    "            bottom, top = ax.get_ylim()\n",
    "            ax.set_ylim(bottom, top+5) \n",
    "            autolabel(rec)\n",
    "\n",
    "\n",
    "            count+=1\n",
    "\n",
    "\n",
    "            fig.set_figheight(5)\n",
    "            fig.set_figwidth(25)\n",
    "            \n",
    "            print(txt_a+'\\n')\n",
    "            \n",
    "            \n",
    "            print('Latency[ms]\\tYour Benchmark')\n",
    "            print('-----------------------------------------------------------------------------------------------')\n",
    "            print('Mean' + ':\\t\\t', float(np.round(mean_a,1)))\n",
    "            print('Std' + ':\\t\\t', float(np.round(std_a,1)))\n",
    "            print('Min' + ':\\t\\t', float(np.round(min_a,1)))\n",
    "            print('Max' + ':\\t\\t', float(np.round(max_a,1)))\n",
    "\n",
    "            \n",
    "            \n",
    "        # if not latency it is the Segment Count    \n",
    "        else:\n",
    "            print('\\n')\n",
    "            print('\\n')\n",
    "            print(\"Segments counted:\")\n",
    "            print(\"In the bags recorded on the Duckiebot during your Benchmark, there was {0} times a segment count measured.\".format(nb_of_seg_cnt_a))\n",
    "            mean_a = compare_a[0]['Engineering Data']['Performance'][prop]['Measurements']['Overall mean segment count']\n",
    "                           \n",
    "            std_a = compare_a[0]['Engineering Data']['Performance'][prop]['Measurements']['Overall std segment count']\n",
    "            cv_a = compare_a[0]['Engineering Data']['Performance'][prop]['Measurements']['Overall CV segment count']\n",
    "            min_a = compare_a[0]['Engineering Data']['Performance'][prop]['Measurements']['Overall min segment count']\n",
    "            max_a = compare_a[0]['Engineering Data']['Performance'][prop]['Measurements']['Overall max segment count']\n",
    "            \n",
    "            txt_a = \"The standard deviation of the segments counted in your Benchmark is {0}, this leads to a coefficient of variation of {1}.\\n The minimal number of counted ever counted was {2} where as the maximal number counted was {3}\".format(std_a,cv_a,min_a,max_a)\n",
    "            \n",
    "            ax = axes.flatten()[count]\n",
    "            rec = ax.bar(x_pos, performance, align='center', alpha=0.5, color=winner_color)\n",
    "            ax.set_xticks(x_pos)\n",
    "            ax.set_xticklabels(objects)\n",
    "            if prop == 'Latency':\n",
    "                ax.set_ylabel('ms')\n",
    "            else:\n",
    "                ax.set_ylabel('amount')\n",
    "            ax.set_title(prop)\n",
    "            bottom, top = ax.get_ylim()\n",
    "            ax.set_ylim(bottom, top+5) \n",
    "            autolabel(rec)\n",
    "            \n",
    "            count+=1\n",
    "\n",
    "\n",
    "            fig.set_figheight(5)\n",
    "            fig.set_figwidth(25)\n",
    "            print(txt_a+'\\n')\n",
    "            print('\\n')\n",
    "            \n",
    "            \n",
    "            print('Segments Counted\\tYour Benchmark')\n",
    "            print('-----------------------------------------------------------------------------------------------')\n",
    "            print('Mean' + ':\\t\\t\\t', float(np.round(mean_a,1)))\n",
    "            print('Std' + ':\\t\\t\\t', float(np.round(std_a,1)))\n",
    "            print('Min' + ':\\t\\t\\t', float(np.round(min_a,1)))\n",
    "            print('Max' + ':\\t\\t\\t', float(np.round(max_a,1)))\n",
    "            \n",
    "            print('\\n')\n",
    "            print('\\n')\n",
    "        \n",
    "else:\n",
    "    print(\"unfortunately this comparison can not be made as one of the Benchmakrks does not have this information\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Behaviour Performance\n",
    "\n",
    "Now lets have a look at the fun part, the actual performance of the Benchmarks.\n",
    "First we plot the mean trajectory of all experiments ran for the two Benchmarks that the Duckiebot was driving. As well as all the different trajectories in differen colors. This will help you already from the eye to see the difference in performance.\n",
    "Further all the different performances are calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some lists used for the analysis\n",
    "tolerances = ['Tolerance_out_of_sight [seconds]', 'Tolerance_too_slow_sec [seconds]', 'Theoretical_length_of_benchmark [seconds]']\n",
    "no_meaningful_comp = ['Tolerance_out_of_sight', 'Tolerance_too_slow_sec', 'Theoretical_length_of_benchmark']\n",
    "no_meaningful_rel_comp = ['Out_of_sight', 'Too_slow']\n",
    "boolean_comp = ['Out_of_sight', 'Too_slow']\n",
    "time_comp = ['Time_out_of_sight', 'Time_too_slow','Actual_length_of_benchmark']\n",
    "list_of_comp_todo = ['Position_too_slow', 'Time_needed_per_straight_tile_sec', 'Time_needed_per_curved_tile']\n",
    "meaningful_comp_results = ['Number_of_tiles_covered','Avg_time_needed_per_tile','mean_diff_btw_estimation_and_ground_truth_offset',\\\n",
    "                          'mean_diff_btw_estimation_and_ground_truth_angle', 'Abs_Measurements_db_mean_offset',\\\n",
    "                          'Abs_Measurements_db_mean_angle','Abs_Ground_truth_wt_mean_offset_interp',\\\n",
    "                          'Abs_Ground_truth_wt_mean_angle_interp','Abs_Ground_truth_wt_mean_offset_non_interp',\\\n",
    "                          'Abs_Ground_truth_wt_mean_angle_non_interp','Time_out_of_sight', 'Time_too_slow',\\\n",
    "                           'Actual_length_of_benchmark']\n",
    "high_better = ['Number_of_tiles_covered','Time_out_of_sight','Time_too_slow','Actual_length_of_benchmark']\n",
    "low_better = ['Avg_time_needed_per_tile','mean_diff_btw_estimation_and_ground_truth_offset',\\\n",
    "              'mean_diff_btw_estimation_and_ground_truth_angle', 'Abs_Measurements_db_mean_offset',\\\n",
    "              'Abs_Measurements_db_mean_angle','Abs_Ground_truth_wt_mean_offset_interp',\\\n",
    "              'Abs_Ground_truth_wt_mean_angle_interp','Abs_Ground_truth_wt_mean_offset_non_interp',\\\n",
    "              'Abs_Ground_truth_wt_mean_angle_non_interp']\n",
    "\n",
    "high_prio_beh = ['Abs_Ground_truth_wt_mean_offset_non_interp','Abs_Ground_truth_wt_mean_angle_non_interp','Out_of_sight', 'Too_slow']\n",
    "low_prio_beh = ['Avg_time_needed_per_tile','Actual_length_of_benchmark']\n",
    "medium_prio_beh = ['mean_diff_btw_estimation_and_ground_truth_offset','mean_diff_btw_estimation_and_ground_truth_angle','Number_of_completed_laps']\n",
    "\n",
    "high_prio_eng = ['Latency','node 0', 'node 1', 'node 2', 'Overall CPU (CPU used in %)']\n",
    "low_prio_eng = []\n",
    "medium_prio_eng = ['Overall Mem (memory used in %)','Overall Swaps (Swaps used in %)']\n",
    "\n",
    "for i in range(0, nb_nodes):\n",
    "    high_prio_eng.append('Node {0} Overall CPU (CPU used in %)'.format(i))\n",
    "    low_prio_eng.append('Node {0} Overall NThreads'.format(i))\n",
    "    low_prio_eng.append('Node {0} Overall PMem (memory used in %)'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_comparison:\n",
    "    for item in tolerances:\n",
    "        tol_a = compare_a[0]['Results'][item]\n",
    "        tol_b = compare_a[0]['Results'][item]\n",
    "        print(\"note that in the Benchmark A the constant \" +item+ \" was set to {0} and in the Benchmark B it was set to {1}\".format(tol_a,tol_b))\n",
    "else:  \n",
    "    for item in tolerances:\n",
    "        tol_a = compare_a[0]['Results'][item]\n",
    "        print(\"note that in the your Benchmark the constant \" +item+ \" was set to {0}.\".format(tol_a,tol_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_trajectories_a = [] \n",
    "all_trajectories_b = [] \n",
    "\n",
    "#extract all trajectories of Benchmark A and B into seperate lists\n",
    "for j,traj in enumerate(compare_a[0]['Results']['all_trajectories']['All results']):\n",
    "    all_cur_traj = []\n",
    "    for i in range(0, len(compare_a[0]['Results']['all_trajectories']['All results'][j])):\n",
    "        cur_traj = np.array(compare_a[0]['Results']['all_trajectories']['All results'][j][i])\n",
    "        all_cur_traj.append(cur_traj)\n",
    "    all_trajectories_a.append(all_cur_traj)\n",
    "    \n",
    "if do_comparison:\n",
    "    for j,traj in enumerate(compare_b[0]['Results']['all_trajectories']['All results']):\n",
    "        all_cur_traj = []\n",
    "        for i in range(0, len(compare_b[0]['Results']['all_trajectories']['All results'][j])):\n",
    "            cur_traj = np.array(compare_b[0]['Results']['all_trajectories']['All results'][j][i])\n",
    "            all_cur_traj.append(cur_traj)\n",
    "        all_trajectories_b.append(all_cur_traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    del m    \n",
    "except:\n",
    "    pass\n",
    "m = dw.load_map('linus_loop')\n",
    "\n",
    "\n",
    "used_lane_segments_list_a, lane_segments_SE2_a = get_used_lanes(all_trajectories_a)\n",
    "if do_comparison:\n",
    "    used_lane_segments_list_b, lane_segments_SE2_b = get_used_lanes(all_trajectories_b)\n",
    "\n",
    "\n",
    "mid = 30 \n",
    "cnt = collections.Counter()\n",
    "for x in used_lane_segments_list_a:\n",
    "    cnt[x[2]] +=1\n",
    "\n",
    "# Number of interpolation points of each tile (approximation, need to do it properly)\n",
    "pts_per_segment = {\n",
    "    'short': int(mid*1/8*math.pi),\n",
    "    'mid': (mid),\n",
    "    'long': int(mid*3/8*math.pi),\n",
    "}\n",
    "\n",
    "# Compute the center line that we will use to resample\n",
    "center_line_global_a, center_line_global_tfs_a = get_global_center_line(m,\n",
    "                                                                    used_lane_segments_list_a,\n",
    "                                                                    lane_segments_SE2_a,\n",
    "                                                                    pts_per_segment)\n",
    "if do_comparison:\n",
    "    center_line_global_b, center_line_global_tfs_b = get_global_center_line(m,\n",
    "                                                                        used_lane_segments_list_b,\n",
    "                                                                        lane_segments_SE2_b,\n",
    "                                                                        pts_per_segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Base transform if the plotting map is not the same as the evaluation map (i.e. plotting a subset \n",
    "# of a large map containing muliple loops)\n",
    "base_transform = np.linalg.inv(geo.SE2_from_translation_angle([0.585 * 0, 0.0], 0))\n",
    "\n",
    "all_traj_tfs_a = []\n",
    "all_traj_tfs_b = []\n",
    "all_int_traj_test_a = {}\n",
    "all_int_traj_test_b = {}\n",
    "\n",
    "# transform the trajectory data\n",
    "for i,traj in enumerate(all_trajectories_a):\n",
    "    int_tfs_traj = []\n",
    "    for el in traj:\n",
    "        if el is not None:\n",
    "            int_tfs_traj.append(dw.SE2Transform.from_SE2(geo.SE2.multiply(base_transform, el)))\n",
    "        else:\n",
    "            int_tfs_traj.append(None)\n",
    "    all_traj_tfs_a.append(int_tfs_traj)\n",
    "\n",
    "all_traj_tfs_a = np.asarray(all_traj_tfs_a).T.tolist()\n",
    "    \n",
    "if do_comparison:\n",
    "    for i,traj in enumerate(all_trajectories_b):\n",
    "        int_tfs_traj = []\n",
    "        for el in traj:\n",
    "            if el is not None:\n",
    "                int_tfs_traj.append(dw.SE2Transform.from_SE2(geo.SE2.multiply(base_transform, el)))\n",
    "            else:\n",
    "                int_tfs_traj.append(None)\n",
    "        all_traj_tfs_b.append(int_tfs_traj)\n",
    "    \n",
    "\n",
    "    all_traj_tfs_b = np.asarray(all_traj_tfs_b).T.tolist()\n",
    "\n",
    "\n",
    "# interpolate the trajectory data of all trajectories of both Benchmarks\n",
    "for i,traj in enumerate(all_trajectories_a):\n",
    "    cur_traj = []\n",
    "    cur_traj.append(traj)\n",
    "    cur_int_trajs = get_interpolated_points(center_line_global_a, cur_traj)\n",
    "    all_int_tfs = []\n",
    "    int_tfs_traj = []\n",
    "\n",
    "    for k,traj_i in enumerate(cur_int_trajs):\n",
    "        for el in traj_i:\n",
    "            if el is not None:\n",
    "                int_tfs_traj.append(dw.SE2Transform.from_SE2(geo.SE2.multiply(base_transform, el)))\n",
    "            else:\n",
    "                int_tfs_traj.append(None)\n",
    "    \n",
    "    all_int_tfs.append(int_tfs_traj)\n",
    "        \n",
    "    all_int_traj_test_a.update({i: int_tfs_traj})\n",
    "\n",
    "int_trajs_a = get_interpolated_points(center_line_global_a, all_trajectories_a)\n",
    "\n",
    "if do_comparison:\n",
    "    for i,traj in enumerate(all_trajectories_b):\n",
    "        cur_traj = []\n",
    "        cur_traj.append(traj)\n",
    "        cur_int_trajs = get_interpolated_points(center_line_global_b, cur_traj)\n",
    "        all_int_tfs = []\n",
    "        int_tfs_traj = []\n",
    "        for k,traj_i in enumerate(cur_int_trajs):\n",
    "            for el in traj_i:\n",
    "                if el is not None:\n",
    "                    int_tfs_traj.append(dw.SE2Transform.from_SE2(geo.SE2.multiply(base_transform, el)))\n",
    "                else:\n",
    "                    int_tfs_traj.append(None)\n",
    "        all_int_tfs.append(int_tfs_traj)\n",
    "\n",
    "        all_int_traj_test_b.update({i: int_tfs_traj})\n",
    "    \n",
    "    int_trajs_b = get_interpolated_points(center_line_global_b, all_trajectories_b)\n",
    "\n",
    "all_int_tfs_a = []\n",
    "for i,traj in enumerate(int_trajs_a):\n",
    "    int_tfs_traj = []\n",
    "    for el in traj:\n",
    "        if el is not None:\n",
    "            int_tfs_traj.append(dw.SE2Transform.from_SE2(geo.SE2.multiply(base_transform, el)))\n",
    "        else:\n",
    "            int_tfs_traj.append(None)\n",
    "    all_int_tfs_a.append(int_tfs_traj)\n",
    "\n",
    "    \n",
    "# Get trajectory statistics\n",
    "mean_tfs_a, std_y_a, std_angle_a, start_idx_a, end_idx_a, cv_y_a, cv_headin_ag, mean_y_a, mean_heading_a \\\n",
    "= get_trajectories_statistics(all_int_tfs_a,center_line_global_tfs_a)\n",
    "\n",
    "if do_comparison:\n",
    "    all_int_tfs_b = []\n",
    "    for i,traj in enumerate(int_trajs_b):\n",
    "        int_tfs_traj = []\n",
    "        for el in traj:\n",
    "            if el is not None:\n",
    "                int_tfs_traj.append(dw.SE2Transform.from_SE2(geo.SE2.multiply(base_transform, el)))\n",
    "            else:\n",
    "                int_tfs_traj.append(None)\n",
    "        all_int_tfs_b.append(int_tfs_traj)\n",
    "    \n",
    "    mean_tfs_b, std_y_b, std_angle_b, start_idx_b, end_idx_b, cv_y_b, cv_heading_b, mean_y_b, mean_heading_b \\\n",
    "    = get_trajectories_statistics(all_int_tfs_b,center_line_global_tfs_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load the map and show the map with all the trajectories recorded as well as its mean for both the Benchmarks as \n",
    "# well as the comparison between the means\n",
    "\n",
    "del m\n",
    "m = dw.load_map(Map_Name)\n",
    "\n",
    "if do_comparison:\n",
    "    print(\"Trajectories of Benchmark A:\")\n",
    "else:\n",
    "    print(\"Trajectories of your Benchmark\")\n",
    "\n",
    "colors = ['green','cyan','blue','yellow','magenta']\n",
    "for k in range(0, compare_a[0]['Number of tests ran']):\n",
    "    current_traj_tfs = all_int_traj_test_a[k]\n",
    "    for i, meant_tf in enumerate(current_traj_tfs):\n",
    "        if not(i%2):\n",
    "            if current_traj_tfs[i] != None:\n",
    "                a = k%5\n",
    "                m.set_object('Data-%s'% k+str(i + 10000), Circle(0.008, color=colors[a]), ground_truth=meant_tf)\n",
    "    print('Traj {0}'.format(k) + ': ' + colors[a])\n",
    "for i, meant_tf in enumerate(mean_tfs_a):\n",
    "    if not(i%2):\n",
    "        m.set_object(str(i), Circle(0.015, color='red'), ground_truth=meant_tf)\n",
    "\n",
    "ipython_draw_svg(m);\n",
    "\n",
    "\n",
    "\n",
    "if do_comparison:\n",
    "    del m\n",
    "    m = dw.load_map(Map_Name)\n",
    "    print(\"Trajectories of Benchmark B:\")\n",
    "    for k in range(0, compare_b[0]['Number of tests ran']):\n",
    "        current_traj_tfs = all_int_traj_test_b[k]\n",
    "        for i, meant_tf in enumerate(current_traj_tfs):\n",
    "            if not(i%2):\n",
    "                if current_traj_tfs[i] != None:\n",
    "                    a = k%5\n",
    "                    m.set_object('Data-%s'% k+str(i + 10000), Circle(0.008, color=colors[a]), ground_truth=meant_tf)\n",
    "        print('Traj {0}'.format(k) + ': ' + colors[a])\n",
    "        \n",
    "    for i, meant_tf in enumerate(mean_tfs_b):\n",
    "        if not(i%2):\n",
    "            m.set_object(str(i), Circle(0.015, color='red'), ground_truth=meant_tf)\n",
    "\n",
    "    ipython_draw_svg(m);\n",
    "    \n",
    "    del m\n",
    "    m = dw.load_map(Map_Name)\n",
    "    \n",
    "    for i, meant_tf in enumerate(mean_tfs_a):\n",
    "        if not(i%2):\n",
    "            m.set_object(str(i+1000), Circle(0.015, color='blue'), ground_truth=meant_tf)\n",
    "\n",
    "    for i, meant_tf in enumerate(mean_tfs_b):\n",
    "        if not(i%2):\n",
    "            m.set_object(str(i), Circle(0.015, color='red'), ground_truth=meant_tf)\n",
    "    \n",
    "    print(\"Mean Trajectories of the two Benchmarks, where the Benchmark A is blue and the Benchmark B is red\")\n",
    "    ipython_draw_svg(m);\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Extract and analyse the number of times the Duckiebot finished early due too crashing or driving too slow or out\n",
    "# of sight\n",
    "fail_time_a = {}\n",
    "time_slow_a = []\n",
    "time_out_of_sight_a = []\n",
    "for i, ind in enumerate(compare_a[0]['Results']['Too_slow']['All results']):\n",
    "    if ind:\n",
    "        time_slow_a.append(compare_a[0]['Results']['Time_too_slow']['All results'][i])\n",
    "\n",
    "for i, ind in enumerate(compare_a[0]['Results']['Out_of_sight']['All results']):\n",
    "    if ind:\n",
    "        time_out_of_sight_a.append(compare_a[0]['Results']['Time_out_of_sight']['All results'][i])\n",
    "\n",
    "fail_time_a.update({'Out_of_sight':time_out_of_sight_a, 'Too_slow':time_slow_a})\n",
    "\n",
    "\n",
    "if do_comparison:\n",
    "    fail_time_b = {}\n",
    "    time_slow_b = []\n",
    "    time_out_of_sight_b = []\n",
    "    for i, ind in enumerate(compare_b[0]['Results']['Too_slow']['All results']):\n",
    "        if ind:\n",
    "            time_slow_b.append(compare_b[0]['Results']['Time_too_slow']['All results'][i])\n",
    "\n",
    "    for i, ind in enumerate(compare_b[0]['Results']['Out_of_sight']['All results']):\n",
    "        if ind:\n",
    "            time_out_of_sight_b.append(compare_b[0]['Results']['Time_out_of_sight']['All results'][i])\n",
    "\n",
    "    fail_time_b.update({'Out_of_sight':time_out_of_sight_b, 'Too_slow':time_slow_b})    \n",
    "    \n",
    "    for item in boolean_comp:\n",
    "        nb_fail_a = compare_a[0]['Results'][item]['Mean']\n",
    "        nb_fail_b = compare_b[0]['Results'][item]['Mean']\n",
    "        am_of_fail_a = nb_fail_a.replace(' failures','')\n",
    "        am_of_fail_b = nb_fail_b.replace(' failures','')\n",
    "        win,brela,diff,sign = comp_low_bet(float(am_of_fail_a),float(am_of_fail_b))\n",
    "        advantages.update({item: win})\n",
    "        print(\"Benchmark A had {0} (rel time when failures occured {1})and Benchmark B had {2} (rel time when failures occured {3}) due to \"\\\n",
    "              .format(nb_fail_a,fail_time_a[item],nb_fail_b,fail_time_b[item])+item)\n",
    "        \n",
    "else:\n",
    "    for item in boolean_comp:\n",
    "        nb_fail_a = compare_a[0]['Results'][item]['Mean']\n",
    "        am_of_fail_a = nb_fail_a.replace(' failures','')\n",
    "        print(\"Your Benchmark had {0} (rel time when failures occured {1})\".format(nb_fail_a,fail_time_a[item])+item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if do_comparison:\n",
    "    print(\"Below you can see some tables comparing some Behaviour performances with each other. These tables show\\\n",
    "    a relative comparison of the different performance measurements and quantify your intuition that you see\\\n",
    "    in the images above.\")\n",
    "    #For each property we calculate the mean, std, median, min, max as well as CV factor and print it into a table\n",
    "    for item in meaningful_comp_results:\n",
    "        mean_a = compare_a[0]['Results'][item]['Mean']\n",
    "        mean_b = compare_b[0]['Results'][item]['Mean']\n",
    "        median_a = compare_a[0]['Results'][item]['Median']\n",
    "        median_b = compare_b[0]['Results'][item]['Median']\n",
    "        std_a = compare_a[0]['Results'][item]['Std']\n",
    "        std_b = compare_b[0]['Results'][item]['Std']\n",
    "        cv_a = compare_a[0]['Results'][item]['coefficient of variation calculation']\n",
    "        cv_b = compare_b[0]['Results'][item]['coefficient of variation calculation']\n",
    "        min_a = compare_a[0]['Results'][item]['Min']\n",
    "        max_a = compare_a[0]['Results'][item]['Max']\n",
    "        min_b = compare_b[0]['Results'][item]['Min']\n",
    "        max_b = compare_b[0]['Results'][item]['Max']\n",
    "        unit = compare_b[0]['Results'][item]['Unit']\n",
    "        if len(item) > 20:\n",
    "            splitat = 28\n",
    "            h, l = item[:splitat], item[splitat:]\n",
    "            if item =='Number_of_tiles_covered':\n",
    "                print(h+'\\t\\tBenchmark A\\tBenchmark B\\tdifference\\tB rel. to A[%]\\t\\tWinner')\n",
    "                print(l+' ['+unit+']')\n",
    "            else:\n",
    "                print(h+'\\tBenchmark A\\tBenchmark B\\tdifference\\tB rel. to A[%]\\t\\tWinner')\n",
    "                print(l+' ['+unit+']')\n",
    "        else:\n",
    "            if item =='Time_out_of_sight':\n",
    "                print(item+'\\t\\tBenchmark A\\tBenchmark B\\tdifference\\tB rel. to A[%]\\t\\tWinner')\n",
    "                print(' ['+unit+']')\n",
    "            elif item =='Time_too_slow':\n",
    "                print(item+'\\t\\t\\tBenchmark A\\tBenchmark B\\tdifference\\tB rel. to A[%]\\t\\tWinner')\n",
    "                print(' ['+unit+']')\n",
    "            else:\n",
    "                print(item+'\\tBenchmark A\\tBenchmark B\\tdifference\\tB relative to A[%]\\t\\tWinner')\n",
    "                print(' ['+unit+']')\n",
    "        print('--------------------------------------------------------------------------------------------------------------')\n",
    "        #below it is checked which Benchmark performed better in the specific property\n",
    "        if item in low_better:\n",
    "            win,brela,diff,sign = comp_low_bet(mean_a,mean_b)\n",
    "        elif item in high_better:\n",
    "            win,brela,diff,sign = comp_high_bet(mean_a,mean_b)\n",
    "        advantages.update({item: win})\n",
    "        print('Mean' + ':\\t\\t\\t\\t', float(np.round(mean_a,1)), '\\t\\t', float(np.round(mean_b,1)), '\\t\\t',float(np.round(diff,1)), '\\t\\t',float(np.round(brela,1)),'\\t\\t\\t',win)\n",
    "        if item in low_better:\n",
    "            win,brela,diff,sign = comp_low_bet(std_a,std_b)\n",
    "        elif item in high_better:\n",
    "            win,brela,diff,sign = comp_high_bet(std_a,std_b)\n",
    "        print('Std' + ':\\t\\t\\t\\t', float(np.round(std_a,1)), '\\t\\t', float(np.round(std_b,1)), '\\t\\t',float(np.round(diff,1)), '\\t\\t',float(np.round(brela,1)),'\\t\\t\\t',win)\n",
    "        if item in low_better:\n",
    "            win,brela,diff,sign = comp_low_bet(min_a,min_b)\n",
    "        elif item in high_better:\n",
    "            win,brela,diff,sign = comp_high_bet(min_a,min_b)\n",
    "        print('Min' + ':\\t\\t\\t\\t', float(np.round(min_a,1)), '\\t\\t', float(np.round(min_b,1)), '\\t\\t',float(np.round(diff,1)), '\\t\\t',float(np.round(brela,1)),'\\t\\t\\t',win)\n",
    "        if item in low_better:\n",
    "            win,brela,diff,sign = comp_low_bet(max_a,max_b)\n",
    "        elif item in high_better:\n",
    "            win,brela,diff,sign = comp_high_bet(max_a,max_b)\n",
    "        print('Max' + ':\\t\\t\\t\\t', float(np.round(max_a,1)), '\\t\\t', float(np.round(max_b,1)), '\\t\\t',float(np.round(diff,1)), '\\t\\t',float(np.round(brela,1)),'\\t\\t\\t',win)\n",
    "\n",
    "        print('\\n')\n",
    "    \n",
    "    print('Number of completed laps\\tBenchmark A\\tBenchmark B\\tdifference\\tB rel. to A[%]\\t\\tWinner')\n",
    "    print('--------------------------------------------------------------------------------------------------------------')\n",
    "    all_res_a = compare_a[0]['Results']['Number_of_completed_laps']['All results']\n",
    "    all_res_b = compare_b[0]['Results']['Number_of_completed_laps']['All results']\n",
    "    sum_lap_a = sum(all_res_a)\n",
    "    sum_lap_b = sum(all_res_b)\n",
    "    mean_lap_a = np.mean(all_res_a)\n",
    "    mean_lap_b = np.mean(all_res_b)\n",
    "    median_lap_a = np.median(all_res_a)\n",
    "    median_lap_b = np.median(all_res_b)\n",
    "    std_lap_a = np.std(all_res_a)\n",
    "    std_lap_b = np.std(all_res_b)\n",
    "    max_lap_a = max(all_res_a)\n",
    "    max_lap_b = max(all_res_b)\n",
    "    min_lap_a = min(all_res_a)\n",
    "    min_lap_b = min(all_res_b)\n",
    "\n",
    "    win,brela,diff,sign = comp_high_bet(mean_lap_a,mean_lap_b)\n",
    "    advantages.update({'Number_of_completed_laps': win})\n",
    "    print('Mean' + ':\\t\\t\\t\\t', float(np.round(mean_lap_a,1)), '\\t\\t', float(np.round(mean_lap_b,1)), '\\t\\t',float(np.round(diff,1)), '\\t\\t',float(np.round(brela,1)),'\\t\\t\\t',win)\n",
    "\n",
    "    win,brela,diff,sign = comp_high_bet(std_lap_a,std_lap_b)\n",
    "    print('Std' + ':\\t\\t\\t\\t', float(np.round(std_lap_a,1)), '\\t\\t', float(np.round(std_lap_b,1)), '\\t\\t',float(np.round(diff,1)), '\\t\\t',float(np.round(brela,1)),'\\t\\t\\t',win)\n",
    "\n",
    "    win,brela,diff,sign = comp_high_bet(median_lap_a,median_lap_b)\n",
    "    print('Median' + ':\\t\\t\\t\\t', float(np.round(median_lap_a,1)), '\\t\\t', float(np.round(median_lap_b,1)), '\\t\\t',float(np.round(diff,1)), '\\t\\t',float(np.round(brela,1)),'\\t\\t\\t',win)\n",
    "\n",
    "    win,brela,diff,sign = comp_high_bet(min_lap_a,min_lap_b)\n",
    "    print('Min' + ':\\t\\t\\t\\t', float(np.round(min_lap_a,1)), '\\t\\t', float(np.round(min_lap_b,1)), '\\t\\t',float(np.round(diff,1)), '\\t\\t',float(np.round(brela,1)),'\\t\\t\\t',win)\n",
    "\n",
    "    win,brela,diff,sign = comp_high_bet(max_lap_a,max_lap_b)\n",
    "    print('Max' + ':\\t\\t\\t\\t', float(np.round(max_lap_a,1)), '\\t\\t', float(np.round(max_lap_b,1)), '\\t\\t',float(np.round(diff,1)), '\\t\\t',float(np.round(brela,1)),'\\t\\t\\t',win)\n",
    "\n",
    "    win,brela,diff,sign = comp_high_bet(sum_lap_a,sum_lap_b)\n",
    "    print('Overall' + ':\\t\\t\\t', float(np.round(sum_lap_a,1)), '\\t\\t', float(np.round(sum_lap_b,1)), '\\t\\t',float(np.round(diff,1)), '\\t\\t',float(np.round(brela,1)),'\\t\\t\\t',win)\n",
    "\n",
    "    print('\\n')\n",
    "        \n",
    "else:\n",
    "    print(\"Below you can see some tables showing some Behaviour performances analysis\")\n",
    "    for item in meaningful_comp_results:\n",
    "        mean_a = compare_a[0]['Results'][item]['Mean']\n",
    "        median_a = compare_a[0]['Results'][item]['Median']\n",
    "        std_a = compare_a[0]['Results'][item]['Std']\n",
    "        cv_a = compare_a[0]['Results'][item]['coefficient of variation calculation']\n",
    "        min_a = compare_a[0]['Results'][item]['Min']\n",
    "        max_a = compare_a[0]['Results'][item]['Max']\n",
    "        unit = compare_b[0]['Results'][item]['Unit']\n",
    "        if len(item) > 20:\n",
    "            splitat = 28\n",
    "            h, l = item[:splitat], item[splitat:]\n",
    "            if item =='Number_of_tiles_covered':\n",
    "                print(h+'\\t\\tBenchmark A')\n",
    "                print(l+' ['+unit+']')\n",
    "            else:\n",
    "                print(h+'\\tBenchmark A')\n",
    "                print(l+' ['+unit+']')\n",
    "        else:\n",
    "            if item =='Time_out_of_sight':\n",
    "                print(item+'\\t\\tBenchmark A')\n",
    "                print(' ['+unit+']')\n",
    "            elif item =='Time_too_slow':\n",
    "                print(item+'\\t\\t\\tBenchmark A')\n",
    "                print(' ['+unit+']')\n",
    "            else:\n",
    "                print(item+'\\tBenchmark A')\n",
    "                print(' ['+unit+']')\n",
    "        print('--------------------------------------------------------------------------------------------------------------')\n",
    "     \n",
    "        print('Mean' + ':\\t\\t\\t\\t', float(np.round(mean_a,2)))\n",
    "        print('Std' + ':\\t\\t\\t\\t', float(np.round(std_a,1)))\n",
    "        print('Min' + ':\\t\\t\\t\\t', float(np.round(min_a,2)))\n",
    "        print('Max' + ':\\t\\t\\t\\t', float(np.round(max_a,2)))\n",
    "\n",
    "        print('\\n')\n",
    "        \n",
    "    print('Number of completed laps\\tBenchmark A')\n",
    "    print('--------------------------------------------------------------------------------------------------------------')\n",
    "    all_res_a = compare_a[0]['Results']['Number_of_completed_laps']['All results']\n",
    "    sum_lap_a = sum(all_res_a)\n",
    "    mean_lap_a = np.mean(all_res_a)\n",
    "    median_lap_a = np.median(all_res_a)\n",
    "    std_lap_a = np.std(all_res_a)\n",
    "    max_lap_a = max(all_res_a)\n",
    "    min_lap_a = min(all_res_a)\n",
    "\n",
    "\n",
    "    print('Mean' + ':\\t\\t\\t\\t', float(np.round(mean_lap_a,1)))\n",
    "\n",
    "    print('Std' + ':\\t\\t\\t\\t', float(np.round(std_lap_a,1)))\n",
    "\n",
    "    print('Median' + ':\\t\\t\\t\\t', float(np.round(median_lap_a,1)))\n",
    "\n",
    "    print('Min' + ':\\t\\t\\t\\t', float(np.round(min_lap_a,1)))\n",
    "\n",
    "    print('Max' + ':\\t\\t\\t\\t', float(np.round(max_lap_a,1)))\n",
    "\n",
    "    print('Overall' + ':\\t\\t\\t', float(np.round(sum_lap_a,1)))\n",
    "\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Final Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if do_comparison:\n",
    "    #sumarize all the advantages\n",
    "    amount_of_adv_a = 0\n",
    "    amount_of_adv_b = 0\n",
    "    amount_of_eq_prop = 0\n",
    "    adv_of_a = []\n",
    "    adv_of_b = []\n",
    "    equal_prop = []\n",
    "    for i, prop in enumerate(advantages):\n",
    "        if advantages[prop] == 'A':\n",
    "            amount_of_adv_a += 1\n",
    "            adv_of_a.append(prop)\n",
    "        elif advantages[prop] == 'B':\n",
    "            amount_of_adv_b += 1\n",
    "            adv_of_b.append(prop)\n",
    "        elif advantages[prop] == 'equal':\n",
    "            amount_of_eq_prop += 1\n",
    "            equal_prop.append(prop)\n",
    "        else:\n",
    "            print(\"Something went wrong with the property: \"+prop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_comparison:\n",
    "    #quanitify and priotize the advantages\n",
    "    beh_high_prio_a = 0\n",
    "    beh_high_prio_a_list = []\n",
    "    beh_medium_prio_a = 0\n",
    "    beh_medium_prio_a_list = []\n",
    "    beh_low_prio_a = 0\n",
    "    beh_low_prio_a_list = []\n",
    "    beh_high_prio_b = 0\n",
    "    beh_high_prio_b_list = []\n",
    "    beh_medium_prio_b = 0\n",
    "    beh_medium_prio_b_list = []\n",
    "    beh_low_prio_b = 0\n",
    "    beh_low_prio_b_list = []\n",
    "\n",
    "    eng_high_prio_a = 0\n",
    "    eng_high_prio_a_list = []\n",
    "    eng_medium_prio_a = 0\n",
    "    eng_medium_prio_a_list = []\n",
    "    eng_low_prio_a = 0\n",
    "    eng_low_prio_a_list = []\n",
    "    eng_high_prio_b = 0\n",
    "    eng_high_prio_b_list = []\n",
    "    eng_medium_prio_b = 0\n",
    "    eng_medium_prio_b_list = []\n",
    "    eng_low_prio_b = 0\n",
    "    eng_low_prio_b_list = []\n",
    "\n",
    "    for i, items in enumerate(advantages):\n",
    "        win = advantages[items]\n",
    "        if items in high_prio_beh:\n",
    "            if win == 'A':\n",
    "                beh_high_prio_a += 1\n",
    "                beh_high_prio_a_list.append(items)\n",
    "            elif win == 'B':\n",
    "                beh_high_prio_b += 1\n",
    "                beh_high_prio_b_list.append(items)\n",
    "        elif items in medium_prio_beh:\n",
    "            if win == 'A':\n",
    "                beh_medium_prio_a += 1\n",
    "                beh_medium_prio_a_list.append(items)\n",
    "            elif win == 'B':\n",
    "                beh_medium_prio_b += 1\n",
    "                beh_medium_prio_b_list.append(items)     \n",
    "        elif items in low_prio_beh:\n",
    "            if win == 'A':\n",
    "                beh_low_prio_a += 1\n",
    "                beh_low_prio_a_list.append(items)\n",
    "            elif win == 'B':\n",
    "                beh_low_prio_b += 1\n",
    "                beh_low_prio_b_list.append(items)\n",
    "\n",
    "        if items in high_prio_eng:\n",
    "            if win == 'A':\n",
    "                eng_high_prio_a += 1\n",
    "                eng_high_prio_a_list.append(items)\n",
    "            elif win == 'B':\n",
    "                eng_high_prio_b += 1\n",
    "                eng_high_prio_b_list.append(items)\n",
    "        elif items in medium_prio_eng:\n",
    "            if win == 'A':\n",
    "                eng_medium_prio_a += 1\n",
    "                eng_medium_prio_a_list.append(items)\n",
    "            elif win == 'B':\n",
    "                eng_medium_prio_b += 1\n",
    "                eng_medium_prio_b_list.append(items)     \n",
    "        elif items in low_prio_eng:\n",
    "            if win == 'A':\n",
    "                eng_low_prio_a += 1\n",
    "                eng_low_prio_a_list.append(items)\n",
    "            elif win == 'B':\n",
    "                eng_low_prio_b += 1\n",
    "                eng_low_prio_b_list.append(items)\n",
    "\n",
    "    print(\"Out of {0} comparisons made, Benchmark A has an advantage in {1} of them, where as Benchmark B has an\\\n",
    "    advantage in {2} of them. Therefore the two Benchmarks are equal in {3} comparison points\"\\\n",
    "          .format(len(advantages), amount_of_adv_a,amount_of_adv_b,amount_of_eq_prop))\n",
    "\n",
    "    print(\"Behaviour Comparison: Out of the list with high priorities the Benchmark A has advantages in {0} of them,\\\n",
    "    compared to the Benchmark A that has {1} advantage.\\n\\\n",
    "    Out of the list with medium priorities the Benchmark A has advantages in {2} of them,\\\n",
    "    compared to the Benchmark A that has {3} advantage.\\n\\\n",
    "    Out of the list with low priorities the Benchmark A has advantages in {4} of them,\\\n",
    "    compared to the Benchmark A that has {5} advantage.\".format(beh_high_prio_a, beh_high_prio_b, beh_medium_prio_a, beh_medium_prio_b,\\\n",
    "                                                                beh_low_prio_a, beh_low_prio_b))\n",
    "    print(\"Engineering Data Comparison: Out of the list with high priorities the Benchmark A has advantages in {0} of them,\\\n",
    "    compared to the Benchmark A that has {1} advantage.\\n\\\n",
    "    Out of the list with medium priorities the Benchmark A has advantages in {2} of them,\\\n",
    "    compared to the Benchmark A that has {3} advantage.\\n\\\n",
    "    Out of the list with low priorities the Benchmark A has advantages in {4} of them,\\\n",
    "    compared to the Benchmark A that has {5} advantage.\".format(eng_high_prio_a,eng_high_prio_b,eng_medium_prio_a,eng_medium_prio_b,\\\n",
    "                                                                eng_low_prio_a,eng_low_prio_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beh_score_a = 0\n",
    "beh_score_b = 0\n",
    "eng_score_a = 0\n",
    "eng_score_b = 0\n",
    "\n",
    "if do_comparison:\n",
    "    print(\"Overall Behviour Comparison:\")\n",
    "    if beh_high_prio_a > beh_high_prio_b:\n",
    "        beh_score_a = beh_score_a + 5\n",
    "        print(\"Benchmark A has more advantages in the field of highly priotized properties (A: {0}; B: {1})\"\\\n",
    "              .format(beh_high_prio_a,beh_high_prio_b))\n",
    "    elif beh_high_prio_a < beh_high_prio_b:\n",
    "        beh_score_b = beh_score_b + 5\n",
    "        print(\"Benchmark B has more advantages in the field of highly priotized properties (A: {0}; B: {1})\"\\\n",
    "              .format(beh_high_prio_a,beh_high_prio_b))\n",
    "    else:\n",
    "        print(\"Both Benchmarks have {0} advantages within the field of high priotized properties\".format(beh_high_prio_a))\n",
    "    \n",
    "    if beh_medium_prio_a > beh_medium_prio_b:\n",
    "        beh_score_a = beh_score_a + 3\n",
    "        print(\"Benchmark A has more advantages in the field of medium priotized properties (A: {0}; B: {1})\"\\\n",
    "              .format(beh_medium_prio_a,beh_medium_prio_b))\n",
    "    elif beh_medium_prio_a < beh_medium_prio_b:\n",
    "        beh_score_b = beh_score_b + 3\n",
    "        print(\"Benchmark B has more advantages in the field of medium priotized properties (A: {0}; B: {1})\"\\\n",
    "              .format(beh_medium_prio_a,beh_medium_prio_b))\n",
    "    else:\n",
    "        print(\"Both Benchmarks have {0} advantages within the field of medium priotized properties\".format(beh_medium_prio_a))\n",
    "    \n",
    "    if beh_low_prio_a > beh_low_prio_b:\n",
    "        beh_score_a = beh_score_a + 1\n",
    "        print(\"Benchmark A has more advantages in the field of low priotized properties (A: {0}; B: {1})\"\\\n",
    "              .format(beh_low_prio_a,beh_high_prio_b))\n",
    "    elif beh_low_prio_a < beh_low_prio_b:\n",
    "        beh_score_b = beh_score_b + 1\n",
    "        print(\"Benchmark B has more advantages in the field of low priotized properties (A: {0}; B: {1})\"\\\n",
    "              .format(beh_low_prio_a,beh_low_prio_b))\n",
    "    else:\n",
    "        print(\"Both Benchmarks have {0} advantages within the field of low priotized properties\".format(beh_low_prio_a))\n",
    "    \n",
    "    print(\"Overall Engineering Data Comparison:\")\n",
    "    if eng_high_prio_a > eng_high_prio_b:\n",
    "        eng_score_a = eng_score_a + 5\n",
    "        print(\"Benchmark A has more advantages in the field of highly priotized properties (A: {0}; B: {1})\"\\\n",
    "              .format(eng_high_prio_a,eng_high_prio_b))\n",
    "    elif eng_high_prio_a < eng_high_prio_b:\n",
    "        eng_score_b = eng_score_b + 5\n",
    "        print(\"Benchmark B has more advantages in the field of highly priotized properties (A: {0}; B: {1})\"\\\n",
    "              .format(eng_high_prio_a,eng_high_prio_b))\n",
    "    else:\n",
    "        print(\"Both Benchmarks have {0} advantages within the field of medium priotized properties\".format(eng_high_prio_a))\n",
    "    \n",
    "    if eng_medium_prio_a > eng_medium_prio_b:\n",
    "        eng_score_a = eng_score_a + 3\n",
    "        print(\"Benchmark A has more advantages in the field of medium priotized properties (A: {0}; B: {1})\"\\\n",
    "              .format(eng_medium_prio_a,eng_medium_prio_b))\n",
    "    elif eng_medium_prio_a < eng_medium_prio_b:\n",
    "        eng_score_b = eng_score_b + 3\n",
    "        print(\"Benchmark B has more advantages in the field of medium priotized properties (A: {0}; B: {1})\"\\\n",
    "              .format(eng_medium_prio_a,eng_medium_prio_b))\n",
    "    else:\n",
    "        print(\"Both Benchmarks have {0} advantages within the field of medium priotized properties\".format(eng_medium_prio_a))\n",
    "    \n",
    "    if eng_low_prio_a > eng_low_prio_b:\n",
    "        eng_score_a = eng_score_a + 1\n",
    "        print(\"Benchmark A has more advantages in the field of low priotized properties (A: {0}; B: {1})\"\\\n",
    "              .format(eng_low_prio_a,eng_high_prio_b))\n",
    "    elif eng_low_prio_a < eng_low_prio_b:\n",
    "        eng_score_b = eng_score_b + 1\n",
    "        print(\"Benchmark B has more advantages in the field of low priotized properties (A: {0}; B: {1})\"\\\n",
    "              .format(eng_low_prio_a,eng_low_prio_b))\n",
    "    else:\n",
    "        print(\"Both Benchmarks have {0} advantages within the field of low priotized properties\".format(eng_low_prio_a))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_comparison:\n",
    "    if beh_score_a > beh_score_b:\n",
    "        print(\"Looking at the actual Behaviour performance the better Benchmark is the Benchmark A (Score A: {0}, Score B: {1})\\n\"\\\n",
    "             .format(beh_score_a,beh_score_b))\n",
    "    elif beh_score_a < beh_score_b:\n",
    "        print(\"Looking at the actual Behaviour performance the better Benchmark is the Benchmark B (Score A: {0}, Score B: {1})\\n\"\\\n",
    "             .format(beh_score_a,beh_score_b))\n",
    "    else:\n",
    "        print(\"Looking at the actual Behaviour performance both Benchmarks are equally strong (Score A: {0}, Score B: {1})\\n\"\\\n",
    "              .format(beh_score_a,beh_score_b))\n",
    "        \n",
    "    if eng_score_a > eng_score_b:\n",
    "        print(\"Looking at the actual Engineering Data performance the better Benchmark is the Benchmark A (Score A: {0}, Score B: {1})\\n\\n\"\\\n",
    "             .format(eng_score_a,eng_score_b))\n",
    "    elif eng_score_a < eng_score_b:\n",
    "        print(\"Looking at the actual Engineering Data performance the better Benchmark is the Benchmark B (Score A: {0}, Score B: {1})\\n\\n\"\\\n",
    "             .format(eng_score_a,eng_score_b))\n",
    "    else:\n",
    "        print(\"Looking at the actual Engineering Data performance both Benchmarks are equally strong (Score A: {0}, Score B: {1})\\n\\n\"\\\n",
    "              .format(eng_score_a,eng_score_b))\n",
    "    \n",
    "    tot_score_a = beh_score_a + eng_score_a\n",
    "    tot_score_b = beh_score_b + eng_score_b\n",
    "    \n",
    "    if tot_score_a > tot_score_b:\n",
    "        print(\"Looking at the overall performance the better Benchmark is the Benchmark A (Score A: {0}, Score B: {1})\\n\"\\\n",
    "             .format(tot_score_a,tot_score_b))\n",
    "        print(\"So we present you with great pleasure the winner: Benchmark A\")\n",
    "    elif tot_score_a < tot_score_b:\n",
    "        print(\"Looking at the overall performance the better Benchmark is the Benchmark B (Score A: {0}, Score B: {1})\\n\"\\\n",
    "             .format(tot_score_a,tot_score_b))\n",
    "        print(\"So we present you with great pleasure the winner: Benchmark B\")\n",
    "    else:\n",
    "        print(\"Looking at the overall performance both Benchmarks are equally strong (Score A: {0}, Score B: {1})\\n\"\\\n",
    "              .format(tot_score_a,tot_score_b))\n",
    "        print(\"So we present you with great pleasure both winners: Benchmark A and B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_comparison:\n",
    "    print(\"List of advantages of Benchmark A\")\n",
    "    print(\"1. Behaviour performance:\")\n",
    "    print(\"\\tHigh prioritized:\")\n",
    "    for i in range(0, len(beh_high_prio_a_list)):\n",
    "        print('\\t\\t'+beh_high_prio_a_list[i])\n",
    "    print(\"\\tMedium prioritized:\")\n",
    "    for i in range(0, len(beh_medium_prio_a_list)):\n",
    "        print('\\t\\t'+beh_medium_prio_a_list[i])\n",
    "    print(\"\\tLow prioritized:\")\n",
    "    for i in range(0, len(beh_low_prio_a_list)):\n",
    "        print('\\t\\t'+beh_low_prio_a_list[i])\n",
    "     \n",
    "    print(\"2. Engineering Data performance:\")\n",
    "    print(\"\\tHigh prioritized:\")\n",
    "    for i in range(0, len(eng_high_prio_a_list)):\n",
    "        print('\\t\\t'+eng_high_prio_a_list[i])\n",
    "    print(\"\\tMedium prioritized:\")\n",
    "    for i in range(0, len(eng_medium_prio_a_list)):\n",
    "        print('\\t\\t'+eng_medium_prio_a_list[i])\n",
    "    print(\"\\tLow prioritized:\")\n",
    "    for i in range(0, len(eng_low_prio_a_list)):\n",
    "        print('\\t\\t'+eng_low_prio_a_list[i])\n",
    "    \n",
    "    print(\"\\n\\n\\n\")\n",
    "    print(\"List of advantages of Benchmark B\")\n",
    "    print(\"1. Behaviour performance:\")\n",
    "    print(\"\\tHigh prioritized:\")\n",
    "    for i in range(0, len(beh_high_prio_b_list)):\n",
    "        print('\\t\\t'+beh_high_prio_b_list[i])\n",
    "    print(\"\\tMedium prioritized:\")\n",
    "    for i in range(0, len(beh_medium_prio_b_list)):\n",
    "        print('\\t\\t'+beh_medium_prio_b_list[i])\n",
    "    print(\"\\tLow prioritized:\")\n",
    "    for i in range(0, len(beh_low_prio_b_list)):\n",
    "        print('\\t\\t'+beh_low_prio_b_list[i])\n",
    "     \n",
    "    print(\"2. Engineering Data performance:\")\n",
    "    print(\"\\tHigh prioritized:\")\n",
    "    for i in range(0, len(eng_high_prio_b_list)):\n",
    "        print('\\t\\t'+eng_high_prio_b_list[i])\n",
    "    print(\"\\tMedium prioritized:\")\n",
    "    for i in range(0, len(eng_medium_prio_b_list)):\n",
    "        print('\\t\\t'+eng_medium_prio_b_list[i])\n",
    "    print(\"\\tLow prioritized:\")\n",
    "    for i in range(0, len(eng_low_prio_b_list)):\n",
    "        print('\\t\\t'+eng_low_prio_b_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_comparison:\n",
    "    print(\"Below we print all the properties, their importance and their winner in which a comparison was done:\\n\")\n",
    "    print('Property\\t\\t\\t\\tType\\t\\t\\tPriority\\tWinner')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    for comp in advantages:\n",
    "        if comp in high_prio_eng:\n",
    "            typ = 'Engineering Data'\n",
    "            prio = 'high'\n",
    "        elif comp in medium_prio_eng:\n",
    "            typ = 'Engineering Data'\n",
    "            prio = 'medium'\n",
    "        elif comp in low_prio_eng:\n",
    "            typ = 'Engineering Data'\n",
    "            prio = 'low'\n",
    "        elif comp in high_prio_beh:\n",
    "            typ = 'Behaviour Performance'\n",
    "            prio = 'high'\n",
    "        elif comp in medium_prio_beh:\n",
    "            typ = 'Behaviour Performance'\n",
    "            prio = 'medium'\n",
    "        elif comp in low_prio_beh:\n",
    "            typ = 'Behaviour Performance'\n",
    "            prio = 'low'\n",
    "        else:\n",
    "            typ = '                   '\n",
    "            prio = '   '\n",
    "        if len(comp) > 20:\n",
    "            splitat = 20\n",
    "            h, l = comp[:splitat], comp[splitat:]\n",
    "            print(h+'\\t\\t\\t'+typ+'\\t'+prio+'\\t\\t'+advantages[comp])\n",
    "            if len(l) > 20:\n",
    "                h, l = comp[:splitat], comp[splitat:]\n",
    "                print(h)\n",
    "                if len(l) > 20:\n",
    "                    h, l = comp[:splitat], comp[splitat:]\n",
    "                    print(h)\n",
    "                    if len(l) > 20:\n",
    "                        h, l = comp[:splitat], comp[splitat:]\n",
    "                        print(h)\n",
    "                        if len(l) > 20:\n",
    "                            h, l = comp[:splitat], comp[splitat:]\n",
    "                            print(h)\n",
    "                            if len(l) > 20:\n",
    "                                h, l = comp[:splitat], comp[splitat:]\n",
    "                                print(h)\n",
    "                                if len(l) > 20:\n",
    "                                    h, l = comp[:splitat], comp[splitat:]\n",
    "                                    print(h)\n",
    "                                    print(l)\n",
    "                                else:\n",
    "                                    print(l)\n",
    "                            else:\n",
    "                                print(l)\n",
    "                        else:\n",
    "                            print(l)\n",
    "                    else:\n",
    "                        print(l)\n",
    "                else:\n",
    "                    print(l)\n",
    "            else:\n",
    "                print(l)\n",
    "        elif len(comp) < 10:\n",
    "            if comp == 'Too_slow':\n",
    "                print(comp +'\\t\\t\\t\\t'+typ+'\\t'+prio+'\\t\\t'+advantages[comp])\n",
    "            else:\n",
    "                print(comp +'\\t\\t\\t\\t\\t'+typ+'\\t'+prio+'\\t\\t'+advantages[comp])\n",
    "            \n",
    "        else:\n",
    "            if comp in ['Time_too_slow','Segment Count']:\n",
    "                print(comp +'\\t\\t\\t'+typ+'\\t'+prio+'\\t\\t\\t'+advantages[comp])\n",
    "            elif comp in ['Out_of_sight']:\n",
    "                print(comp +'\\t\\t\\t\\t'+typ+'\\t'+prio+'\\t\\t'+advantages[comp])\n",
    "            else:\n",
    "                print(comp +'\\t\\t\\t'+typ+'\\t'+prio+'\\t\\t'+advantages[comp])\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
