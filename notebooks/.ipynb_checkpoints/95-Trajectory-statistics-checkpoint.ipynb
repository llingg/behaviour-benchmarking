{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing statistics from trajectories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal of the notebook\n",
    "\n",
    "The goal of the notebook is to analyze the statistice of the performance of the Benchmakr. Please run cell after cell and follow the instructions.\n",
    "In the end of this notebook, you will get a .yaml file that includes information about:\n",
    " * The absolute mean of the offset (distance and angle) of the Duckiebot in respect to the center of the lane [m]\n",
    " * The number of rounds completed (entirely completed by the center of the April Tag placed on the localization standoff on your Duckiebot\n",
    " * The number of tiles covered (center of April Tag coompletely passed the tile)\n",
    " * Avg time needed per tile in seconds\n",
    " * Length of the Benchmark in seconds\n",
    " * Actual length of the benchmark in seconds\n",
    " * Mean absolute lane offset measured by Watchtowers (ground truth) [m]\n",
    " * Std of the absolute lane offset measured by Watchtowers (ground truth) [m]\n",
    " * Mean absolute relative angle measured by Watchtowers (ground truth) [deg]\n",
    " * Std of the absolute relative angle measured by Watchtowers (ground truth) [deg]\n",
    " * Mean absolute lane offset measured by the Duckiebot [m]\n",
    " * Std of the absolute lane offset measured by the Duckiebot [m]\n",
    " * Mean absolute relative angle measured by the Duckiebot [deg]\n",
    " * Std of the absolute relative angle measured by the Duckiebot [deg]\n",
    " * Mean of the absolute difference between lane offset measured by the Duckiebot and by the Watchtowers (ground truth) [m]\n",
    " * Std of the absolute difference between lane offset measured by the Duckiebot and by the Watchtowers (ground truth) [m]\n",
    " * Mean of the absolute difference between the relative angle measured by the Duckiebot and by the Watchtowers (ground truth) [deg]\n",
    " * Std of the absolute difference between the relative angle measured by the Duckiebot and by the Watchtowers (ground truth) [deg]\n",
    " \n",
    " \n",
    " Please note that a offset calculated by the Watchtowers that is positive, means that the Duckiebot drives more on the right side of the lane.\n",
    " The same is valid for the db_offset estimation (a positive estimation means he stands a bit to the right of the lane)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you run this notebook for each test_run you did of the specific Benchmark, please change below the variable `test_run` to the number of test_run you are currently running the notebook with.\n",
    "(Ex. test_run = '01' or test_run = '12' etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_run = 'XY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import contracts\n",
    "contracts.disable_all()\n",
    "\n",
    "import geometry as geo\n",
    "import math \n",
    "import numpy as np\n",
    "from os import path, listdir\n",
    "from scipy import stats\n",
    "import yaml\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import FileUpload\n",
    "import collections\n",
    "\n",
    "import duckietown_world as dw\n",
    "from duckietown_world.svg_drawing.ipython_utils import ipython_draw_svg, ipython_draw_html\n",
    "from duckietown_world.world_duckietown.tile import get_lane_poses\n",
    "from duckietown_world import draw_static"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please change the Map_Name below if you ran the experiment of your Benchmark on a different map than `linus_loop`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Map_Name = 'linus_loop'\n",
    "\n",
    "m = dw.load_map(Map_Name)\n",
    "\n",
    "ipython_draw_svg(m);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we prepare all kinds of different lists in which the data read from the files will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset_db = []\n",
    "offset_wt_interp=[]\n",
    "offset_wt_mine_1 = []\n",
    "offset_wt_non_interp = []\n",
    "time_db = []\n",
    "time_wt = []\n",
    "time_wt_mine =[]\n",
    "offset_wt_x = []\n",
    "offset_wt_y = []\n",
    "angle_db = []\n",
    "angle_wt_interp = []\n",
    "angle_wt_mine_1 = []\n",
    "angle_wt_non_interp = []\n",
    "\n",
    "last_seen_idx = []\n",
    "last_index_slow = []\n",
    "time_lasted = []\n",
    "time_lasted_slow = []\n",
    "total_length_bag = []\n",
    "\n",
    "actual_length = []\n",
    "actual_length_indx = []\n",
    "\n",
    "time_rel_db = []\n",
    "d_rel_db = []\n",
    "phi_rel_db = []\n",
    "\n",
    "all_traj_save = []\n",
    "all_traj_db_save = []\n",
    "int_trajs_save = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below the stopping conditions of the Lane Following Benchmark are defined.\n",
    "At the moment the Benchmark is stopped if the Duckiebot is out of sight of the watchtowers for longer than 3 seconds or if the Duckiebot crashes respectively takes more than 30 seconds to get across one single tile.\n",
    "\n",
    "If you think that the time, the Duckiebot needs to be out of sight such that the Benchmark is stoped, needs to be different from 3 seconds, please change the variable `tolerance_out_of_sight`.\n",
    "\n",
    "Also if you think that the \"crash\" condition is not right please change the variable `max_time_on_tile`\n",
    "\n",
    "Also the theoretical lenght of the bag recorded could be changed below.\n",
    "\n",
    "Please not that if you change these variables you the change the benchmark results. This is why it is necessary to leave the constants at the suggested values such that you can compare your Benchmark with others later.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tolerance_out_of_sight = 5.0\n",
    "max_time_on_tile = 15.0\n",
    "\n",
    "theoretical_length = 50.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some fucntions that we are going to use later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AFakeBar(dw.PlacedObject):\n",
    "    \"Ellipse object with a large ration between the radii\"\n",
    "\n",
    "    def __init__(self, len=0, fill_opacity=0.5, color='pink', *args, **kwargs):\n",
    "        self.len = len\n",
    "        self.fill_opacity = fill_opacity\n",
    "        self.color = color\n",
    "        dw.PlacedObject.__init__(self, *args, **kwargs)\n",
    "\n",
    "    def draw_svg(self, drawing, g):\n",
    "        # drawing is done using the library svgwrite\n",
    "        c = drawing.ellipse(center=(0, 0), r=(0.03,self.len), fill=self.color, fill_opacity=self.fill_opacity)\n",
    "        g.add(c)\n",
    "        # draws x,y axes\n",
    "        dw.draw_axes(drawing, g)\n",
    "        \n",
    "\n",
    "class Circle(dw.PlacedObject):\n",
    "    \"Circle object.\"\n",
    "\n",
    "    def __init__(self, radius, color='pink', *args, **kwargs):\n",
    "        self.radius = radius\n",
    "        self.color = color\n",
    "        dw.PlacedObject.__init__(self, *args, **kwargs)\n",
    "\n",
    "    def draw_svg(self, drawing, g):\n",
    "        # drawing is done using the library svgwrite\n",
    "        c = drawing.circle(center=(0, 0), r=self.radius, fill=self.color)\n",
    "        g.add(c)\n",
    "        # draws x,y axes\n",
    "        dw.draw_axes(drawing, g)\n",
    "\n",
    "    def extent_points(self):\n",
    "        # set of points describing the boundary\n",
    "        L = self.radius\n",
    "        return [(-L, -L), (+L, +L)]\n",
    "\n",
    "\n",
    "def relative_pose(q0, q1):\n",
    "    \"Computes the relative pose between two points in SE2\"\n",
    "    return geo.SE2.multiply(geo.SE2.inverse(q0), q1)\n",
    "\n",
    "\n",
    "def interpolate(q0, q1, alpha):\n",
    "    \"Interpolates between two points in SE2, given a coefficient alpha.\"\n",
    "    q1_from_q0 = relative_pose(q0, q1)\n",
    "    vel = geo.SE2.algebra_from_group(q1_from_q0)\n",
    "    rel = geo.SE2.group_from_algebra(vel * alpha)\n",
    "    q = geo.SE2.multiply(q0, rel)\n",
    "    return q\n",
    "\n",
    "def find_nearest(array, value):\n",
    "    \"\"\"Finds the index within the array for which the entry is closest to value\"\"\"\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return idx\n",
    "\n",
    "def extract_trajectory(localization_log):\n",
    "    \"From a log file from the localization system, extract a list of transforms describing the trajectory \\\n",
    "    measured by the watchtowers.\"\n",
    "\n",
    "    final_trajectory = []\n",
    "    begin_time_stamp_wt = localization_log['begin_time_stamp']\n",
    "    len_trajectory = len(localization_log['trajectory_data'])\n",
    "    x, y = np.zeros(len_trajectory), np.zeros(len_trajectory)\n",
    "    R = np.zeros((3, 3, len_trajectory))\n",
    "    phi = np.zeros((3, len_trajectory))\n",
    "    prev_time_stamp = 0.0\n",
    "    i =0\n",
    "    out_of_sight = False\n",
    "    \n",
    "    total_length_bag.append(float((sorted(localization_log['trajectory_data'].keys()))[-1]))\n",
    "    actual_length.append(float((sorted(localization_log['trajectory_data'].keys()))[-1]))\n",
    "    actual_length_indx.append(len_trajectory)\n",
    "    \n",
    "    for i, (time, traj) in enumerate(localization_log['trajectory_data'].items()):\n",
    "        # Checks if the Duckiebot has been out of sight for more than `tolerance_out_of_sight` (= 3) seconds \n",
    "        # if it was, the trajectory extraction is stopped and the time when the Duckiebot was last seen by the \n",
    "        # watchtowers is saved. Also it saves the index when it was last seen and shortens the trajectory to this \n",
    "        # length as benchmark must stop there!\n",
    "        if ((float(time) - float(prev_time_stamp) >= tolerance_out_of_sight) and (i != 0)):\n",
    "            time_lasted.append(float(\"{:.4f}\".format(float(prev_time_stamp))))\n",
    "            actual_length[0] = (float(\"{:.4f}\".format(float(prev_time_stamp))))\n",
    "            last_seen_idx.append(i-1)\n",
    "            actual_length_indx[0] = i-1\n",
    "            out_of_sight = True\n",
    "            break\n",
    "            \n",
    "        # Computes the different values for (x,y) position, the rotation matrix as well as the angle phi at each time\n",
    "        # stamp \n",
    "        time_wt.append(time)\n",
    "        prev_time_stamp = float(\"{:.4f}\".format(float(time)))\n",
    "        x[i] = np.array(float(\"{:.4f}\".format(float(traj[0]))))\n",
    "        y[i] = np.array(float(\"{:.4f}\".format(float(traj[1]))))\n",
    "        offset_wt_x.append(x[i])\n",
    "        offset_wt_y.append(y[i])\n",
    "        R[:, :, i] = np.reshape(np.array(traj[3:]), (3, 3))\n",
    "        phi[:, i] = np.array([np.arctan2(-R[1, 2, i], R[2, 2, i]),\n",
    "                              np.arctan2(R[0, 2, i], np.sqrt(R[0, 0, i] ** 2 + R[0, 1, i] ** 2)),\n",
    "                              np.arctan2(-R[0, 1, i], R[0, 0, i])])\n",
    "\n",
    "        z = float(\"{:.4f}\".format(phi[2, i]))\n",
    "        points = np.array([x[i], y[i]])\n",
    "        final_trajectory.append([points, z])\n",
    "        \n",
    "    final_array = final_trajectory.copy()\n",
    "\n",
    "    # transforms trajectory from the translation_angle into SE2\n",
    "    traj_tfs = []\n",
    "    \n",
    "    for entry in range(0, len(final_array)):\n",
    "        x, y = final_array[entry][0][0:2]\n",
    "        theta = final_array[entry][1]\n",
    "        q = geo.SE2_from_translation_angle([x, y], theta)\n",
    "        q_test = geo.SE2_from_translation_angle([x.item(), y.item()], theta)\n",
    "        traj_tfs.append(q)\n",
    "        all_traj_save.append(q_test.tolist())\n",
    "    \n",
    "    return traj_tfs, out_of_sight, begin_time_stamp_wt\n",
    "\n",
    "\n",
    "def extract_offset_db(localization_log, begin_time_stamp_wt):\n",
    "    \"From a log file from the Duckiebot, extract the information of the calculated offset and angle by the Duckiebot.\"\n",
    "\n",
    "    final_trajectory = []\n",
    "    \n",
    "    # compares the to beginning time_stamps to take into account slight offset between the bag recordings\n",
    "    begin_time_stamp_db_post = localization_log['begin_time_stamp']\n",
    "    diff_btw_time_stamps_dbpost_wt = float(\"{:.4f}\".format((begin_time_stamp_db_post - begin_time_stamp_wt)))\n",
    "#     print(diff_btw_time_stamps_dbpost_wt)\n",
    "    len_trajectory = len(localization_log['odometry_data'])\n",
    "    x = np.zeros(len_trajectory)\n",
    "    Q = np.zeros((4, len_trajectory))\n",
    "    theta = np.zeros(len_trajectory)\n",
    "    phi = np.zeros( len_trajectory)\n",
    "\n",
    "    # extracts the offset which is given in meters as well as the quaternion describing the rotation\n",
    "    # the quaternion then is transformed into the angle using the fact that phi = arctan(2*Q2*Q3/(1-2*Q2^2)) \n",
    "    # according to https://en.wikipedia.org/wiki/Conversion_between_quaternions_and_Euler_angles\n",
    "    \n",
    "    for i, (time, traj) in enumerate(localization_log['odometry_data'].items()):\n",
    "        if float(time) + diff_btw_time_stamps_dbpost_wt > float(time_wt[-1]):\n",
    "            break\n",
    "        x[i] = np.array(float(\"{:.4f}\".format(float(traj[0]))))\n",
    "        offset_db.append(x[i].item())\n",
    "        Q[:, i] = np.array(traj[3:])\n",
    "        phi[i] = math.atan2((2*Q[2,i]*Q[3,i]),(1 - 2*Q[2,i]*Q[2,i]))\n",
    "        #Or simply reverse the calculation done within the post_processor\n",
    "        #phi[i] = math.pi - math.asin(Q[3,i])*2\n",
    "        time_db.append(str(np.round((float(time) + diff_btw_time_stamps_dbpost_wt),6)))\n",
    "        z = float(\"{:.4f}\".format(phi[i]))\n",
    "        angle_db.append(z)\n",
    "        points = x[i]\n",
    "        all_traj_db_save.append([points.item(), z])\n",
    "        final_trajectory.append([points, z])\n",
    "        \n",
    "    final_array = final_trajectory.copy()\n",
    "    \n",
    "\n",
    "    return final_array\n",
    "\n",
    "\n",
    "def get_interpolated_points(center_line, trajectories):\n",
    "    \"\"\"Generates an interpolated point for each point on the center line, for each trajectory as long as the point\n",
    "    lies between two trajectory points.\"\"\"\n",
    "    \n",
    "    # ToDo if time, check if the interpolated points can be used for further calculations\n",
    "    closest_behind = [None] * len(trajectories)\n",
    "    interpolated_trajectories = []\n",
    "    for center_point in center_line:\n",
    "        interpolated_points = []\n",
    "        interpolated_points_test = []\n",
    "        for idx_t, traj in enumerate(trajectories):\n",
    "            interpolated_point_traj = None\n",
    "            begin_t = closest_behind[idx_t] if closest_behind[idx_t] else 0\n",
    "            for idx_point in range(begin_t, len(traj)):\n",
    "                if a_behind_b(a=traj[idx_point], b=center_point):\n",
    "                    closest_behind[idx_t] = idx_point\n",
    "                    continue\n",
    "\n",
    "                if closest_behind[idx_t] is None:\n",
    "                    # If there is no point behind we cannot compute the interpolation\n",
    "                    interpolated_point_traj = None\n",
    "                    break\n",
    "                else:\n",
    "                    try:\n",
    "                        interpolated_point_traj = interpolate_magic(center_point,\n",
    "                                                                    traj[closest_behind[idx_t]],\n",
    "                                                                    traj[closest_behind[idx_t] + 1])\n",
    "                        break\n",
    "\n",
    "                    except IndexError:\n",
    "                        print('The index is outside the list!')\n",
    "                        interpolated_point_traj = None\n",
    "                        break\n",
    "            interpolated_points.append(interpolated_point_traj)\n",
    "            try:\n",
    "                interpolated_points_test.append(interpolated_point_traj.tolist())\n",
    "            except:\n",
    "                interpolated_points_test.append(interpolated_point_traj)\n",
    "        interpolated_trajectories.append(interpolated_points)\n",
    "        int_trajs_save.append(interpolated_points_test)\n",
    "    return interpolated_trajectories\n",
    "\n",
    "\n",
    "def a_behind_b(a=None, b=None):\n",
    "    \"\"\"Check if a is behind b wrt the heading direction of a.\"\"\"\n",
    "    if a is None or b is None:\n",
    "        return False\n",
    "    rel_pose = relative_pose(b, a)\n",
    "    return dw.SE2Transform.from_SE2(rel_pose).p[0] < 0\n",
    "\n",
    "\n",
    "def interpolate_magic(center_pt, previous_pt, next_pt):\n",
    "    \"\"\"Returns an interpolated point between previoust_pt and next_pt at the height of center_pt\"\"\"\n",
    "    tf_prev = relative_pose(center_pt, previous_pt)\n",
    "    d_prev = dw.SE2Transform.from_SE2(tf_prev).p[0]\n",
    "\n",
    "    tf_next = relative_pose(center_pt, next_pt)\n",
    "    d_next = dw.SE2Transform.from_SE2(tf_next).p[0]\n",
    "\n",
    "    alpha = np.abs(d_prev) / (np.abs(d_prev) + d_next)\n",
    "    interpolated_pt = interpolate(previous_pt, next_pt, alpha)\n",
    "    return interpolated_pt\n",
    "\n",
    "\n",
    "def get_used_lanes(trajectories):\n",
    "    \"\"\"Returns a list with all used lanes and a dictionary containing the transform to each lane segment.\"\"\"\n",
    "    \"\"\"It also calculates the number of completed laps, the time needed per tile and it counts the number\"\"\"\n",
    "    \"\"\"of tiles covered (total as well as specific for different types)\"\"\"\n",
    "    \"\"\"Moreover it checks if the Duckiebot had a crash or drives too slow -> if the center of the April Tag\"\"\"\n",
    "    \"\"\"of the  Duckiebot takes more than 30 seconds to get across one tile the Benchmark is stoped there\"\"\"\n",
    "    \"\"\"The time when this happened is saved and the trajectories are shorten to that time\"\"\"\n",
    "    \n",
    "    # If in future for another Benchmark there are other tiles part of the loop just add a dictionary for them as well\n",
    "    used_lane_segs = set()\n",
    "    used_lane_segs_list = []\n",
    "    lane_segs_tfs = dict()\n",
    "    last_lane_seg = dict()\n",
    "    prev_lane_seg = ()\n",
    "    current_lane_seg = ()\n",
    "    start_tile = ()\n",
    "    \n",
    "    total_nb_of_tiles = 0\n",
    "    nb_straight_tiles = 0\n",
    "    nb_curve_left = 0\n",
    "    nb_curve_right = 0\n",
    "    nb_complete_laps = 0\n",
    "    \n",
    "    too_slow = False\n",
    "    \n",
    "    first_time_on_tile = 0.0\n",
    "    start = False\n",
    "    new_tile = False\n",
    "    count = 0\n",
    "    \n",
    "    \n",
    "    for traj in trajectories:\n",
    "        for pose in traj:\n",
    "            count += 1\n",
    "            try:\n",
    "                tl = list(get_lane_poses(m, pose))[0]\n",
    "                lane_segment_name = tl.lane_segment_fqn\n",
    "                if not start:\n",
    "                    # if other tiles are part of the loop, just add another if condition with the name of the tile\n",
    "                    total_nb_of_tiles += 1\n",
    "                    # checks what kind of tile that it is\n",
    "                    if lane_segment_name[2] == \"straight\":  \n",
    "                        nb_straight_tiles += 1  \n",
    "                    elif lane_segment_name[2] == \"curve_left\": \n",
    "                        nb_curve_left += 1;\n",
    "                    elif lane_segment_name[2] == \"curve_right\":\n",
    "                        nb_curve_right += 1\n",
    "                    \n",
    "                    start_tile = lane_segment_name\n",
    "                    current_lane_seg = lane_segment_name\n",
    "                    prev_lane_seg = lane_segment_name\n",
    "                    first_time_on_tile = float(time_wt[count])\n",
    "                    start = True\n",
    "                    \n",
    "                if lane_segment_name[1] == current_lane_seg[1]:\n",
    "                    new_tile = False\n",
    "                    # the following condoition checks if the Duckiebot drives too slow or not\n",
    "                    if abs(float(first_time_on_tile) - float(time_wt[count])) > max_time_on_tile:\n",
    "                        last_index_slow.append(count)\n",
    "                        time_lasted_slow.append(float(time_wt[count]))\n",
    "                        print(\"DB too slow\")\n",
    "                        too_slow = True\n",
    "                        break\n",
    "                elif lane_segment_name[1] != current_lane_seg[1]:\n",
    "                    new_tile = True\n",
    "                    # if other tiles are part of the loop, just add another if condition with the name of the tile\n",
    "                    total_nb_of_tiles += 1\n",
    "                    # checks what kind of tile that it is\n",
    "                    if lane_segment_name[2] == \"straight\":  \n",
    "                        nb_straight_tiles += 1  \n",
    "                    elif lane_segment_name[2] == \"curve_left\": \n",
    "                        nb_curve_left += 1;\n",
    "                    elif lane_segment_name[2] == \"curve_right\":\n",
    "                        nb_curve_right += 1\n",
    "                    first_time_on_tile = float(time_wt[count])\n",
    "                    current_lane_seg = lane_segment_name\n",
    "                    \n",
    "                    if lane_segment_name[1] == start_tile[1]:\n",
    "                        print(\"new round\")\n",
    "                        nb_complete_laps +=1\n",
    "                \n",
    "                #checks if the lane segment appears for the first time or not\n",
    "                #if it appears for the first time the new lane segment is added to the list of used lane segments\n",
    "                if lane_segment_name not in used_lane_segs:\n",
    "                    used_lane_segs.add(lane_segment_name)\n",
    "                    used_lane_segs_list.append(lane_segment_name)\n",
    "                    lane_segs_tfs[lane_segment_name] = tl.lane_segment_transform.asmatrix2d().m\n",
    "            except IndexError:\n",
    "                pass\n",
    "\n",
    "    return used_lane_segs_list, lane_segs_tfs, nb_complete_laps, too_slow, total_nb_of_tiles, \\\n",
    "    nb_straight_tiles, nb_curve_left, nb_curve_right\n",
    "\n",
    "\n",
    "def get_global_center_line(map, used_lane_segs, global_segs_SE2, pts_per_segment, length):\n",
    "    \"Builds a center line for all the used lanes in the global coordinate frame.\"\n",
    "    center_line = []\n",
    "    center_line_global = []\n",
    "    center_line_global_tfs = []\n",
    "    \n",
    "    # The number of points genereated for the center line depends on the tile \n",
    "    # mid is the number of points for a straight tile\n",
    "    # long is the number of points for a left curve tile\n",
    "    # short is the number of points for a right curve tile\n",
    "    for i, lane_segment in enumerate(used_lane_segs):\n",
    "        if lane_segment[2] == 'straight':\n",
    "            n_inter = int(pts_per_segment['mid'])\n",
    "        elif lane_segment[-1] == 'lane2':\n",
    "            n_inter = int(pts_per_segment['long'])\n",
    "        elif lane_segment[-1] == 'lane1':\n",
    "            n_inter = int(pts_per_segment['short'])\n",
    "        lane = map[lane_segment]\n",
    "\n",
    "        # The end point is part of next tile\n",
    "        steps = np.linspace(0, len(lane.control_points) - 1, num=n_inter, endpoint=False)\n",
    "\n",
    "        for beta in steps:\n",
    "            center_point_local_SE2 = lane.center_point(beta)\n",
    "            center_line.append(center_point_local_SE2)\n",
    "\n",
    "            # get SE2 of the point in global coords\n",
    "            center_point_global_SE2 = geo.SE2.multiply(global_segs_SE2[lane_segment],\n",
    "                                                       center_point_local_SE2)\n",
    "\n",
    "            center_line_global.append(center_point_global_SE2)\n",
    "            center_line_global_tfs.append(dw.SE2Transform.from_SE2(center_point_global_SE2))\n",
    "\n",
    "    \n",
    "    \n",
    "    return center_line_global, center_line_global_tfs\n",
    "\n",
    "def get_trajectories_statistics(trajectories, center_line):\n",
    "    \"\"\"Computes mean trajectory and std deviations for y and angle given a list of trajectories sampled at the same x\"\"\"\n",
    "    mean_tfs = []\n",
    "    std_y = []\n",
    "    std_heading = []\n",
    "\n",
    "    start_idx = None\n",
    "    end_idx = None\n",
    "    # We need to find the first amd last index for which all trajectories have a point\n",
    "    for idx, trajs_points in enumerate(trajectories):\n",
    "        if all(trajs_points) and start_idx is None:\n",
    "            start_idx = idx\n",
    "        elif not all(trajs_points) and start_idx is not None:\n",
    "            end_idx = idx\n",
    "            break\n",
    "    end_idx = -1 if end_idx is None else end_idx\n",
    "    complete_trajectories = trajectories[start_idx:end_idx]\n",
    "\n",
    "    for tfs, tfs_center in zip(complete_trajectories,center_line):\n",
    "        xs = [tf.p[0] for tf in tfs]\n",
    "        ys = [tf.p[1] for tf in tfs]\n",
    "        headings = [tf.theta for tf in tfs]\n",
    "        mean_x = np.mean(xs)\n",
    "        mean_y = np.mean(ys)\n",
    "        point = [mean_x , mean_y]\n",
    "        # To compute mean angles we need to pay attention\n",
    "        mean_angle = np.arctan2(np.mean(np.sin(headings)),np.mean(np.cos(headings)))\n",
    "        \n",
    "        mean_tfs.append(dw.SE2Transform.from_SE2(geo.SE2_from_translation_angle([mean_x, mean_y], mean_angle)))\n",
    "        \n",
    "        indx, x_rel, y_rel, theta_rel, rel_offset_cr_min = find_nearest_2d(center_line,point, mean_angle)\n",
    "        # Compute all transforms wrt to the mean trajectory to compute the standard deviations\n",
    "        #lateral_deviation = [(mean_x-t.p[0])*np.sin(t.theta)+(mean_y-t.p[1])*np.cos(t.theta) for t in tfs]\n",
    "        lateral_deviation = []\n",
    "        mean_point = geo.SE2_from_translation_angle([mean_x, mean_y], mean_angle)\n",
    "\n",
    "        relative_tf = dw.SE2Transform.from_SE2(relative_pose(mean_point, center_line[indx].as_SE2()))\n",
    "\n",
    "        lateral_deviation.append(float(np.round(relative_tf.p[1],6)))\n",
    "        offset_wt_interp.append(float(np.round(relative_tf.p[1].item(),6)))\n",
    "        angle_wt_interp.append(float(np.round(relative_tf.theta,6)))\n",
    "        std_y.append(float(np.round(np.std(lateral_deviation),6)))\n",
    "        std_heading.append(float(np.round(stats.circstd(headings, low=-math.pi, high=math.pi),6)))\n",
    "        \n",
    "    return mean_tfs, std_y, std_heading, start_idx, end_idx\n",
    "\n",
    "def find_nearest_2d(mid_line, point, theta):\n",
    "    \"\"\"Function to find the nearest point on the midle line to a specific point in 2d\"\"\"\n",
    "    \"\"\"It then calculates the relative x and y offset of the point to the nearest point on the center line\"\"\"\n",
    "    \"\"\" as well as the relative angle of the April Tag on your Duckiebot compared to the cener line\"\"\"\n",
    "#     print(value)\n",
    "    min_dist = 100000\n",
    "    rel_offset_cr_min = 10000\n",
    "#     print(type(mid_line))\n",
    "    start = True\n",
    "    indx = 0\n",
    "    for i in range(1, len(mid_line)):\n",
    "        xs_c = mid_line[i].p[0]\n",
    "        ys_c = mid_line[i].p[1]\n",
    "        xs_p = mid_line[i-1].p[0]\n",
    "        ys_p = mid_line[i-1].p[1]\n",
    "        p1 = np.array([xs_p,ys_p])\n",
    "        p2 = np.array([xs_c,ys_c])\n",
    "        p3 = np.array([point[0],point[1]])\n",
    "        rel_offset_cr = np.cross(p2-p1,p3-p1)/np.linalg.norm(p2-p1)\n",
    "        if rel_offset_cr < rel_offset_cr_min:\n",
    "            rel_offset_cr_min = rel_offset_cr\n",
    "            indx = i\n",
    "        dist = (point[0]-xs_c)**2 + (point[1]-ys_c)**2\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            \n",
    "        \n",
    "            \n",
    "    rel_x = point[0] - mid_line[indx].p[0] \n",
    "    rel_y = point[1] - mid_line[indx].p[1] \n",
    "    rel_angle = mid_line[indx].theta\n",
    "    theta_rel = np.arctan2(np.mean(np.sin(theta-rel_angle)),np.mean(np.cos(theta-rel_angle)))\n",
    "    \n",
    "#     indx = (mid_line.index(idx))    \n",
    "    return indx, rel_x, rel_y, theta_rel, rel_offset_cr_min\n",
    "\n",
    "def get_trajectories_statistics_not_int(trajectories, center_line):\n",
    "    \"\"\"For each point on the trajectory of the Duckiebot, the relative offset as well as its angle of the center of \"\"\"\n",
    "    \"\"\"the April Tag of your Duckiebot is calculated\"\"\"\n",
    "    mean_tfs = []\n",
    "    std_y = []\n",
    "    std_heading = []\n",
    "\n",
    "    complete_trajectories = trajectories[:]\n",
    "    lateral_deviation_tes = []\n",
    "    rel_offset_cr = []\n",
    "    theta_rel_cr = []\n",
    "    \n",
    "    for tfs in complete_trajectories:\n",
    "        xs = [tf.p[0] for tf in tfs]\n",
    "        ys = [tf.p[1] for tf in tfs]        \n",
    "        headings = [tf.theta for tf in tfs]\n",
    "        mean_x = np.mean(xs)\n",
    "        mean_y = np.mean(ys)\n",
    "        point = [mean_x , mean_y]\n",
    "        mean_angle = np.arctan2(np.mean(np.sin(headings)),np.mean(np.cos(headings)))\n",
    "        \n",
    "        mean_point = geo.SE2_from_translation_angle([mean_x, mean_y], mean_angle)\n",
    "        \n",
    "        \n",
    "        indx, x_rel, y_rel, theta_rel, rel_offset_cr_min = find_nearest_2d(center_line,point, mean_angle)\n",
    "        \n",
    "        relative_tf = dw.SE2Transform.from_SE2(relative_pose(mean_point, center_line[indx].as_SE2()))\n",
    "        \n",
    "        rel_offset_cr.append(rel_offset_cr_min)\n",
    "        theta_rel_cr.append(theta_rel)\n",
    "        \n",
    "        # Compute all transforms wrt to the mean trajectory to compute the standard deviations\n",
    "        #lateral_deviation = [(mean_x-t.p[0])*np.sin(t.theta)+(mean_y-t.p[1])*np.cos(t.theta) for t in tfs]\n",
    "        lateral_deviation_tes.append((x_rel)*np.sin(theta_rel)+(y_rel)*np.cos(theta_rel))\n",
    "#         print((x_rel)*np.sin(theta_rel)+(y_rel)*np.cos(theta_rel))\n",
    "\n",
    "        #Another version of how to caluclate offset_wt\n",
    "#         offset_wt_mine_1.append(float(np.round(rel_offset_cr_min.item(),6)))\n",
    "#         angle_wt_mine_1.append(float(np.round(theta_rel.item(),6)))\n",
    "        offset_wt_non_interp.append(float(np.round(relative_tf.p[1].item(),6)))\n",
    "        angle_wt_non_interp.append(float(np.round(relative_tf.theta,6)))\n",
    "    return lateral_deviation_tes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to load the trajectories from the logs of the localization system. Therefor, we need to load the yaml file created by the graph-optimizer called `AutobotAPRILTAGNB.yaml` where APRILTAGNR is the number of the April Tag that is placed on top of your Duckiebot. This file should be found in the folder `BenchmarkXY/yaml/graph_optimizer`.\n",
    "\n",
    "If your file is placed at a different position, please adapte the logs_path accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_dir = ''\n",
    "\n",
    "logs_path = path.join(experiment_dir, '../data/BenchmarkXY/yaml/graph_optimizer')\n",
    "\n",
    "localization_logs = [path.join(logs_path, f) for f in listdir(logs_path) if path.isfile(path.join(logs_path, f))]\n",
    "print(f'Logs found: {localization_logs}')\n",
    "\n",
    "all_logs = []\n",
    "for filename in localization_logs:\n",
    "    with open(filename, 'r') as file:\n",
    "        all_logs.append(yaml.safe_load(file))\n",
    "        \n",
    "# Load the evaluation map\n",
    "m = dw.load_map(Map_Name)\n",
    "\n",
    "# Get a list of the trajectory\n",
    "all_trajectories = []\n",
    "for log in all_logs:\n",
    "    # this function extracts the trajectory up until the end if the Duckiebot was never out of sight for more than \n",
    "    # 3 sec\n",
    "    ext_traj, out_of_sight, begin_time_stamp_wt = extract_trajectory(log)\n",
    "    all_trajectories.append(ext_traj)\n",
    "if not out_of_sight:\n",
    "    last_seen_idx.append(actual_length_indx[0])\n",
    "    time_lasted.append(actual_length[0])\n",
    "\n",
    "# Evaluate the used lane and extract some interesting information out of it.\n",
    "used_lane_segments_list, lane_segments_SE2, nb_complete_laps, too_slow, total_nb_of_tiles, \\\n",
    "nb_straight_tiles, nb_curve_left, nb_curve_right \\\n",
    "= get_used_lanes(all_trajectories)\n",
    "\n",
    "# shortens the trajectory and everything if the Duckiebot was too slow or crashed\n",
    "if too_slow:\n",
    "    all_traj_save = all_traj_save[:last_index_slow[0]]\n",
    "    all_trajectories[0] = all_trajectories[0][:last_index_slow[0]]\n",
    "    time_wt = time_wt[:last_index_slow[0]]\n",
    "    actual_length[0] = time_lasted_slow[0]\n",
    "    actual_length_indx[0] = last_index_slow[0]\n",
    "if not too_slow:\n",
    "    last_index_slow.append(actual_length_indx[0])\n",
    "    time_lasted_slow.append(actual_length[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the .yaml file created by the post processor called `BAGNAME_db_estimation.yaml`. If you followed the instructions carefully this file should be found in the folder `BenchmarkXY/yaml/post_processor`.\n",
    "\n",
    "If your file is placed at a different position, please adapte the logs_path accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_dir_db = ''\n",
    "\n",
    "logs_path_db = path.join(experiment_dir_db, '../data/BenchmarkXY/yaml/post_processor')\n",
    "\n",
    "localization_logs_db = [path.join(logs_path_db, f) for f in listdir(logs_path_db) if path.isfile(path.join(logs_path_db, f))]\n",
    "print(f'Logs found: {localization_logs_db}')\n",
    "\n",
    "all_logs_db = []\n",
    "for filename_db in localization_logs_db:\n",
    "    with open(filename_db, 'r') as file_db:\n",
    "#         data = file.read()\n",
    "        all_logs_db.append(yaml.safe_load(file_db))\n",
    "    \n",
    "# Load the evaluation map\n",
    "m_db = dw.load_map('linus_loop')\n",
    "\n",
    "\n",
    "# Extract the 2d position and the angle of the Duckiebot within the map\n",
    "all_trajectories_db = []\n",
    "for log_db in all_logs_db:\n",
    "    all_trajectories_db.append(extract_offset_db(log_db, begin_time_stamp_wt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you recorded a bag on the Duckiebot whilst running the experiment, please set the variable `recorded_db_bag` below to `True` and upload below the BAGNAME_lane_pose.json file.\n",
    "\n",
    "If you did not that is fine, as the pose estimation of the Duckiebot has already been extracted from the `.yaml` file created by the post-processor.\n",
    "In this case please set the variable `recorded_db_bag` below to `False` and ignore the upload button below. Also make sure you type in the BAGNAME (following the convention explained in the documentation) when you are asked to.\n",
    "\n",
    "Note: This button is not taken out of the Notebook even though all the results analysis can be done without it, this is due to the reason that for future work there might lie some improvement potential here and it could become handy. For example a bag recorded directly on the Duckiebot is able to record messages from all the different nodes, therefore in future one can maybe do some more detailed and fine analysis. This should be possible as soon as the `daffy_new_deal` version is published."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recorded_db_bag = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not recorded_db_bag:\n",
    "    print(\"Please enter below the BAGNAME you have chosen for the recorings of the Bags whilst running the experiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BAGNAME = 'BAGNAME'\n",
    "\n",
    "if not recorded_db_bag:\n",
    "    name = BAGNAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lane_pose = FileUpload(accept='.json',\n",
    "    multiple=False)\n",
    "lane_pose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to run the next cell after having uploaded the correct file. \n",
    "\n",
    "If you do have a file from the Duckiebot, then the cell below will load all the data from the `.json` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if recorded_db_bag:    \n",
    "    assert lane_pose.data, 'File missing, please upload in above cell'\n",
    "    data = json.loads(lane_pose.data[0].decode('utf-8'))\n",
    "    dict_total_lat = {}\n",
    "\n",
    "    # extracts BAGNAME\n",
    "    name = next(iter(lane_pose.value))\n",
    "    name = name.replace(\"_lane_pose.json\",\"\")\n",
    "\n",
    "    begin_time_stamp_db_dir = data['time'][0]\n",
    "    diff_btw_time_stamps_dbdir_wt = float(\"{:.4f}\".format((begin_time_stamp_db_dir - begin_time_stamp_wt)))\n",
    "#     print(diff_btw_time_stamps_dbdir_wt)\n",
    "\n",
    "\n",
    "\n",
    "    # all the data calculated directly by the Duckiebot is extracted which are: timestamp, offset and angle\n",
    "    for i in range(0,len(data['phi'])):\n",
    "        if data['time_rel'][i] + diff_btw_time_stamps_dbdir_wt > float(time_wt[-1]):\n",
    "            break\n",
    "        time_rel_db.append(float((data['time_rel'][i] + diff_btw_time_stamps_dbdir_wt)))\n",
    "        d_rel_db.append(float(np.round(data['d'][i],6)))\n",
    "        phi_rel_db.append(float(np.round(data['phi'][i],6)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the map on which the Benchmark was run\n",
    "try:\n",
    "    del m    \n",
    "except:\n",
    "    pass\n",
    "m = dw.load_map(Map_Name)\n",
    "\n",
    "\n",
    "length = ((np.asarray(all_trajectories).shape[1]))\n",
    "mid = 30 \n",
    "cnt = collections.Counter()\n",
    "\n",
    "for x in used_lane_segments_list:\n",
    "    cnt[x[2]] +=1\n",
    "\n",
    "# Number of interpolation points of each tile (approximation, needed to do it properly)\n",
    "pts_per_segment = {\n",
    "    'short': int(mid*1/8*math.pi),\n",
    "    'mid': (mid),\n",
    "    'long': int(mid*3/8*math.pi),\n",
    "}\n",
    "\n",
    "# Compute the center line that we will use to resample\n",
    "center_line_global, center_line_global_tfs = get_global_center_line(m,\n",
    "                                                                    used_lane_segments_list,\n",
    "                                                                    lane_segments_SE2,\n",
    "                                                                    pts_per_segment, length)\n",
    "\n",
    "\n",
    "# Base transform if the plotting map is not the same as the evaluation map (i.e. plotting a subset \n",
    "# of a large map containing muliple loops)\n",
    "base_transform = np.linalg.inv(geo.SE2_from_translation_angle([0.585 * 0, 0.0], 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below calculates calculates the lateral_offset of the Duckiebot based on the measurements of the watchtowers (Ground Truth).\n",
    "\n",
    "This is done by simply looking for the shortest distance of the center of the April Tag on the Duckiebot to the center line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trajectories_trans = all_trajectories[0]\n",
    "all_traj_tfs = []\n",
    "\n",
    "# Compute the transforms of those trajectories for plotting\n",
    "for traj in all_trajectories:\n",
    "    int_tfs_traj = []\n",
    "    for el in traj:\n",
    "        if el is not None:\n",
    "            int_tfs_traj.append(dw.SE2Transform.from_SE2(geo.SE2.multiply(base_transform, el)))\n",
    "        else:\n",
    "            int_tfs_traj.append(None)\n",
    "    all_traj_tfs.append(int_tfs_traj)\n",
    "\n",
    "all_traj_tfs = np.asarray(all_traj_tfs).T.tolist()\n",
    "\n",
    "#gets the offset as well as the heading angle based on the non-interpolated trajectory\n",
    "lateral_deviation_tes = get_trajectories_statistics_not_int(all_traj_tfs, center_line_global_tfs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the average trajectory and statistics\n",
    "\n",
    "Once we have our trajectories loaded, we can start to compute our average trajectory. To do so we first compute a center line that passes through each lane segment on which our Duckiebot passed. This will be used as a reference to re-sample the trajectories in order to have coherent data for comparison and averaging.\n",
    "\n",
    "Out of the trajectory measured by the Watchtowers we will extract all the different informtion that is interesting for the Benchmark. All the measurements done by the Watchtowers are as already mentioned above are in respect to the position of the center of the April tag placed on the localization standoff of the Duckiebot.  \n",
    "\n",
    "Please note, that the data calculated based on the interpolated trajectory is stored within `offset_wt_interp` resp. `angle_wt_interp`.\n",
    "On the other hand, the data calculated based the non-interpolated trajectory is stored within `offset_wt_non_interp` resp. `angle_wt_non_interp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# roughly the number of times the DB went around the loop:\n",
    "circul = nb_complete_laps + 1\n",
    "length_loop = (np.asarray(center_line_global).shape[0])\n",
    "center_line_global_test = [0.0] * length_loop * circul\n",
    "for i in range(0,circul):\n",
    "    \n",
    "    center_line_global_test[i * length_loop:(i+1) * length_loop] = center_line_global\n",
    "    \n",
    "# Compute the interpolated trajectories\n",
    "int_trajs = get_interpolated_points(center_line_global_test, all_trajectories)\n",
    "\n",
    "# Compute the transforms of those trajectories for plotting\n",
    "all_int_tfs = []\n",
    "for traj in int_trajs:\n",
    "    int_tfs_traj = []\n",
    "    for el in traj:\n",
    "        if el is not None:\n",
    "            int_tfs_traj.append(dw.SE2Transform.from_SE2(geo.SE2.multiply(base_transform, el)))\n",
    "        else:\n",
    "            int_tfs_traj.append(None)\n",
    "    all_int_tfs.append(int_tfs_traj)\n",
    "\n",
    "# Finally, compute the statistics on the resampled trajectory\n",
    "# Gets the offset as well as the heading angle based on the interpolated trajectory\n",
    "mean_tfs, std_y, std_angle, start_idx, end_idx = get_trajectories_statistics(all_int_tfs,center_line_global_tfs)\n",
    "\n",
    "#Calculate the time needed per tile\n",
    "tpt = actual_length[0]/total_nb_of_tiles\n",
    "\n",
    "# Load the plotting map, this can be different than the previous map (for example, if you don't want apriltags\n",
    "# in the final plot.)\n",
    "del m\n",
    "m = dw.load_map('linus_loop')\n",
    "\n",
    "# Create objects for drawing\n",
    "for i, meant_tf in enumerate(mean_tfs):\n",
    "    if not(i%2):\n",
    "        m.set_object(str(i + 10000), Circle(0.01, color='purple'), ground_truth=meant_tf)\n",
    "for i, meant_tf in enumerate(mean_tfs):\n",
    "    m.set_object(str(i + 1000), AFakeBar(len=std_y[i], color='green'), ground_truth=meant_tf)\n",
    "\n",
    "# Draw!\n",
    "outdir = path.join('/home/linuslingg/out', \"ipython_draw_svg\", \"%s\" % id(m))\n",
    "\n",
    "ipython_draw_svg(m);\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please take a look at the trajectory plotted below, if this trajectory has a really weird shape, this means if the line is not at all continuous and does not at all correspond to the trajectory your duckiebot took, then this means that your localization system somehow does not work properly. This can be just bad luck so before going crazy, just try to run another experiment and have a look. Otherwise, please make sure that each section of the loop is well seen by at least one watchtower, and that your watchtowers do not have infrared cameras and that the localization system is working properly.\n",
    "In the case the trajectory does not at all correspond to the movement of your Duckiebot and the trajectory jumps around a lot (not continuous) please restart this notebook with another experiment, as this one will falsify your results.\n",
    "Bad example:\n",
    "\n",
    "![Bad example](media/ex_bad.png)\n",
    "\n",
    "Okay example:\n",
    "\n",
    "![Bad example](media/ex_okay.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the measurements to the right format\n",
    "dif_offset = []\n",
    "dif_theta = []\n",
    "dif_offset_true = []\n",
    "dif_theta_true = []\n",
    "\n",
    "time_wt_ar = np.asarray(time_wt)\n",
    "time_db_ar = np.asarray(time_db)\n",
    "time_wt_ar_fl = time_wt_ar.astype(float)\n",
    "time_db_ar_fl = time_db_ar.astype(float)\n",
    "\n",
    "angle_wt_interp_ar = np.asarray(angle_wt_interp)\n",
    "angle_db_ar = np.asarray(angle_db)\n",
    "angle_wt_interp_ar_fl = angle_wt_interp_ar.astype(float)\n",
    "angle_db_ar_fl = angle_db_ar.astype(float)\n",
    "angle_wt_non_interp_ar = np.asarray(angle_wt_non_interp)\n",
    "angle_wt_non_interp_ar_fl = angle_wt_non_interp_ar.astype(float)\n",
    "\n",
    "offset_wt_interp_ar = np.asarray(offset_wt_interp)\n",
    "offset_db_ar = np.asarray(offset_db)\n",
    "offset_wt_interp_ar_fl = offset_wt_interp_ar.astype(float)\n",
    "offset_db_ar_fl = offset_db_ar.astype(float)\n",
    "offset_wt_non_interp_ar = np.asarray(offset_wt_non_interp)\n",
    "offset_wt_non_interp_ar_fl = offset_wt_non_interp_ar.astype(float)\n",
    "\n",
    "#Convert the data measured on the Duckiebot to the right format\n",
    "if recorded_db_bag:\n",
    "    time_rel_db_ar = np.asarray(time_rel_db)\n",
    "    d_rel_db_ar = np.asarray(d_rel_db)\n",
    "    phi_rel_db_ar = np.asarray(phi_rel_db)\n",
    "    time_rel_db_ar_fl = time_rel_db_ar.astype(float)\n",
    "    d_rel_db_ar_fl = d_rel_db_ar.astype(float)\n",
    "    phi_rel_db_ar_fl = phi_rel_db_ar.astype(float)\n",
    "\n",
    "    \n",
    "last = -1\n",
    "# Calculates the difference between the relative pose estimation (offset and heading) calculated by the Duckiebot\n",
    "# an the ground truth measurement done by the Watchtower.\n",
    "# If there was a bag recorded directly on the Duckiebot this estimation could be taken for the calculation, \n",
    "# however at the moment the estimation found from the post-processor is used\n",
    "if recorded_db_bag:   \n",
    "    for i in range(0,len(time_wt_ar_fl)):\n",
    "        idx = (find_nearest(time_rel_db_ar_fl, time_wt_ar_fl[i]))\n",
    "        if time_rel_db_ar_fl[idx] != last:\n",
    "            #if you want to use the data measured directly on the Duckiebot, uncomment the two line below and\n",
    "            #comment the 2 lines after that\n",
    "#             dif_offset.append(np.abs(offset_wt_non_interp_ar_fl[i] - d_rel_db_ar_fl[idx]))\n",
    "#             dif_theta.append(np.abs(angle_wt_non_interp_ar_fl[i] - phi_rel_db_ar_fl[idx]))\n",
    "            \n",
    "            dif_offset.append(np.abs(offset_wt_non_interp_ar_fl[i] - offset_db_ar_fl[idx]))\n",
    "            dif_theta.append(np.abs(angle_wt_non_interp_ar_fl[i] - angle_db_ar_fl[idx]))\n",
    "            \n",
    "        last = time_rel_db_ar_fl[idx]\n",
    "else:\n",
    "    for i in range(0,len(time_wt_ar_fl)):\n",
    "        idx = (find_nearest(time_db_ar_fl, time_wt_ar_fl[i]))\n",
    "        if time_db_ar_fl[idx] != last:\n",
    "            dif_offset.append(np.abs(offset_wt_non_interp_ar_fl[i] - offset_db_ar_fl[idx]))\n",
    "            dif_theta.append(np.abs(angle_wt_non_interp_ar_fl[i] - angle_db_ar_fl[idx]))\n",
    "            \n",
    "        last = time_db_ar_fl[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data\n",
    "Now it is time to save all the data collected to be able to proceed with the Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save everything, just in case\n",
    "# Save map as yaml file\n",
    "outdir = path.join(experiment_dir, 'out')\n",
    "outmap = path.join(outdir, 'out_test.yaml')\n",
    "benchmark_result = path.join(experiment_dir, '../data/Benchmark02/benchmarks/same_bm/' + name + '_benchmark_results_test_' + test_run + '.yaml')\n",
    "\n",
    "if not recorded_db_bag:\n",
    "    phi_rel_db = 'Not available'\n",
    "    time_rel_db = 'Not available'\n",
    "    d_rel_db = 'Not available'\n",
    "    phi_rel_db = 'Not available'\n",
    "\n",
    "# Create a dictionary which can then be saved\n",
    "results = dict(\n",
    "            Benchmark_Type = 'Lane Following',\n",
    "            Results = dict(\n",
    "                Number_of_completed_laps = nb_complete_laps,\n",
    "                Number_of_tiles_covered = total_nb_of_tiles,\n",
    "                Avg_time_needed_per_tile = tpt,\n",
    "                Time_needed_per_straight_tile_sec = 'ToDo',\n",
    "                Time_needed_per_curved_tile = 'ToDo',\n",
    "                Theoretical_length_of_benchmark = theoretical_length,\n",
    "                Length_of_recorded_bag = total_length_bag[0],\n",
    "                Actual_length_of_benchmark = actual_length[0],\n",
    "                Tolerance_out_of_sight = tolerance_out_of_sight,\n",
    "                Out_of_sight = out_of_sight,\n",
    "                Time_out_of_sight = time_lasted[0],\n",
    "                Tolerance_too_slow_sec = max_time_on_tile,\n",
    "                Too_slow = too_slow,\n",
    "                Time_too_slow = time_lasted_slow[0],\n",
    "                Position_too_slow = 'ToDo',\n",
    "                Abs_Ground_truth_wt_mean_offset_non_interp = float(np.round(np.mean(abs(offset_wt_non_interp_ar_fl)),4)),\n",
    "                Abs_Ground_truth_wt_mean_angle_non_interp = float(np.round(math.degrees(stats.circmean(abs(angle_wt_non_interp_ar_fl), low=-math.pi, high=math.pi)),4)),\n",
    "                Abs_Ground_truth_wt_median_offset_non_interp = float(np.round(np.median(abs(offset_wt_non_interp_ar_fl)),4)),\n",
    "                Abs_Ground_truth_wt_median_angle_non_interp = float(np.round(math.degrees(np.median(abs(angle_wt_non_interp_ar_fl))),4)),\n",
    "                Abs_Ground_truth_wt_std_offset_non_interp = float(np.round(np.std(abs(offset_wt_non_interp_ar_fl)),4)),\n",
    "                Abs_Ground_truth_wt_std_angle_non_interp = float(np.round(math.degrees(stats.circstd(abs(angle_wt_non_interp_ar_fl), low=-math.pi, high=math.pi)),4)),\n",
    "                Abs_Ground_truth_wt_mean_offset_interp = float(np.round(np.mean(abs(offset_wt_interp_ar_fl)),4)),\n",
    "                Abs_Ground_truth_wt_mean_angle_interp = float(np.round(math.degrees(stats.circmean(abs(angle_wt_interp_ar_fl), low=-math.pi, high=math.pi)),4)),\n",
    "                Abs_Ground_truth_wt_median_offset_interp = float(np.round(np.median(abs(offset_wt_interp_ar_fl)),4)),\n",
    "                Abs_Ground_truth_wt_median_angle_interp = float(np.round(math.degrees(np.median(abs(angle_wt_interp_ar_fl))),4)),\n",
    "                Abs_Ground_truth_wt_std_offset_interp = float(np.round(np.std(abs(offset_wt_interp_ar_fl)),4)),\n",
    "                Abs_Ground_truth_wt_std_angle_interp = float(np.round(math.degrees(stats.circstd(abs(angle_wt_interp_ar_fl), low=-math.pi, high=math.pi)),4)),\n",
    "                Abs_Measurements_db_std_offset = float(np.round(np.std(abs(offset_db_ar_fl)),4)),\n",
    "                Abs_Measurements_db_std_angle = float(np.round(math.degrees(np.std(abs(angle_db_ar_fl))),4)),\n",
    "                Abs_Measurements_db_mean_offset = float(np.round(np.mean(abs(offset_db_ar_fl)),4)),\n",
    "                Abs_Measurements_db_mean_angle = float(np.round(math.degrees(np.mean(abs(angle_db_ar_fl))),4)),\n",
    "                Abs_Measurements_db_median_offset = float(np.round(np.median(abs(offset_db_ar_fl)),4)),\n",
    "                Abs_Measurements_db_median_angle = float(np.round(math.degrees(np.median(abs(angle_db_ar_fl))))),\n",
    "                std_diff_btw_estimation_and_ground_truth_offset = float(np.round(np.std(dif_offset),4)),\n",
    "                std_diff_btw_estimation_and_ground_truth_angle = float(np.round(math.degrees(stats.circstd(dif_theta, low=-math.pi, high=math.pi)),4)),\n",
    "                mean_diff_btw_estimation_and_ground_truth_offset = float(np.round(np.mean(dif_offset),4)),\n",
    "                mean_diff_btw_estimation_and_ground_truth_angle = float(np.round(math.degrees(stats.circmean(dif_theta, low=-math.pi, high=math.pi)),4)),\n",
    "                median_diff_btw_estimation_and_ground_truth_offset = float(np.round(np.median(dif_offset),4)),\n",
    "                median_diff_btw_estimation_and_ground_truth_angle = float(np.round(math.degrees(np.median(dif_theta)),4)),\n",
    "                angle_wt_interp = angle_wt_interp,\n",
    "                angle_db = angle_db,\n",
    "                angle_wt_non_interp = angle_wt_non_interp, \n",
    "                angle_db_true = phi_rel_db,\n",
    "                offset_wt_interp= offset_wt_interp,\n",
    "                offset_db = offset_db,\n",
    "                offset_wt_non_interp = offset_wt_non_interp,\n",
    "                offset_db_true = d_rel_db,\n",
    "                time_wt = time_wt,\n",
    "                time_db = time_db,\n",
    "                time_db_true = time_rel_db,\n",
    "                all_trajectories = all_traj_save,\n",
    "                all_trajectories_db = all_traj_db_save,\n",
    "                int_trajs = int_trajs_save,\n",
    "                begin_time_stamp_wt = begin_time_stamp_wt\n",
    "                )\n",
    "            )\n",
    "\n",
    "# save all the data in the file called 'BAGNAME_benchmark_results_test_XY.yaml' \n",
    "#in the folder ~/behaviour-benchmarking/data/BenchmarkXY/benchmarks/same_bm/\n",
    "with open(benchmark_result, 'w') as yaml_file:\n",
    "    yaml.dump(results, yaml_file, default_flow_style=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
