{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computating statistics of Benchmark Data\n",
    "\n",
    "This notebook serves for the following steps within the Benchmark:\n",
    "* It gathers all the information recorded and checks if the data is stable enough (this means if the Std of some measurements is small enough)\n",
    "* If this is not the case the user gets toled to record another bag\n",
    "* If this is the case the data is analysed and the means are calculated\n",
    "* 1 final .yaml file called `BAGNAME_benchmark_final_results.yaml` is designed including on one hand all the engineering data and on the other hand the overall results of the actual behaviour. This file is stored into the folder `~/behaviour-benchmarking/data/BenchmarkXY/benchmarks/final`. Please change the variable Benchmark to the name of your Benchmark (or the name of the foldet containing all information and where you wan to store the results\n",
    "\n",
    "Please run cell after cell reading carefully the instructions between the cell as well as the outputs computed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Benchmark = 'BenchmarkXY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "#All imports necessary for this Notebook\n",
    "import contracts\n",
    "contracts.disable_all()\n",
    "\n",
    "import geometry as geo\n",
    "import math \n",
    "import numpy as np\n",
    "from os import path, listdir\n",
    "from scipy import stats\n",
    "import yaml\n",
    "import json\n",
    "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
    "import matplotlib.pyplot as plt\n",
    "from math import ceil\n",
    "from scipy import interpolate\n",
    "from os import path, listdir\n",
    "from ipywidgets import FileUpload\n",
    "import statistics\n",
    "import collections\n",
    "\n",
    "import duckietown_world as dw\n",
    "from duckietown_world.svg_drawing.ipython_utils import ipython_draw_svg, ipython_draw_html\n",
    "from duckietown_world.world_duckietown.tile import get_lane_poses\n",
    "from duckietown_world import draw_static"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below there are some functions that are used later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_pose(q0, q1):\n",
    "    \"Computes the relative pose between two points in SE2\"\n",
    "    return geo.SE2.multiply(geo.SE2.inverse(q0), q1)\n",
    "\n",
    "class AFakeBar(dw.PlacedObject):\n",
    "    \"Ellipse object with a large ration between the radii\"\n",
    "\n",
    "    def __init__(self, len=0, fill_opacity=0.5, color='pink', *args, **kwargs):\n",
    "        self.len = len\n",
    "        self.fill_opacity = fill_opacity\n",
    "        self.color = color\n",
    "        dw.PlacedObject.__init__(self, *args, **kwargs)\n",
    "\n",
    "    def draw_svg(self, drawing, g):\n",
    "        # drawing is done using the library svgwrite\n",
    "        c = drawing.ellipse(center=(0, 0), r=(0.03,self.len), fill=self.color, fill_opacity=self.fill_opacity)\n",
    "        g.add(c)\n",
    "        # draws x,y axes\n",
    "        dw.draw_axes(drawing, g)\n",
    "        \n",
    "def find_nearest_2d(mid_line, point, theta):\n",
    "    \"\"\"Function to find the nearest point on the midle line to a specific point in 2d\"\"\"\n",
    "    \"\"\"It then calculates the relative x and y offset of the point to the nearest point on the center line\"\"\"\n",
    "    \"\"\" as well as the relative angle of the April Tag on your Duckiebot compared to the cener line\"\"\"\n",
    "#     print(value)\n",
    "    min_dist = 100000\n",
    "    rel_offset_cr_min = 10000\n",
    "#     print(type(mid_line))\n",
    "    start = True\n",
    "    indx = 0\n",
    "    for i in range(1, len(mid_line)):\n",
    "        xs_c = mid_line[i].p[0]\n",
    "        ys_c = mid_line[i].p[1]\n",
    "        xs_p = mid_line[i-1].p[0]\n",
    "        ys_p = mid_line[i-1].p[1]\n",
    "        p1 = np.array([xs_p,ys_p])\n",
    "        p2 = np.array([xs_c,ys_c])\n",
    "        p3 = np.array([point[0],point[1]])\n",
    "        rel_offset_cr = np.cross(p2-p1,p3-p1)/np.linalg.norm(p2-p1)\n",
    "        if rel_offset_cr < rel_offset_cr_min:\n",
    "            rel_offset_cr_min = rel_offset_cr\n",
    "            indx = i\n",
    "        dist = (point[0]-xs_c)**2 + (point[1]-ys_c)**2\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            \n",
    "        \n",
    "            \n",
    "    rel_x = point[0] - mid_line[indx].p[0] \n",
    "    rel_y = point[1] - mid_line[indx].p[1] \n",
    "    rel_angle = mid_line[indx].theta\n",
    "    theta_rel = np.arctan2(np.mean(np.sin(theta-rel_angle)),np.mean(np.cos(theta-rel_angle)))\n",
    "    \n",
    "#     indx = (mid_line.index(idx))    \n",
    "    return indx, rel_x, rel_y, theta_rel, rel_offset_cr_min\n",
    "\n",
    "class Circle(dw.PlacedObject):\n",
    "    \"Circle object.\"\n",
    "\n",
    "    def __init__(self, radius, color='pink', *args, **kwargs):\n",
    "        self.radius = radius\n",
    "        self.color = color\n",
    "        dw.PlacedObject.__init__(self, *args, **kwargs)\n",
    "\n",
    "    def draw_svg(self, drawing, g):\n",
    "        # drawing is done using the library svgwrite\n",
    "        c = drawing.circle(center=(0, 0), r=self.radius, fill=self.color)\n",
    "        g.add(c)\n",
    "        # draws x,y axes\n",
    "        dw.draw_axes(drawing, g)\n",
    "\n",
    "    def extent_points(self):\n",
    "        # set of points describing the boundary\n",
    "        L = self.radius\n",
    "        return [(-L, -L), (+L, +L)]\n",
    "    \n",
    "    \n",
    "def interpolate_custom(q0, q1, alpha):\n",
    "    \"Interpolates between two points in SE2, given a coefficient alpha.\"\n",
    "    q1_from_q0 = relative_pose(q0, q1)\n",
    "    vel = geo.SE2.algebra_from_group(q1_from_q0)\n",
    "    rel = geo.SE2.group_from_algebra(vel * alpha)\n",
    "    q = geo.SE2.multiply(q0, rel)\n",
    "    return q\n",
    "\n",
    "def get_global_center_line(map, used_lane_segs, global_segs_SE2, pts_per_segment):\n",
    "    \"Builds a center line for all the used lanes in the global coordinate frame.\"\n",
    "    center_line = []\n",
    "    center_line_global = []\n",
    "    center_line_global_tfs = []\n",
    "    \n",
    "    # The number of points genereated for the center line depends on the tile \n",
    "    # mid is the number of points for a straight tile\n",
    "    # long is the number of points for a left curve tile\n",
    "    # short is the number of points for a right curve tile\n",
    "    for i, lane_segment in enumerate(used_lane_segs):\n",
    "        if lane_segment[2] == 'straight':\n",
    "            n_inter = int(pts_per_segment['mid'])\n",
    "        elif lane_segment[-1] == 'lane2':\n",
    "            n_inter = int(pts_per_segment['long'])\n",
    "        elif lane_segment[-1] == 'lane1':\n",
    "            n_inter = int(pts_per_segment['short'])\n",
    "        lane = map[lane_segment]\n",
    "\n",
    "        # The end point is part of next tile\n",
    "        steps = np.linspace(0, len(lane.control_points) - 1, num=n_inter, endpoint=False)\n",
    "\n",
    "        for beta in steps:\n",
    "            center_point_local_SE2 = lane.center_point(beta)\n",
    "            center_line.append(center_point_local_SE2)\n",
    "\n",
    "            # get SE2 of the point in global coords\n",
    "            center_point_global_SE2 = geo.SE2.multiply(global_segs_SE2[lane_segment],\n",
    "                                                       center_point_local_SE2)\n",
    "\n",
    "            center_line_global.append(center_point_global_SE2)\n",
    "            center_line_global_tfs.append(dw.SE2Transform.from_SE2(center_point_global_SE2))\n",
    "\n",
    "    \n",
    "    \n",
    "    return center_line_global, center_line_global_tfs\n",
    "\n",
    "def get_used_lanes_mine(trajectories):\n",
    "    \"\"\"Returns a list with all used lanes and a dictionary containing the transform to each lane segment.\"\"\"\n",
    "    \"\"\"It also calculates the number of completed laps, the time needed per tile and it counts the number\"\"\"\n",
    "    \"\"\"of tiles covered (total as well as specific for different types)\"\"\"\n",
    "    \"\"\"Moreover it checks if the Duckiebot had a crash or drives too slow -> if the center of the April Tag\"\"\"\n",
    "    \"\"\"of the  Duckiebot takes more than 30 seconds to get across one tile the Benchmark is stoped there\"\"\"\n",
    "    \"\"\"The time when this happened is saved and the trajectories are shorten to that time\"\"\"\n",
    "    \n",
    "    # If in future for another Benchmark there are other tiles part of the loop just add a dictionary for them as well\n",
    "    used_lane_segs = set()\n",
    "    used_lane_segs_list = []\n",
    "    lane_segs_tfs = dict()\n",
    "    last_lane_seg = dict()\n",
    "    prev_lane_seg = ()\n",
    "    current_lane_seg = ()\n",
    "    start_tile = ()\n",
    "    \n",
    "    total_nb_of_tiles = 0\n",
    "    nb_straight_tiles = 0\n",
    "    nb_curve_left = 0\n",
    "    nb_curve_right = 0\n",
    "    nb_complete_laps = 0\n",
    "    \n",
    "    too_slow = False\n",
    "    \n",
    "    first_time_on_tile = 0.0\n",
    "    start = False\n",
    "    new_tile = False\n",
    "    count = 0\n",
    "    \n",
    "    \n",
    "    for traj in trajectories:\n",
    "        for pose in traj:\n",
    "            count += 1\n",
    "            try:\n",
    "                tl = list(get_lane_poses(m, pose))[0]\n",
    "                lane_segment_name = tl.lane_segment_fqn\n",
    "                if not start:\n",
    "                    # if other tiles are part of the loop, just add another if condition with the name of the tile\n",
    "                    total_nb_of_tiles += 1\n",
    "                    # checks what kind of tile that it is\n",
    "                    if lane_segment_name[2] == \"straight\":  \n",
    "                        nb_straight_tiles += 1  \n",
    "                    elif lane_segment_name[2] == \"curve_left\": \n",
    "                        nb_curve_left += 1;\n",
    "                    elif lane_segment_name[2] == \"curve_right\":\n",
    "                        nb_curve_right += 1\n",
    "                    \n",
    "                    start_tile = lane_segment_name\n",
    "                    current_lane_seg = lane_segment_name\n",
    "                    prev_lane_seg = lane_segment_name\n",
    "                    start = True\n",
    "                    \n",
    "                if lane_segment_name[1] == current_lane_seg[1]:\n",
    "                    new_tile = False\n",
    "                    # the following condoition checks if the Duckiebot drives too slow or not\n",
    "\n",
    "                elif lane_segment_name[1] != current_lane_seg[1]:\n",
    "                    new_tile = True\n",
    "                    # if other tiles are part of the loop, just add another if condition with the name of the tile\n",
    "                    total_nb_of_tiles += 1\n",
    "                    # checks what kind of tile that it is\n",
    "                    if lane_segment_name[2] == \"straight\":  \n",
    "                        nb_straight_tiles += 1  \n",
    "                    elif lane_segment_name[2] == \"curve_left\": \n",
    "                        nb_curve_left += 1;\n",
    "                    elif lane_segment_name[2] == \"curve_right\":\n",
    "                        nb_curve_right += 1\n",
    "                        \n",
    "                    current_lane_seg = lane_segment_name\n",
    "                    \n",
    "                    if lane_segment_name[1] == start_tile[1]:\n",
    "                        print(\"new round\")\n",
    "                        nb_complete_laps +=1\n",
    "                \n",
    "                #checks if the lane segment appears for the first time or not\n",
    "                #if it appears for the first time the new lane segment is added to the list of used lane segments\n",
    "                if lane_segment_name not in used_lane_segs:\n",
    "                    used_lane_segs.add(lane_segment_name)\n",
    "                    used_lane_segs_list.append(lane_segment_name)\n",
    "                    lane_segs_tfs[lane_segment_name] = tl.lane_segment_transform.asmatrix2d().m\n",
    "            except IndexError:\n",
    "                pass\n",
    "\n",
    "    return used_lane_segs_list, lane_segs_tfs, nb_complete_laps, total_nb_of_tiles, nb_straight_tiles, \\\n",
    "nb_curve_left, nb_curve_right\n",
    "\n",
    "def get_used_lanes(trajectories):\n",
    "    \"\"\"Returns a list with all used lanes and a dictionary containing the transform to each lane segment.\"\"\"\n",
    "    used_lane_segs = set()\n",
    "    used_lane_segs_list = []\n",
    "    lane_segs_tfs = dict()\n",
    "\n",
    "    for traj in trajectories:\n",
    "        for pose in traj:\n",
    "            try:\n",
    "                tl = list(get_lane_poses(m, pose))[0]\n",
    "                lane_segment_name = tl.lane_segment_fqn\n",
    "\n",
    "                if lane_segment_name not in used_lane_segs:\n",
    "                    used_lane_segs.add(lane_segment_name)\n",
    "                    used_lane_segs_list.append(lane_segment_name)\n",
    "                    lane_segs_tfs[lane_segment_name] = tl.lane_segment_transform.asmatrix2d().m\n",
    "            except IndexError:\n",
    "                pass\n",
    "    return used_lane_segs_list, lane_segs_tfs\n",
    "\n",
    "def get_interpolated_points(center_line, trajectories):\n",
    "    \"\"\"Generates an interpolated point for each point on the center line, for each trajectory as long as the point\n",
    "    lies between two trajectory points.\"\"\"\n",
    "    closest_behind = [None] * len(trajectories)\n",
    "    interpolated_trajectories = []\n",
    "    for center_point in center_line:\n",
    "        interpolated_points = []\n",
    "        for idx_t, traj in enumerate(trajectories):\n",
    "            interpolated_point_traj = None\n",
    "            begin_t = closest_behind[idx_t] if closest_behind[idx_t] else 0\n",
    "            for idx_point in range(begin_t, len(traj)):\n",
    "                if a_behind_b(a=traj[idx_point], b=center_point):\n",
    "                    closest_behind[idx_t] = idx_point\n",
    "                    continue\n",
    "\n",
    "                if closest_behind[idx_t] is None:\n",
    "                    # If there is no point behind we cannot compute the interpolation\n",
    "                    interpolated_point_traj = None\n",
    "                    break\n",
    "                else:\n",
    "                    try:\n",
    "                        interpolated_point_traj = interpolate_magic(center_point,\n",
    "                                                                    traj[closest_behind[idx_t]],\n",
    "                                                                    traj[closest_behind[idx_t] + 1])\n",
    "                        break\n",
    "\n",
    "                    except IndexError:\n",
    "                        print('The index is outside the list!')\n",
    "                        interpolated_point_traj = None\n",
    "                        break\n",
    "            interpolated_points.append(interpolated_point_traj)\n",
    "        interpolated_trajectories.append(interpolated_points)\n",
    "    return interpolated_trajectories\n",
    "\n",
    "\n",
    "def a_behind_b(a=None, b=None):\n",
    "    \"\"\"Check if a is behind b wrt the heading direction of a.\"\"\"\n",
    "    if a is None or b is None:\n",
    "        return False\n",
    "    rel_pose = relative_pose(b, a)\n",
    "    return dw.SE2Transform.from_SE2(rel_pose).p[0] < 0\n",
    "\n",
    "\n",
    "def interpolate_magic(center_pt, previous_pt, next_pt):\n",
    "    \"\"\"Returns an interpolated point between previoust_pt and next_pt at the height of center_pt\"\"\"\n",
    "    tf_prev = relative_pose(center_pt, previous_pt)\n",
    "    d_prev = dw.SE2Transform.from_SE2(tf_prev).p[0]\n",
    "\n",
    "    tf_next = relative_pose(center_pt, next_pt)\n",
    "    d_next = dw.SE2Transform.from_SE2(tf_next).p[0]\n",
    "\n",
    "    alpha = np.abs(d_prev) / (np.abs(d_prev) + d_next)\n",
    "    interpolated_pt = interpolate_custom(previous_pt, next_pt, alpha)\n",
    "    return interpolated_pt\n",
    "\n",
    "\n",
    "def get_trajectories_statistics(trajectories,center_line):\n",
    "    \"\"\"Computes mean trajectory and std deviations for y and angle given a list of trajectories sampled at the same x\"\"\"\n",
    "    mean_tfs = []\n",
    "    std_y = []\n",
    "    mean_offset = []\n",
    "    cv_y = []\n",
    "    cv_heading = []\n",
    "    std_heading = []\n",
    "    mean_heading = []\n",
    "\n",
    "    start_idx = None\n",
    "    end_idx = None\n",
    "    # We need to find the first amd last index for which all trajectories have a point\n",
    "    for idx, trajs_points in enumerate(trajectories):\n",
    "        if all(trajs_points) and start_idx is None:\n",
    "            start_idx = idx\n",
    "        elif not all(trajs_points) and start_idx is not None:\n",
    "            end_idx = idx\n",
    "            break\n",
    "    end_idx = -1 if end_idx is None else end_idx\n",
    "    complete_trajectories = trajectories[start_idx:end_idx]\n",
    "    \n",
    "    for tfs in complete_trajectories:\n",
    "        xs = [tf.p[0] for tf in tfs]\n",
    "        ys = [tf.p[1] for tf in tfs]\n",
    "        headings = [tf.theta for tf in tfs]\n",
    "        mean_x = np.mean(xs)\n",
    "        mean_y = np.mean(ys)\n",
    "        point = [mean_x , mean_y]\n",
    "        # To compute mean angles we need to pay attention\n",
    "        mean_angle = np.arctan2(np.mean(np.sin(headings)),np.mean(np.cos(headings)))\n",
    "        mean_tfs.append(dw.SE2Transform.from_SE2(geo.SE2_from_translation_angle([mean_x, mean_y], mean_angle)))\n",
    "        \n",
    "        cur_ind = []\n",
    "        cur_offset_mine = []\n",
    "        cur_heading_mine = []\n",
    "        \n",
    "        #find closest point on center_line to the mean of all of the points at this specific x\n",
    "        indx, x_rel, y_rel, theta_rel, rel_offset_cr_min = find_nearest_2d(center_line,point, mean_angle)\n",
    "        for i in range(0, len(xs)):\n",
    "            #transform the point to SE2\n",
    "            point_cur_tf = geo.SE2_from_translation_angle([xs[i], ys[i]], headings[i])\n",
    "            #for each point get relative position to the center line\n",
    "            relative_tf_mine = dw.SE2Transform.from_SE2(relative_pose(point_cur_tf, center_line[indx].as_SE2()))\n",
    "            #extract the offset and the heading angle deviation of the point relative to the center line\n",
    "            cur_offset_mine.append(relative_tf_mine.p[1].item())\n",
    "            cur_heading_mine.append(relative_tf_mine.theta)\n",
    "\n",
    "\n",
    "        # Compute all transforms wrt to the mean trajectory to compute the standard deviations\n",
    "        #lateral_deviation = [(mean_x-t.p[0])*np.sin(t.theta)+(mean_y-t.p[1])*np.cos(t.theta) for t in tfs]\n",
    "        lateral_deviation = []\n",
    "        mean_point = geo.SE2_from_translation_angle([mean_x, mean_y], mean_angle)\n",
    "        for t in tfs:\n",
    "            relative_tf = dw.SE2Transform.from_SE2(relative_pose(mean_point, t.as_SE2()))\n",
    "            lateral_deviation.append(relative_tf.p[1])\n",
    "        \n",
    "        relative_tf_mine = dw.SE2Transform.from_SE2(relative_pose(mean_point, center_line[indx].as_SE2()))\n",
    "        \n",
    "        offset_wt_interp_all.append(relative_tf_mine.p[1].item())\n",
    "        angle_wt_interp_all.append(relative_tf_mine.theta)\n",
    "\n",
    "        #calculate the avg and std lateral offset of the points (that are at a specific x position) of all the  \n",
    "        #different trajectories to the center line\n",
    "        std_y_cur = float(np.round(np.std(cur_offset_mine),6))\n",
    "        mean_y_cur = float(np.round(np.mean(cur_offset_mine),6))\n",
    "\n",
    "        mean_offset.append(float(np.round(np.mean(cur_offset_mine),6)))\n",
    "        std_y.append(float(np.round(np.std(cur_offset_mine),6)))\n",
    "        if mean_y_cur != 0.0:\n",
    "            cv_y.append(float(np.round(abs(std_y_cur/mean_y_cur),6)))\n",
    "        else:\n",
    "            if std_y_cur == 0.0:\n",
    "                cv_y.append(0.0)\n",
    "            else:\n",
    "                cv_y.append(10.0)\n",
    "                \n",
    "        #calculate the avg and std heading deviation of the points (that are at a specific x position) of all the  \n",
    "        #different trajectories to the center line\n",
    "        std_angle_cur = float(np.round(stats.circstd(cur_heading_mine, low=-math.pi, high=math.pi),6))\n",
    "        mean_angle_cur = float(np.round(stats.circmean(cur_heading_mine, low=-math.pi, high=math.pi),6))\n",
    "        mean_heading.append(mean_angle_cur)\n",
    "        std_heading.append(float(np.round(stats.circstd(cur_heading_mine, low=-math.pi, high=math.pi),6)))\n",
    "        if mean_angle_cur != 0.0:\n",
    "            cv_heading.append(float(np.round(abs(std_angle_cur/mean_angle_cur),6)))\n",
    "        else:\n",
    "            if std_angle_cur == 0.0:\n",
    "                cv_heading.append(0.0)\n",
    "            else:\n",
    "                cv_heading.append(10.0)\n",
    "        \n",
    "    return mean_tfs, std_y, std_heading, start_idx, end_idx,cv_y, cv_heading, mean_offset, mean_heading\n",
    "\n",
    "def get_trajectories_statistics_mean_traj(trajectories, center_line):\n",
    "    \"\"\"For each point on the trajectory of the Duckiebot, the relative offset as well as its angle of the center of \"\"\"\n",
    "    \"\"\"the April Tag of your Duckiebot is calculated\"\"\"\n",
    "    mean_tfs = []\n",
    "    std_y = []\n",
    "    std_heading = []\n",
    "\n",
    "    complete_trajectories = trajectories[:]\n",
    "    lateral_deviation_tes = []\n",
    "    rel_offset_cr = []\n",
    "    theta_rel_cr = []\n",
    "    \n",
    "    for tfs in complete_trajectories:\n",
    "        xs = tfs.p[0]\n",
    "        ys = tfs.p[1]      \n",
    "        headings = tfs.theta\n",
    "        mean_x = np.mean(xs)\n",
    "        mean_y = np.mean(ys)\n",
    "        point = [mean_x , mean_y]\n",
    "        mean_angle = np.arctan2(np.mean(np.sin(headings)),np.mean(np.cos(headings)))\n",
    "        \n",
    "        mean_point = geo.SE2_from_translation_angle([mean_x, mean_y], mean_angle)\n",
    "        \n",
    "        \n",
    "        indx, x_rel, y_rel, theta_rel, rel_offset_cr_min = find_nearest_2d(center_line,point, mean_angle)\n",
    "        \n",
    "        relative_tf = dw.SE2Transform.from_SE2(relative_pose(mean_point, center_line[indx].as_SE2()))\n",
    "        \n",
    "        rel_offset_cr.append(rel_offset_cr_min)\n",
    "        theta_rel_cr.append(theta_rel)\n",
    "        \n",
    "        # Compute all transforms wrt to the mean trajectory to compute the standard deviations\n",
    "        #lateral_deviation = [(mean_x-t.p[0])*np.sin(t.theta)+(mean_y-t.p[1])*np.cos(t.theta) for t in tfs]\n",
    "        lateral_deviation_tes.append((x_rel)*np.sin(theta_rel)+(y_rel)*np.cos(theta_rel))\n",
    "#         print((x_rel)*np.sin(theta_rel)+(y_rel)*np.cos(theta_rel))\n",
    "        offset_wt_non_interp_all.append(relative_tf.p[1].item())\n",
    "        angle_wt_non_interp_all.append(relative_tf.theta)\n",
    "        \n",
    "    return lateral_deviation_tes\n",
    "\n",
    "def Average(lst): \n",
    "    \"\"\"Calculates the average of a list\"\"\"\n",
    "    lst_abs = [abs(x) for x in lst]\n",
    "    return sum(lst_abs) / len(lst) \n",
    "\n",
    "def get_unit(info):\n",
    "    \"\"\"Function returning the unit of the different informations\"\"\" \n",
    "    if info == 'Number_of_completed_laps':\n",
    "        unit = 'laps'\n",
    "    elif info == 'Number_of_tiles_covered':\n",
    "        unit = 'tiles'\n",
    "    elif info == 'Avg_time_needed_per_tile':\n",
    "        unit = 'seconds per tile'\n",
    "    elif info == 'Time_needed_per_straight_tile_sec':\n",
    "        unit = 'seconds per tile'\n",
    "    elif info == 'Time_needed_per_curved_tile':\n",
    "        unit = 'seconds per tile'\n",
    "    elif info == 'Length_of_recorded_bag':\n",
    "        unit = 'seconds' \n",
    "    elif info == 'Actual_length_of_benchmark':\n",
    "        unit = 'seconds'   \n",
    "    elif info == 'Theoretical_length_of_benchmark':\n",
    "        unit = 'seconds'\n",
    "    elif info == 'Out_of_sight':\n",
    "        unit = ' '  \n",
    "    elif info == 'Tolerance_out_of_sight':\n",
    "        unit = 'seconds' \n",
    "    elif info == 'Time_out_of_sight':\n",
    "        unit = 'seconds' \n",
    "    elif info == 'Too_slow':\n",
    "        unit = ' '  \n",
    "    elif info == 'Time_too_slow':\n",
    "        unit = 'seconds' \n",
    "    elif info == 'Tolerance_too_slow_sec':\n",
    "        unit = 'seconds' \n",
    "    elif info == 'Position_too_slow':\n",
    "        unit = ' '\n",
    "    elif info == 'Abs_Ground_truth_wt_std_offset_non_interp':\n",
    "        unit = 'meters' \n",
    "    elif info == 'Abs_Ground_truth_wt_std_angle_non_interp':\n",
    "        unit = 'degree' \n",
    "    elif info == 'Abs_Ground_truth_wt_mean_offset_non_interp':\n",
    "        unit = 'meters' \n",
    "    elif info == 'Abs_Ground_truth_wt_mean_angle_non_interp':\n",
    "        unit = 'degree' \n",
    "    elif info == 'Abs_Ground_truth_wt_median_offset_non_interp':\n",
    "        unit = 'meters' \n",
    "    elif info == 'Abs_Ground_truth_wt_median_angle_non_interp':\n",
    "        unit = 'degree' \n",
    "    elif info == 'Abs_Ground_truth_wt_std_offset_interp':\n",
    "        unit = 'meters' \n",
    "    elif info == 'Abs_Ground_truth_wt_std_angle_interp':\n",
    "        unit = 'degree' \n",
    "    elif info == 'Abs_Ground_truth_wt_mean_offset_interp':\n",
    "        unit = 'meters' \n",
    "    elif info == 'Abs_Ground_truth_wt_mean_angle_interp':\n",
    "        unit = 'degree' \n",
    "    elif info == 'Abs_Ground_truth_wt_median_offset_interp':\n",
    "        unit = 'meters' \n",
    "    elif info == 'Abs_Ground_truth_wt_median_angle_interp':\n",
    "        unit = 'degree'\n",
    "    elif info == 'Abs_Measurements_db_std_offset':\n",
    "        unit = 'meters'\n",
    "    elif info == 'Abs_Measurements_db_std_angle':\n",
    "        unit = 'degree' \n",
    "    elif info == 'Abs_Measurements_db_mean_offset':\n",
    "        unit = 'meters' \n",
    "    elif info == 'Abs_Measurements_db_mean_angle':\n",
    "        unit = 'degree' \n",
    "    elif info == 'Abs_Measurements_db_median_offset':\n",
    "        unit = 'meters' \n",
    "    elif info == 'Abs_Measurements_db_median_angle':\n",
    "        unit = 'degree' \n",
    "    elif info == 'std_diff_btw_estimation_and_ground_truth_offset':\n",
    "        unit = 'meters' \n",
    "    elif info == 'std_diff_btw_estimation_and_ground_truth_angle':\n",
    "        unit = 'degree'\n",
    "    elif info == 'mean_diff_btw_estimation_and_ground_truth_offset':\n",
    "        unit = 'meters' \n",
    "    elif info == 'mean_diff_btw_estimation_and_ground_truth_angle':\n",
    "        unit = 'degree'\n",
    "    elif info == 'median_diff_btw_estimation_and_ground_truth_offset':\n",
    "        unit = 'meters' \n",
    "    elif info == 'median_diff_btw_estimation_and_ground_truth_angle':\n",
    "        unit = 'degree'\n",
    "    elif info == 'offset_db':\n",
    "        unit = 'meters' \n",
    "    elif info == 'angle_db':\n",
    "        unit = 'degree'\n",
    "    elif info == 'offset_wt_non_interp':\n",
    "        unit = 'meters' \n",
    "    elif info == 'angle_wt_non_interp':\n",
    "        unit = 'degree'\n",
    "    else:\n",
    "        unit = 'Uuuups' \n",
    "    \n",
    "    return unit\n",
    "\n",
    "def count_true(results):\n",
    "    \"\"\"Function that calculates how many times the boolean True is within the list results\"\"\"\n",
    "    counter = 0\n",
    "    for i in range(0,len(results)):\n",
    "        if results[i] == True:\n",
    "            counter += 1\n",
    "    return counter\n",
    "\n",
    "\n",
    "def save_data():\n",
    "    \"\"\"Function that saves the results in a the corresponding yaml files\"\"\"\n",
    "    # Path and name of the created yaml file, the file 'BAGNAME_eng_perf_data_all.yaml' can be found\n",
    "    # in the folder behaviour_benchmarking/data/BenchmarkXY/out\n",
    "    eng_perf_data_all = path.join(outdir, name + '_eng_perf_data_all.yaml')\n",
    "\n",
    "    # Safe the eng_data_all dictionary in a yaml file called eng_perf_data_all.yaml    \n",
    "    with open(eng_perf_data_all, 'w') as yaml_file:\n",
    "        yaml.dump(eng_data_all, yaml_file, default_flow_style=False)   \n",
    "\n",
    "    # Path and name under which the static_things dictionary is saved\n",
    "    static_things_path = path.join(outdir, name + '_software_information.yaml')\n",
    "\n",
    "    # Path and name of the created yaml file, the file 'BAGNAME_software_information.yaml' can be found\n",
    "    # in the folder behaviour_benchmarking/data/BenchmarkXY/out\n",
    "    with open(static_things_path, 'w') as yaml_file:\n",
    "        yaml.dump(static_things, yaml_file, default_flow_style=False)\n",
    "        \n",
    "    final_results.update({'Engineering Data': {'Performance':eng_data_all,'Static':static_things}})\n",
    "    \n",
    "    with open(benchmark_final_results, 'w') as yaml_file:\n",
    "        yaml.dump(final_results, yaml_file, default_flow_style=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard deviation calculation of trajectories\n",
    "\n",
    "First of all, the overall mean trajectories measured by the Watchtowers (Ground Truth are compared) and the Standard Deviation is calculated. This means, that for each position within the loop, the average position (offset and heading) out of all the experiments ran of the Duckiebot measured by the Watchtowers (Ground Truth) is calculated. Based on this the standard deviations of the offset and the heading are also computed for each position within the loop.  \n",
    "\n",
    "If any of the standard deviation is too high the user is asked to run another experiment and collect more data.\n",
    "This avoids a lucky punch of the performance.\n",
    "\n",
    "To be able to calculate all this, we need to extract the ground truth trajectory (measured by the Watchtowers) out of each `BAGNAME_benchmark_results_test_XY.yaml` file.\n",
    "\n",
    "But first of all lets load all the `BAGNAME_benchmark_results_test_XY.yaml` available and extract all the data and check if the experiments have actually been done for the same type of Benchmark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please adapt `logs_path` if you stored your `BAGNAME_benchmark_results_test_XY.yaml` in another folder.\n",
    "Also, please note that it is necessary to have at least 2 different yaml files in the folder. Please make sure that the bags are named considering the naming convention explained in the documents. This means that within the folder there should at least be the files: `BAGNAME_benchmark_results_test_01.yaml` and `BAGNAME_benchmark_results_test_02.yaml`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_dir = ''\n",
    "logs_path = path.join(experiment_dir, '../data/'+Benchmark+'/benchmarks/same_bm')\n",
    "logs_path_save = path.join(experiment_dir, '../data/'+ Benchmark)\n",
    "\n",
    "#Looking what files that are fined within the destination logs_path\n",
    "localization_logs = [path.join(logs_path, f) for f in listdir(logs_path) if path.isfile(path.join(logs_path, f))]\n",
    "print(f'Logs found: {localization_logs}')\n",
    "\n",
    "#Calculates the number of Benchmarks found\n",
    "nb_bm_found = len(localization_logs)\n",
    "all_logs = []\n",
    "names=[]\n",
    "\n",
    "# loads all the data found in all the different yaml files and stores it into all_logs\n",
    "i = 0\n",
    "for filename in localization_logs:\n",
    "    with open(filename, 'r') as file:\n",
    "        current = []\n",
    "        names.append(path.basename(filename))\n",
    "        current.append(yaml.safe_load(file))\n",
    "        \n",
    "    all_logs.append(current)\n",
    "\n",
    "    i += 1\n",
    "\n",
    "#Number of experiments ran so far\n",
    "number_of_tests = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please check which `BAGNAME_benchmark_results_test_XY.yaml` is listed first above in the Logs found.\n",
    "And change the variable `first_loaded_bag` to `_benchmark_results_test_XY.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_loaded_bag = \"_benchmark_results_test_XY.yaml\"\n",
    "#extract the BAGNAME\n",
    "name_a = path.basename(names[0])\n",
    "name = name_a.replace(first_loaded_bag,\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please change the Map_Name below if you ran the experiments for your Benchmark on a different map than linus_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Map_Name = 'linus_loop'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below some lists are created to sort the different results that can be found within the files can be seperated from each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of results that have boolean as entries\n",
    "no_meaningful_rel_comp = ['Out_of_sight', 'Too_slow']\n",
    "#list of results that are not defined yet\n",
    "list_of_comp_todo = ['Position_too_slow', 'Time_needed_per_straight_tile_sec', 'Time_needed_per_curved_tile']\n",
    "#list of thing we want to check for low enough std\n",
    "meaningful_comp_results = ['Number_of_tiles_covered','Avg_time_needed_per_tile','mean_diff_btw_estimation_and_ground_truth_offset',\\\n",
    "                          'mean_diff_btw_estimation_and_ground_truth_angle', 'Abs_Measurements_db_mean_offset',\\\n",
    "                          'Abs_Measurements_db_mean_angle','Abs_Ground_truth_wt_mean_offset_interp',\\\n",
    "                          'Abs_Ground_truth_wt_mean_angle_interp','Abs_Ground_truth_wt_mean_offset_non_interp',\\\n",
    "                          'Abs_Ground_truth_wt_mean_angle_non_interp']\n",
    "#list of all the constant tolerances \n",
    "tolerances = ['Tolerance_out_of_sight', 'Tolerance_too_slow_sec', 'Theoretical_length_of_benchmark']\n",
    "#list including all resultrs that just stored trajectories\n",
    "raw_traj_info = ['int_trajs', 'all_trajectories', 'time_wt', 'time_db', 'time_db_true','angle_wt_interp','angle_db',\\\n",
    "                      'angle_wt_non_interp','angle_db_true','offset_wt_interp','offset_db','offset_wt_non_interp',\\\n",
    "                      'offset_db_true', 'all_trajectories_db']\n",
    "#list including all resultrs that just stored trajectories information\n",
    "overall_traj_info = ['angle_wt_interp','angle_db', 'angle_wt_non_interp','angle_db_true','offset_wt_interp',\\\n",
    "                     'offset_db','offset_wt_non_interp', 'offset_db_true']\n",
    "#list with all results of which we want to calculate mean etc but don't care if std is low enough\n",
    "general_info = ['Length_of_recorded_bag','Actual_length_of_benchmark','Time_out_of_sight','Time_too_slow',\\\n",
    "               'Abs_Ground_truth_wt_median_offset_non_interp','Abs_Ground_truth_wt_median_angle_non_interp',\\\n",
    "               'Abs_Ground_truth_wt_std_offset_non_interp','Abs_Ground_truth_wt_std_angle_non_interp',\\\n",
    "               'Abs_Ground_truth_wt_median_offset_interp','Abs_Ground_truth_wt_median_angle_interp',\\\n",
    "               'Abs_Ground_truth_wt_std_offset_interp','Abs_Ground_truth_wt_std_angle_interp',\\\n",
    "               'Abs_Measurements_db_std_offset','Abs_Measurements_db_std_angle','Abs_Measurements_db_median_offset',\\\n",
    "               'Abs_Measurements_db_median_angle','std_diff_btw_estimation_and_ground_truth_offset',\\\n",
    "               'std_diff_btw_estimation_and_ground_truth_angle','median_diff_btw_estimation_and_ground_truth_offset',\\\n",
    "               'median_diff_btw_estimation_and_ground_truth_angle','']\n",
    "\n",
    "#Calculates the number of properties found per file\n",
    "nb_of_properties = len(all_logs[0][0]['Results'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below checks if the Benchmarks for which the mean respectively standard deviation are calculated are of the same type. \n",
    "If not, such a comparison does not make any sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, nb_bm_found):\n",
    "    if all_logs[0][0]['Benchmark_Type'] != all_logs[i][0]['Benchmark_Type']:\n",
    "        print(\"This avg calculation work as the results come from two different Benchmarks, please stop here\\\n",
    "        and upload results from the same Benchmark type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some paths to outputs that will be generated\n",
    "benchmark_std = path.join(experiment_dir, 'out/benchmark_std.yaml')\n",
    "benchmark_mean = path.join(experiment_dir, 'out/'+ name + '_benchmark_mean.yaml')\n",
    "benchmark_final_results = path.join(experiment_dir, '../data/'+Benchmark+'/benchmarks/final/'+ name + '_benchmark_final_results.yaml')\n",
    "benchmark_final_results_wo_eng_data = path.join(experiment_dir, 'out/'+ name + '_benchmark_final_results_wo_eng_data.yaml')\n",
    "benchmark_std_graph = path.join(experiment_dir, 'out/benchmark_std_graph.jpg')\n",
    "benchmark_boxplot_graph = path.join(experiment_dir, 'out/benchmark_boxplot_graph.jpg')\n",
    "\n",
    "#some lists needed later\n",
    "offset_wt_non_interp_all = []\n",
    "angle_wt_non_interp_all = []\n",
    "offset_wt_interp_all = []\n",
    "angle_wt_interp_all = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below extracts all the needed information of all the different results file. The names of the lists where they are stored in are self explenatory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the trajectories measured by the Watchtowers (ground truth) over all experiments\n",
    "all_trajectories = []\n",
    "for log in all_logs:\n",
    "    all_cur_traj = []\n",
    "    for i in range(0, len(log[0]['Results']['all_trajectories'])):\n",
    "        cur_traj = np.array(log[0]['Results']['all_trajectories'][i])\n",
    "        all_cur_traj.append(cur_traj)\n",
    "    all_trajectories.append(all_cur_traj)\n",
    "\n",
    "# All the relative pose estimations (offset and heading) done by the Duckiebot over the time of the Benchmark\n",
    "all_trajectories_db = []\n",
    "for log in all_logs:\n",
    "    all_cur_traj = []\n",
    "    for i in range(0, len(log[0]['Results']['all_trajectories_db'])):\n",
    "        cur_traj = np.array(log[0]['Results']['all_trajectories_db'][i])\n",
    "        all_cur_traj.append(cur_traj)\n",
    "    all_trajectories_db.append(all_cur_traj)\n",
    "\n",
    "# Timestamps of the Watchtower measurements\n",
    "time_wt = {}\n",
    "for k, log in enumerate(all_logs):\n",
    "    all_cur_time = []\n",
    "    for i in range(0, len(log[0]['Results']['time_wt'])):\n",
    "        all_cur_time.append(log[0]['Results']['time_wt'][i])\n",
    "    time_wt.update({k: all_cur_time})\n",
    "\n",
    "# Timestamps of the Duckiebote pose estimations\n",
    "time_db = {}\n",
    "for k, log in enumerate(all_logs):\n",
    "    all_cur_time = []\n",
    "    for i in range(0, len(log[0]['Results']['time_db'])):\n",
    "        all_cur_time.append(log[0]['Results']['time_db'][i])\n",
    "    time_db.update({k: all_cur_time})    \n",
    "\n",
    "# Timestamps of the Duckiebot pose estimations recorded from on the Duckiebot itself (if available)\n",
    "time_db_true = {}\n",
    "for k, log in enumerate(all_logs):\n",
    "    all_cur_time = []\n",
    "    for i in range(0, len(log[0]['Results']['time_db_true'])):\n",
    "        all_cur_time.append(log[0]['Results']['time_db_true'][i])\n",
    "    time_db_true.update({k: all_cur_time})\n",
    "\n",
    "# Offsets estimated by the Duckiebot\n",
    "offset_db = {}\n",
    "for k, log in enumerate(all_logs):\n",
    "    all_cur_offset = []\n",
    "    for i in range(0, len(log[0]['Results']['offset_db'])):\n",
    "        all_cur_offset.append(log[0]['Results']['offset_db'][i])\n",
    "    offset_db.update({k: all_cur_offset})\n",
    "    \n",
    "# Offsets calculated by the Duckiebot, recorded on the Duckiebot itself (if available)\n",
    "offset_db_true = {}\n",
    "for k, log in enumerate(all_logs):\n",
    "    all_cur_offset = []\n",
    "    for i in range(0, len(log[0]['Results']['offset_db_true'])):\n",
    "        all_cur_offset.append(log[0]['Results']['offset_db_true'][i])\n",
    "    offset_db_true.update({k: all_cur_offset})\n",
    "\n",
    "# Offsets calculated by from the interpolated Watchtower measurements(ground truth)\n",
    "offset_wt_interp = {}\n",
    "for k, log in enumerate(all_logs):\n",
    "    all_cur_offset = []\n",
    "    for i in range(0, len(log[0]['Results']['offset_wt_interp'])):\n",
    "        all_cur_offset.append(log[0]['Results']['offset_wt_interp'][i])\n",
    "    offset_wt_interp.update({k: all_cur_offset})\n",
    "\n",
    "# Offsets calculated by from the non-interpolated Watchtower measurements(ground truth)    \n",
    "offset_wt_non_interp = {}\n",
    "for k, log in enumerate(all_logs):\n",
    "    all_cur_offset = []\n",
    "    for i in range(0, len(log[0]['Results']['offset_wt_non_interp'])):\n",
    "        all_cur_offset.append(log[0]['Results']['offset_wt_non_interp'][i])\n",
    "    offset_wt_non_interp.update({k: all_cur_offset})\n",
    "\n",
    "# Heading estimated by the Duckiebot\n",
    "angle_db = {}\n",
    "for k, log in enumerate(all_logs):\n",
    "    all_cur_angle = []\n",
    "    for i in range(0, len(log[0]['Results']['angle_db'])):\n",
    "        all_cur_angle.append(log[0]['Results']['angle_db'][i])\n",
    "    angle_db.update({k: all_cur_angle})\n",
    "\n",
    "# Heading calculated by the Duckiebot, recorded on the Duckiebot itself (if available)\n",
    "angle_db_true = {}\n",
    "for k, log in enumerate(all_logs):\n",
    "    all_cur_angle = []\n",
    "    for i in range(0, len(log[0]['Results']['angle_db_true'])):\n",
    "        all_cur_angle.append(log[0]['Results']['angle_db_true'][i])\n",
    "    angle_db_true.update({k: all_cur_angle})\n",
    "    \n",
    "# Heading calculated by from the interpolated Watchtower measurements(ground truth)\n",
    "angle_wt_interp = {}\n",
    "for k, log in enumerate(all_logs):\n",
    "    all_cur_angle = []\n",
    "    for i in range(0, len(log[0]['Results']['angle_wt_interp'])):\n",
    "        all_cur_angle.append(log[0]['Results']['angle_wt_interp'][i])\n",
    "    angle_wt_interp.update({k: all_cur_angle})\n",
    "\n",
    "# Heading calculated by from the non-interpolated Watchtower measurements(ground truth)\n",
    "angle_wt_non_interp = {}\n",
    "for k, log in enumerate(all_logs):\n",
    "    all_cur_angle = []\n",
    "    for i in range(0, len(log[0]['Results']['angle_wt_non_interp'])):\n",
    "        all_cur_angle.append(log[0]['Results']['angle_wt_non_interp'][i])\n",
    "    angle_wt_non_interp.update({k: all_cur_angle})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saves the runtimes of the different bags recorded and also finds the minimum\n",
    "runtime = []\n",
    "for i in range (0, nb_bm_found):\n",
    "    runtime.append(time_wt[i][-1])\n",
    "    \n",
    "min_runtime = float(min(runtime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the map used for these Benchmarks\n",
    "try:\n",
    "    del m    \n",
    "except:\n",
    "    pass\n",
    "m = dw.load_map(Map_Name)\n",
    "\n",
    "# Calculate the used lane segments by all the trajectories\n",
    "used_lane_segments_list, lane_segments_SE2 = get_used_lanes(all_trajectories)\n",
    "\n",
    "# Calculates the length of the trajectories\n",
    "mid = 30 \n",
    "cnt = collections.Counter()\n",
    "for x in used_lane_segments_list:\n",
    "    cnt[x[2]] +=1\n",
    "\n",
    "# Number of interpolation points of each tile (approximation, need to do it properly)\n",
    "pts_per_segment = {\n",
    "    'short': int(mid*1/8*math.pi),\n",
    "    'mid': (mid),\n",
    "    'long': int(mid*3/8*math.pi),\n",
    "}\n",
    "\n",
    "# Compute the center line that we will use to resample\n",
    "center_line_global, center_line_global_tfs = get_global_center_line(m,\n",
    "                                                                    used_lane_segments_list,\n",
    "                                                                    lane_segments_SE2,\n",
    "                                                                    pts_per_segment)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base transform if the plotting map is not the same as the evaluation map (i.e. plotting a subset \n",
    "# of a large map containing muliple loops)\n",
    "base_transform = np.linalg.inv(geo.SE2_from_translation_angle([0.585 * 0, 0.0], 0))\n",
    "\n",
    "all_traj_tfs = []\n",
    "all_traj_test = {}\n",
    "all_int_traj_test = {}\n",
    "\n",
    "# Compute the transforms of those trajectories for plotting\n",
    "for i,traj in enumerate(all_trajectories):\n",
    "    int_tfs_traj = []\n",
    "    for el in traj:\n",
    "        if el is not None:\n",
    "            int_tfs_traj.append(dw.SE2Transform.from_SE2(geo.SE2.multiply(base_transform, el)))\n",
    "        else:\n",
    "            int_tfs_traj.append(None)\n",
    "    all_traj_tfs.append(int_tfs_traj)\n",
    "    all_traj_test.update({i: int_tfs_traj})\n",
    "\n",
    "all_traj_tfs = np.asarray(all_traj_tfs).T.tolist()\n",
    "\n",
    "# Compute the transforms of those trajectories for plotting\n",
    "for i,traj in enumerate(all_trajectories):\n",
    "    cur_traj = []\n",
    "    cur_traj.append(traj)\n",
    "    cur_int_trajs = get_interpolated_points(center_line_global, cur_traj)\n",
    "    all_int_tfs = []\n",
    "    int_tfs_traj = []\n",
    "    for k,traj_i in enumerate(cur_int_trajs):\n",
    "        \n",
    "        for el in traj_i:\n",
    "            if el is not None:\n",
    "                int_tfs_traj.append(dw.SE2Transform.from_SE2(geo.SE2.multiply(base_transform, el)))\n",
    "            else:\n",
    "                int_tfs_traj.append(None)\n",
    "        all_int_tfs.append(int_tfs_traj)\n",
    "        \n",
    "    all_int_traj_test.update({i: int_tfs_traj})\n",
    "\n",
    "# Compute the interpolated trajectories    \n",
    "int_trajs = get_interpolated_points(center_line_global, all_trajectories)\n",
    "\n",
    "# Compute the transforms of those trajectories for plotting\n",
    "all_int_tfs = []\n",
    "for i,traj in enumerate(int_trajs):\n",
    "    int_tfs_traj = []\n",
    "    for el in traj:\n",
    "        if el is not None:\n",
    "            int_tfs_traj.append(dw.SE2Transform.from_SE2(geo.SE2.multiply(base_transform, el)))\n",
    "        else:\n",
    "            int_tfs_traj.append(None)\n",
    "    all_int_tfs.append(int_tfs_traj)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all the data ready to calculate the trajectory statistics.\n",
    "The variable names are pretty self explenatory but here is a short description:\n",
    "* mean_tfs: average trajecory computed over all the different experiments\n",
    "* mean_y: Vector with all the mean offsets calculated for each position within the loop\n",
    "* mean_heading: Vector with all the mean heading angles calculated for each position within the loop\n",
    "* std_y: Vector with all the std of the offset calculated for each position within the loop\n",
    "* std_angle: Vector with all the std of the heading angle calculated for each position within the loop\n",
    "* cv_y: Vector with all the CV factors (std/mean) of the offset calculated for each position within the loop\n",
    "* cv_heading: Vector with all the CV factors (std/mean) of the heading angle calculated for each position within the loop\n",
    "\n",
    "Note CV stands for Coefficient of Variation and it helps quantifying the dispersion of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory_info = {}\n",
    "\n",
    "mean_tfs, std_y, std_angle, start_idx, end_idx, cv_y, cv_heading, mean_y, mean_heading \\\n",
    "= get_trajectories_statistics(all_int_tfs,center_line_global_tfs)\n",
    "\n",
    "# save all the trajectory info found\n",
    "trajectory_info.update({'Mean offset':mean_y,'Std offset':std_y,'CV offset':{'all':cv_y,'mean':Average(cv_y),\\\n",
    "                                                                             'std':statistics.stdev(cv_y),\\\n",
    "                                                                             'min':min(cv_y),'max':max(cv_y)},\\\n",
    "                        'Mean angle':mean_heading,'Std angle':std_angle,'CV angle':{'all':cv_heading,'mean':Average(cv_heading),\\\n",
    "                                                                                    'std':statistics.stdev(cv_heading),\\\n",
    "                                                                                    'min':min(cv_heading),'max':max(cv_heading)}})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below scans through the vectors `cv_y` and `cv_heading` to check no more than 30 of the entries are above 1.0\n",
    "If this is the case, please run another experiment to collect more data.\n",
    "\n",
    "\n",
    "If this is not the case you have collected enough data trajectory wise and you can continue running this Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enough_data_traj = True\n",
    "cnt_cv_y = 0\n",
    "cnt_cv_heading = 0\n",
    "for i in cv_y:\n",
    "    if i > 1.0:\n",
    "        cnt_cv_y += 1        \n",
    "for i in cv_heading:\n",
    "    if i > 1.0:\n",
    "        cnt_cv_heading += 1\n",
    "\n",
    "#checks if at least 75% of all calculated CV are below 1\n",
    "if cnt_cv_y > len(cv_y)*0.25:\n",
    "    enough_data_traj = False\n",
    "    print(\"Std of the lateral offset is too large which means that the trajectory is not stable enough, and we can not be sure that you code just had a lucky run or not.\" )\n",
    "\n",
    "#check if at least 25% of all calculated CV of the heading are below one\n",
    "#this is less trict as the heading does vary way more then de offset and mainly the offset is important for \n",
    "#stability. However the check is still made such that not all heading measurements have a too large CV\n",
    "if cnt_cv_heading > len(cv_heading)*0.75:\n",
    "    enough_data_traj = False        \n",
    "    print(\"Std of the heading is too large which means that the trajectory is not stable enough, and we can not be sure that you code just had a lucky run or not.\")\n",
    "    \n",
    "if enough_data_traj:\n",
    "    print(\"Trajectory wise you have collected enough data, however there are a couple of more things to check.\\n So please keep running the upcoming cells.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below the mean trajectory of all your experiments is plotted in red circles.\n",
    "\n",
    "Also all the interpolated trajectories of the different experiments are ploted in other colors in circles that are a bit smaller than the ones of the overall mean trajectory.\n",
    "\n",
    "This visualization can help you understand how \"stable\" you trajectory data is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the map\n",
    "del m\n",
    "m = dw.load_map(Map_Name)\n",
    "\n",
    "# colors in which the different trajectories will be drawn\n",
    "colors = ['green','cyan','blue','yellow','magenta']\n",
    "# draw all the different trajectories\n",
    "for k in range(0, nb_bm_found):\n",
    "    current_traj_tfs = all_int_traj_test[k]\n",
    "    for i, meant_tf in enumerate(current_traj_tfs):\n",
    "        if not(i%2):\n",
    "            if current_traj_tfs[i] != None:\n",
    "                a = k%5\n",
    "                m.set_object('Data-%s'% k+str(i + 10000), Circle(0.008, color=colors[a]), ground_truth=meant_tf)\n",
    "    print(names[k] + ':' + colors[a])\n",
    "# draw the average trajectory of all the Benchmarks\n",
    "for i, meant_tf in enumerate(mean_tfs):\n",
    "    if not(i%2):\n",
    "        m.set_object(str(i), Circle(0.015, color='red'), ground_truth=meant_tf)\n",
    "\n",
    "# Draw!\n",
    "ipython_draw_svg(m);\n",
    "    \n",
    "# Save the image\n",
    "outdir_im = path.join('/home/linuslingg/out', \"ipython_draw_svg\", \"%s\" % id(m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next the remaining data is checked. This means that the following results are compared to each other to check if the results of your Benchmark are stable enough or not. \n",
    "\n",
    "* Number_of_tiles_covered\n",
    "* Avg_time_needed_per_tile\n",
    "* mean_diff_btw_estimation_and_ground_truth_offset\n",
    "* mean_diff_btw_estimation_and_ground_truth_angle\n",
    "* Abs_Measurements_db_mean_offset\n",
    "* Abs_Measurements_db_mean_angle\n",
    "* Abs_Ground_truth_wt_mean_offset_interp\n",
    "* Abs_Ground_truth_wt_mean_angle_interp\n",
    "* Abs_Ground_truth_wt_mean_offset_non_interp\n",
    "* Abs_Ground_truth_wt_mean_angle_non_interp\n",
    "\n",
    "Based on this analysis the decision is made if another experiment run has to be done or if enough data has been collected to make a meaningful conclusion.\n",
    "\n",
    "To visualize this analysis so the user gets a better understanding of why he has enough data or not, the boxplots as well as the the Error bar plots are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plots mean and Standard Deviation \n",
    "mean_bm = {'Benchmark_Type': all_logs[0][0]['Benchmark_Type']}\n",
    "std_bm = {'Benchmark_Type': all_logs[0][0]['Benchmark_Type']}\n",
    "final_results = {'Benchmark_Type': all_logs[0][0]['Benchmark_Type']}\n",
    "all_results_gen = {}\n",
    "all_results_mean = {}\n",
    "all_results_std = {}\n",
    "nb_of_oos = 0\n",
    "nb_of_ts = 0\n",
    "enough_data_res = []\n",
    "\n",
    "# Prepare figure for the plots, on one hand the Error bar plots and on the other hand the Boxplots\n",
    "fig1, axes1 = plt.subplots(nrows=ceil(len(meaningful_comp_results)/2), ncols=2)\n",
    "fig2, axes2 = plt.subplots(nrows=ceil(len(meaningful_comp_results)/2), ncols=2)\n",
    "fig1.subplots_adjust(hspace=0.5)\n",
    "fig1.suptitle('Error bar plot', y=.895)\n",
    "fig2.subplots_adjust(hspace=0.5)\n",
    "fig2.suptitle('Boxplot', y=.895)\n",
    "\n",
    "#Saves the number of experiments that were run in the dictionary.\n",
    "mean_bm.update({'Number of tests ran': number_of_tests})\n",
    "final_results.update({'Number of tests ran': number_of_tests})\n",
    "#Saves the actual length of the different experiments that were run in the dictionary.\n",
    "final_results.update({'runtime': runtime})\n",
    "\n",
    "#Counter for the plots\n",
    "count = 0\n",
    "\n",
    "#Scroll through all the different results stored within the files\n",
    "for i, prop in enumerate(all_logs[0][0]['Results']):\n",
    "    results = []\n",
    "    current_prop = {}\n",
    "    std_exist = False\n",
    "    keep_orig = False\n",
    "    save_just_col = False\n",
    "    overall_traj_inf_bool = False\n",
    "    #extracts the results from the different runs\n",
    "    for j in range(0, nb_bm_found):\n",
    "            results.append(all_logs[j][0]['Results'][prop])\n",
    "    \n",
    "    # Saves all the results found in the dictionary 'current_prop'\n",
    "    current_prop.update({'All results': results})\n",
    "    # Get the unit of the current prop we are looking at\n",
    "    unit = get_unit(prop)\n",
    "    \n",
    "    #checks if the current property is one that is not yet implemented (Ex. marked with 'ToDo')\n",
    "    if (prop not in list_of_comp_todo): \n",
    "        #Checks if the current property is in the list of properties that do not have a meaningful\n",
    "        #relative comparison, respectively which have booleans stored\n",
    "        if prop in no_meaningful_rel_comp:\n",
    "            #those variables are not applicable as boolean results\n",
    "            std_result = 'N/A'\n",
    "            max_result = 'N/A'\n",
    "            min_result = 'N/A'\n",
    "            cv_result = 'N/A'\n",
    "            median_result = 'N/A'\n",
    "            #returns the number of 'True's found within the results, which helps counting the number of crashes\n",
    "            counter = count_true(results)\n",
    "            avg_result = '{} failures'.format(counter)\n",
    "        #Checks if the current proposition is within either the list of general information or the list \n",
    "        #meaningful_comp_results (details of those lists can be found above)\n",
    "        elif prop in meaningful_comp_results or prop in general_info:\n",
    "            #Calculates the average of a list\n",
    "            avg_result = Average(results)\n",
    "            #Calculate the median, std, max and min of the results from the current property\n",
    "            median_result = statistics.median(results)\n",
    "            std_result = statistics.stdev(results)\n",
    "            max_result = max(results)\n",
    "            min_result = min(results)\n",
    "            \n",
    "            # coefficient of variation calculation:\n",
    "            # if lower than 1, the std can be considered small enough and one can stop running tests\n",
    "            # https://www.researchgate.net/post/What_do_you_consider_a_good_standard_deviation\n",
    "            # First checks if the avg is different from 0 to avoid a division by 0\n",
    "            # If avg is 0 and std is 0 the cv is still 0\n",
    "            # However, if avg is 0 and std is defferent from 0 then the CV is large\n",
    "            if avg_result != 0:\n",
    "                cv_result = std_result/avg_result\n",
    "                cv_median = std_result/median_result\n",
    "            else:\n",
    "                if std_result == 0:\n",
    "                    cv_result = 0\n",
    "                else:\n",
    "                    cv_result = 100\n",
    "            \n",
    "            #if current property is in meaningful_comp_results we want to check if the std is low enough\n",
    "            #this is why in this case we set std_exist = True\n",
    "            if prop in meaningful_comp_results:\n",
    "                std_exist = True\n",
    "        \n",
    "        #if the current property is in the tolerances list we just keep the original information as those \n",
    "        # should and do not change over the different experiments\n",
    "        elif prop in tolerances:\n",
    "            keep_orig = True\n",
    "            cur_orig = results[0]\n",
    "        \n",
    "        #if the current property is within the list seen below, we just save the collected data and do not analyse it\n",
    "        elif prop in ('begin_time_stamp_wt','Number_of_completed_laps'):\n",
    "            save_just_col = True\n",
    "        \n",
    "        # Checks if the current property is within the list raw_traj_info\n",
    "        elif prop in raw_traj_info:\n",
    "            # if the current property is within the list overall_traj_info, we calculate if possible (hence if data is\n",
    "            # available and it is not written 'Not available') the statistics over all the experiments ran\n",
    "            # also we set the boolean 'overall_traj_inf_bool' equal True.\n",
    "            if prop in overall_traj_info:\n",
    "                if results[0] == 'Not available':\n",
    "                    save_just_col = True\n",
    "                else:\n",
    "                    results  = np.hstack(results)\n",
    "                    \n",
    "                    overall_traj_inf_bool = True\n",
    "                    avg_result = float(np.round(np.mean(results),6))\n",
    "                    median_result = float(np.round(np.median(results),6))\n",
    "                    std_result = float(np.round(np.std(results),6))\n",
    "                    max_result = float(np.round(max(results),6))\n",
    "                    min_result = float(np.round(min(results),6))\n",
    "                    \n",
    "                    if avg_result != 0:\n",
    "                        cv_result = float(np.round(std_result/avg_result,6))\n",
    "                        cv_median = float(np.round(std_result/median_result,6))\n",
    "\n",
    "                    else:\n",
    "                        if std_result == 0:\n",
    "                            cv_result = 0.0\n",
    "                        else:\n",
    "                            cv_result = 100.0\n",
    "                            \n",
    "            #if the current property is not in the list overall_traj_info, we just save all the collected data\n",
    "            #without further analysing it\n",
    "            else:\n",
    "                save_just_col = True\n",
    "                \n",
    "    # Save ToDo in the dictionary if the value of the current property is 'ToDo'\n",
    "    else:\n",
    "        keep_orig = True\n",
    "        cur_orig = 'ToDO'\n",
    "\n",
    "    #if the standard deviation exist, we check if it is small enough for the current property\n",
    "    #for this we check the coefficient of variation (if it is lower 1 or not)\n",
    "    #If for one property this CV is larger than 1 this property will be marked red within the plots (otw green)\n",
    "    # and the user is asked to collect more data\n",
    "    if std_exist:\n",
    "        if cv_result >= 1.0:\n",
    "            color = 'red'\n",
    "            answer = 'no'\n",
    "            enough_data_res.append(False)\n",
    "        elif cv_result < 1.0:\n",
    "            color = 'green'\n",
    "            answer = 'yes'\n",
    "            enough_data_res.append(True)\n",
    "        else:\n",
    "            color = 'black'\n",
    "            answer = 'Sth went wrong'\n",
    "            enough_data_res.append(False)\n",
    "    \n",
    "    # checks if there was some analysis of the data done above or not\n",
    "    if not save_just_col and not keep_orig:\n",
    "        #if there was it checks if we care about the standard deviation of this property or not\n",
    "        if std_exist:\n",
    "            #if we do, we update the dictionnary and we plot the results (Boxlot and Error bar plot).\n",
    "            current_prop.update({'Mean': avg_result, 'Std': std_result, 'coefficient of variation calculation': cv_result,\\\n",
    "                                 'Median': median_result, 'Max': max_result, 'Min': min_result, 'Unit': unit,\\\n",
    "                                 'Enough tests run for meaningful BM analyzis': answer})\n",
    "            \n",
    "            # Figure 1: Mean and Std\n",
    "            ax1 = axes1.flatten()[count]\n",
    "            ax1.set_ylabel(unit)\n",
    "            ax1.set_xlabel(prop)\n",
    "            ax1.set_xticklabels([])\n",
    "            ax1.errorbar(1, avg_result, yerr=std_result, linestyle='None', marker='^', ecolor = color)\n",
    "            ax1.xaxis.label.set_color(color)\n",
    "            # Figure 2: Boxplot:\n",
    "            ax2 = axes2.flatten()[count]\n",
    "            ax2.set_ylabel(unit)\n",
    "            ax2.set_xlabel(prop)\n",
    "            test = [1.0,2.0,3.0,4.0]\n",
    "            ax2.boxplot(results, showmeans = True, notch=False, patch_artist=True,boxprops=dict(color=color,facecolor=(0.5,0.5,0.5,0.5)))\n",
    "            ax2.set_xticklabels([])\n",
    "            ax2.xaxis.label.set_color(color)\n",
    "            #Note if the CV of the current property is too large the property will be marked red otw. green\n",
    "            y = results\n",
    "            x = np.random.normal(1, 0.004, size=len(y))\n",
    "            ax2.plot(x, y, 'ro', alpha=0.5)\n",
    "            x = np.random.normal(1, 0.0, size=len(y))\n",
    "            ax1.plot(x, y, 'ro', alpha=0.5)\n",
    "\n",
    "            count += 1\n",
    "            \n",
    "            all_results_std.update({'Std of ' + prop: std_result})\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            #if no std exists, respectively if we don't care about the stv we just save the found data in the dictionary\n",
    "            current_prop.update({'Mean': avg_result, 'Std': std_result, 'coefficient of variation calculation': cv_result,\\\n",
    "                                 'Median': median_result, 'Max': max_result, 'Min': min_result, 'Unit': unit})\n",
    "    \n",
    "    #update the dictionary all_results_mean\n",
    "    all_results_mean.update({prop: current_prop})\n",
    "    #checks if we just want to save the original data or also the things analysed\n",
    "    if not keep_orig:\n",
    "        all_results_gen.update({prop: current_prop})\n",
    "    else:\n",
    "        #saves the original data with its unit\n",
    "        all_results_gen.update({prop+' ['+unit+']': cur_orig})\n",
    "\n",
    "#updates the dictioaries\n",
    "std_bm.update( {\"Results\" : all_results_std} )\n",
    "mean_bm.update( {\"Results\" : all_results_mean} )\n",
    "\n",
    "\n",
    "fig1.set_figheight(40)\n",
    "fig1.set_figwidth(15)\n",
    "fig2.set_figheight(40)\n",
    "fig2.set_figwidth(15)\n",
    "\n",
    "#Saves the plots\n",
    "fig1.savefig(benchmark_std_graph, dpi=None, facecolor='w', edgecolor='w',\n",
    "        orientation='portrait', papertype=None, format=None,\n",
    "        transparent=False, bbox_inches=None, pad_inches=0.1,\n",
    "        frameon=None, metadata=None)\n",
    "fig2.savefig(benchmark_boxplot_graph, dpi=None, facecolor='w', edgecolor='w',\n",
    "        orientation='portrait', papertype=None, format=None,\n",
    "        transparent=False, bbox_inches=None, pad_inches=0.1,\n",
    "        frameon=None, metadata=None)\n",
    "\n",
    "#saves the yaml file including all the std analysis.\n",
    "with open(benchmark_std, 'w') as yaml_file:\n",
    "    yaml.dump(std_bm, yaml_file, default_flow_style=False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if any(enough_data_res) == False:\n",
    "    enough_data_res_fin = False\n",
    "    print(\"unfortunately you did not collect enough data yet please run another experiment, analyse the date with \\\n",
    "    the notebook 95, add the results to the folder and then run this notebook from the beginning again\")\n",
    "else:\n",
    "    enough_data_res_fin = True\n",
    "    print(\"You collected enough data and can now continue withe the cell below to analyse the engineering\\\n",
    "    data and save all the results found\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get a green light of all different parts (meaning that the name is colored in green), please run the cell below to save the overall mean of the results to a yamle file with the name `BAGNAME_benchmark_final_results_wo_eng_data.yaml`.\n",
    "\n",
    "If you do not get a green light and still have some red colored names in the graphs above please run another experiment as the variation of the results is too high and nothing can really be said about the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updates the bag\n",
    "final_results.update({'Enough data traj wise': enough_data_traj})\n",
    "final_results.update({'Enough data results wise': enough_data_res_fin})\n",
    "final_results.update({'Results': all_results_gen})\n",
    "final_results.update({'Trajectory info': trajectory_info})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(benchmark_final_results_wo_eng_data, 'w') as yaml_file:\n",
    "    yaml.dump(final_results, yaml_file, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Engineering Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please run cell by cell following the instructions.\n",
    "\n",
    "First load the .yaml file that was computed by the hw_check you ran earlier. For this, change the variable hw_check_yaml_name below to the name your file has (Ex. CH_ETHZ_Linus_2020-05-05_DB18_hardware-compliance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hw_check_yaml_name = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the file of the hardware check is at a different location, please change the logs_path below\n",
    "experiment_dir = ''\n",
    "outdir = path.join(logs_path_save, 'out')\n",
    "hw_check_yaml_file = r'../data/'+Benchmark+'/' + hw_check_yaml_name + '.yaml'\n",
    "# Dictionary containing all the information\n",
    "eng_data_all = {}\n",
    "dict_total_cnst = {}\n",
    "static_things = {}\n",
    "\n",
    "# dictionary in which the hw info will be saved\n",
    "hw_info = {'item': 'documentation'}\n",
    "#load the file and save it in the dictionary\n",
    "with open(hw_check_yaml_file) as file:\n",
    "    documents = yaml.full_load(file)\n",
    "    hostname = documents[\"hostname\"]\n",
    "    for item, doc in documents.items():\n",
    "        hw_info.update({item: doc})\n",
    "        print(item, \":\", doc)\n",
    "\n",
    "# save the data found\n",
    "eng_data_all.update({'HW_info': hw_info}) \n",
    "\n",
    "#extract the hostname\n",
    "hostname_minus = hostname+'-'\n",
    "hostname_underline = hostname+'_'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you did not record a bag on the Duckiebot, please set the variable `db_data` below to `False`. If you did record a bag on the Duckiebot at least once and you successfully ran rosbag analysis with it set it to `True`.\n",
    "\n",
    "If you did not run the Diagnostic toolbox during any experiment please set the variable `diag_toolbox_data` below to `False`. If you did run the diagnostic toolbox at least once please set it to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_data = True\n",
    "diag_toolbox_data = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update the dictionary containing all the final results\n",
    "final_results.update({'db_data': db_data})\n",
    "final_results.update({'diag_toolbox_data': diag_toolbox_data})\n",
    "\n",
    "\n",
    "if (not db_data) and (not diag_toolbox_data):\n",
    "    print(\"Saving the data that was found, you can stop running this notebook now and start the final comparison\\\n",
    "    notebook called: 96_..\")\n",
    "    save_data()\n",
    "elif not db_data and diag_toolbox_data:\n",
    "    print(\"Skip all the cells and jump straight to the cell with the title: Diagnostic toolbox analysis \")\n",
    "elif not diag_toolbox_data and db_data:\n",
    "    print(\"Run all the cells up to the cell with the title: Diagnostic toolbox analysis then stop\")\n",
    "elif diag_toolbox_data and db_data:\n",
    "    print(\"Run all the cells up to the cells up until the end of the notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note, that if somehow the analyze_rosbag did not generate one of the `.json` files mentioned in the upcoming cells, please just skip the cells corresponding to that `.json` file and continue with the one after.\n",
    "\n",
    "Run the next cell such that the upload button appears. Then click on the button and select at least one BAGNAME_duckiebot_segment_count.json file that you should find within the folder data/BenchmarkXY/json. If you have run several experiments and have therefore several files please select all of them!\n",
    "Make sure that after selecting the file, you click into the cell below the upload button before continuing to run.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_count = FileUpload(accept='.json',\n",
    "    multiple=True)\n",
    "segment_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you click into the cell below after you selected the correct .json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert segment_count.data, 'File missing, please upload in above cell'\n",
    "all_data_dict = {}\n",
    "all_seg_cnt = {}\n",
    "overall_all_segment_count = []\n",
    "start_rec_time = []\n",
    "#checks how many files that were uploaded\n",
    "nb_files = len(segment_count.value)\n",
    "#counts the number of messages found \n",
    "nb_segm_cnts = len(json.loads(segment_count.data[0].decode('utf-8'))['meas'])\n",
    "\n",
    "\n",
    "\n",
    "# This loads all the file and saves number of segments into the eng_data_all dictionary \n",
    "for i in range(0, len(segment_count.value)):\n",
    "    all_segment_count = []\n",
    "    cnt_valid_meas = 0\n",
    "    dict_total_segment_count = {}\n",
    "    data = json.loads(segment_count.data[i].decode('utf-8'))\n",
    "    all_data_dict.update({i: data})\n",
    "    start_rec_time.append(data['time'][0])\n",
    "    for j in range(0,len(data['meas'])):\n",
    "        time_stamp = data['time'][j]\n",
    "        if time_stamp-float(start_rec_time[i]) > float(runtime[i]):\n",
    "            break\n",
    "        \n",
    "        cnt_valid_meas += 1\n",
    "        dict_cur = {'Segments': data['meas'][j]}\n",
    "        all_segment_count.append(data['meas'][j])\n",
    "        overall_all_segment_count.append(data['meas'][j])\n",
    "        \n",
    "        #uncomment the following line, if you want to save the data of each file and not just the mean and the std of them\n",
    "        #dict_total_segment_count.update({time_stamp: dict_cur})\n",
    "    \n",
    "    #change the format to float\n",
    "    all_segment_count_fl = [float(k) for k in all_segment_count]    \n",
    "\n",
    "    # Calculate the mean, median, min, max and std of the latencies measured at different time stamps of current file.\n",
    "    mean_segment_count = statistics.mean(all_segment_count_fl)\n",
    "    median_segment_count = statistics.median(all_segment_count_fl)\n",
    "    min_segment_count = min(all_segment_count_fl)\n",
    "    max_segment_count = max(all_segment_count_fl)   \n",
    "    \n",
    "    #checks if we have enough data to calculate the std and cv\n",
    "    if nb_segm_cnts > 1 and cnt_valid_meas > 1:\n",
    "        std_segment_count = statistics.stdev(all_segment_count_fl)\n",
    "        if mean_segment_count != 0:\n",
    "            cv_segment_count = std_segment_count/mean_segment_count\n",
    "        else:\n",
    "            if std_segment_count == 0:\n",
    "                cv_segment_count = 0\n",
    "            else:\n",
    "                cv_segment_count = 10\n",
    "    else: \n",
    "        std_segment_count = 'N/A as only once a segment count published'\n",
    "        cv_segment_count = 'N/A as only once a segment count published'\n",
    "        \n",
    "    dict_total_segment_count.update({'mean segment count': mean_segment_count, 'median segment count': median_segment_count, \\\n",
    "                           'std segment count': float(np.round(std_segment_count,4)),'CV segment count': float(np.round(cv_segment_count,4)),\\\n",
    "                           'min segment count': min_segment_count, 'max segment count': max_segment_count})\n",
    "\n",
    "    # Add all the information to the eng_data_all dictionary if wanted\n",
    "    all_seg_cnt.update({'Segment count of file ' + str(i): dict_total_segment_count})    \n",
    "\n",
    "#change the format    \n",
    "overall_all_segment_count_fl = [float(k) for k in overall_all_segment_count]    \n",
    "\n",
    "# Calculate the mean, median, min, max and std of the latencies measured at different time stamps of current file.\n",
    "overall_mean_segment_count = statistics.mean(overall_all_segment_count_fl)\n",
    "overall_median_segment_count = statistics.median(overall_all_segment_count_fl)\n",
    "overall_min_segment_count = min(overall_all_segment_count_fl)\n",
    "overall_max_segment_count = max(overall_all_segment_count_fl)\n",
    "if nb_files > 1 or (nb_segm_cnts > 1 and cnt_valid_meas > 1):\n",
    "    overall_std_segment_count = statistics.stdev(overall_all_segment_count_fl)\n",
    "    if overall_mean_segment_count != 0:\n",
    "        overall_cv_segment_count = overall_std_segment_count/overall_mean_segment_count\n",
    "    else:\n",
    "        if overall_std_segment_count == 0:\n",
    "            overall_cv_segment_count = 0\n",
    "        else:\n",
    "            overall_cv_segment_count = 10\n",
    "    \n",
    "else:\n",
    "    overall_std_segment_count = 'N/A as only one file uploaded'\n",
    "    overall_cv_segment_count = 'N/A as only one file uploaded'\n",
    "\n",
    "#saves the date in the dictionary\n",
    "all_seg_cnt.update({'Overall mean segment count': overall_mean_segment_count, \\\n",
    "                'Overall median segment count': overall_median_segment_count, \\\n",
    "                'Overall std segment count': float(np.round(overall_std_segment_count,4)), \\\n",
    "                'Overall CV segment count': float(np.round(overall_cv_segment_count,4)),\\\n",
    "                'Overall min segment count': overall_min_segment_count, \\\n",
    "                'Overall max segment count': overall_max_segment_count})\n",
    "    \n",
    "eng_data_all.update({'Segment Count': {'Nb of files': nb_files, 'Nb of segment count meas. per file': nb_segm_cnts, \\\n",
    "                                 'Measurements': all_seg_cnt}})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next cell such that the upload button appears. Then click on the button and select at least one  BAGNAME_duckiebot_latencies.json file that you should find within the folder data/BenchmarkXY/json. If you have run several experiments and have therefore several files please select all of them! Make sure that after selecting the file, you click into the cell below the upload button before continuing to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latency = FileUpload(accept='.json',\n",
    "    multiple=True)\n",
    "latency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you click into the cell below after you selected the correct .json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert latency.data, 'File missing, please upload in above cell'\n",
    "all_data_dict = {}\n",
    "all_lat = {}\n",
    "overall_all_latencies = []\n",
    "\n",
    "#checks how many files that were uploaded\n",
    "nb_files = len(latency.value)\n",
    "#checks how many latencies that were measured\n",
    "nb_lat_meas = len(json.loads(latency.data[0].decode('utf-8'))['meas'])\n",
    "\n",
    "# This loads all the file and saves latencies into the eng_data_all dictionary \n",
    "# This is the total lag up to and including the detector node, it is measured in ms at different time stamps\n",
    "for i in range(0, len(latency.value)):\n",
    "    all_latencies = []\n",
    "    cnt_valid_meas = 0\n",
    "    dict_total_lat = {}\n",
    "    #load the data\n",
    "    data = json.loads(latency.data[i].decode('utf-8'))\n",
    "    all_data_dict.update({i: data})\n",
    "    for j in range(0,len(data['meas'])):\n",
    "        time_stamp = data['time'][j]\n",
    "        # This should be stoped if the time where the latency was published\n",
    "        # is after the crash occured, compare this time to the \n",
    "#         if time_stamp-float(start_rec_time[i]) > float(runtime[i]):\n",
    "#             break\n",
    "        cnt_valid_meas += 1\n",
    "        dict_cur = {'latency': data['meas'][j] + \" ms\"}\n",
    "        all_latencies.append(data['meas'][j])\n",
    "        overall_all_latencies.append(data['meas'][j])\n",
    "        \n",
    "        #uncomment the following line, if you want to save the data of each file and not just the mean and the std of them\n",
    "        #dict_total_lat.update({time_stamp: dict_cur})\n",
    "    \n",
    "    #change the format\n",
    "    all_latencies_fl = [float(k) for k in all_latencies]    \n",
    "\n",
    "    # Calculate the mean, median, min, max and std of the latencies measured at different time stamps of current file.\n",
    "    mean_latency = statistics.mean(all_latencies_fl)\n",
    "    median_latency = statistics.median(all_latencies_fl)\n",
    "    min_latency = min(all_latencies_fl)\n",
    "    max_latency = max(all_latencies_fl)\n",
    "    \n",
    "    #checks if we have enough data to calculate std and cv\n",
    "    if nb_lat_meas > 1 and cnt_valid_meas > 1:\n",
    "        std_latency = float(np.round(statistics.stdev(all_latencies_fl),4))\n",
    "        if mean_latency != 0:\n",
    "            cv_lat = float(np.round(std_latency/mean_latency,4))\n",
    "        else:\n",
    "            if std_latency == 0:\n",
    "                cv_lat = 0\n",
    "            else:\n",
    "                cv_lat = 10\n",
    "    else: \n",
    "        std_latency = 'N/A as only one valid latency measured'\n",
    "        cv_lat = 'N/A as only one valid latency measured'\n",
    "        \n",
    "    dict_total_lat.update({'mean latency (ms)': mean_latency, 'median latency (ms)': median_latency, \\\n",
    "                           'std latency (ms)': std_latency,'CV latency': cv_lat,\\\n",
    "                           'min latency (ms)': min_latency, 'max latency(ms)': max_latency})\n",
    "\n",
    "    # Add all the information to the eng_data_all dictionary if wanted\n",
    "    all_lat.update({'Latency of file ' + str(i): dict_total_lat})    \n",
    "\n",
    "#change the format    \n",
    "overall_all_latencies_fl = [float(k) for k in overall_all_latencies]    \n",
    "\n",
    "\n",
    "\n",
    "#checks if we have enough measurements to calculate the std and cv\n",
    "if (nb_files > 1 and cnt_valid_meas > 1) or (nb_lat_meas > 1 and cnt_valid_meas > 1) :\n",
    "    # Calculate the mean, median, min, max and std of the latencies measured at different time stamps of current file.\n",
    "    overall_mean_latency = statistics.mean(overall_all_latencies_fl)\n",
    "    overall_median_latency = statistics.median(overall_all_latencies_fl)\n",
    "    overall_min_latency = min(overall_all_latencies_fl)\n",
    "    overall_max_latency = max(overall_all_latencies_fl)\n",
    "    overall_std_latency = float(np.round(statistics.stdev(overall_all_latencies_fl),4))\n",
    "    if overall_mean_latency != 0:\n",
    "        overall_cv_lat = float(np.round(overall_std_latency/overall_mean_latency,4))\n",
    "    else:\n",
    "        if overall_std_latency == 0:\n",
    "            overall_cv_lat = 0\n",
    "        else:\n",
    "            overall_cv_lat = 10\n",
    "    \n",
    "else:\n",
    "    # Calculate the mean, median, min, max and std of the latencies measured at different time stamps of current file.\n",
    "    overall_mean_latency = overall_all_latencies_fl\n",
    "    overall_median_latency = overall_all_latencies_fl\n",
    "    overall_min_latency = min(overall_all_latencies_fl)\n",
    "    overall_max_latency = max(overall_all_latencies_fl)\n",
    "    overall_std_latency = 'N/A as only one file uploaded'\n",
    "    overall_cv_lat = 'N/A as only one file uploaded'\n",
    "\n",
    "all_lat.update({'Overall mean latency (ms)': overall_mean_latency, 'Overall median latency (ms)': overall_median_latency, \\\n",
    "                'Overall std latency (ms)': overall_std_latency, 'Overall CV latency': overall_cv_lat,\\\n",
    "                'Overall min latency (ms)': overall_min_latency, 'Overall max latency(ms)': overall_max_latency})\n",
    "    \n",
    "eng_data_all.update({'Latency': {'Nb of files': nb_files, 'Nb of latency meas. per file': nb_lat_meas, \\\n",
    "                                 'Measurements': all_lat}}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next cell such that the upload button appears. Then click on the button and select at least one BAGNAME_duckiebot_node_info.json file that you should find within the folder data/BenchmarkXY/json. \n",
    "If you have run several experiments and have therefore several files please select all of them! \n",
    "Make sure that after selecting the file, you click into the cell below the upload button before continuing to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_inf = FileUpload(accept='.json',\n",
    "    multiple=True)\n",
    "node_inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you click into the cell below after you selected the correct .json file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert node_inf.data, 'File missing, please upload in above cell'\n",
    "\n",
    "all_data_dict = {}\n",
    "dict_total = {}\n",
    "all_nodes = {}\n",
    "#checks how many nodes that were recorded\n",
    "nb_nodes = len(json.loads(node_inf.data[0].decode('utf-8'))['node'])\n",
    "#checks how many files that were uploaded\n",
    "nb_files = len(node_inf.value)\n",
    "\n",
    "#extract the uploaded information\n",
    "for i in range(0, len(node_inf.value)):\n",
    "    data = json.loads(node_inf.data[i].decode('utf-8'))\n",
    "    all_data_dict.update({i: data})\n",
    "    # This loads the file and saves all the informations about the nodes into the eng_data_all dictionary \n",
    "    for j in range(0,len(data['node'])):\n",
    "        dict_cur = {'frequency (Hz)': float(np.round(data['frequency'][j],4)), 'message_count': float(np.round(data['message_count'][j],4)), \\\n",
    "                    'connections': float(np.round(data['connections'][j],4))}\n",
    "        node_name = data['node'][j]\n",
    "        dict_total[node_name] = dict_cur\n",
    "        \n",
    "    #uncomment the following line, if you want to save the data of each file and not just the mean and the std of them\n",
    "    #eng_data_all.update({'Node Info of file'+ str(i): dict_total})\n",
    "\n",
    "    \n",
    "if nb_files > 1:\n",
    "    #if there are more then one file uploaded calculate the mean, std and cv of all of the nodes\n",
    "    for i in range(0,nb_nodes):\n",
    "        current ={}\n",
    "        freq_all = []\n",
    "        mes_cnt_all = []\n",
    "        nb_connections_all = []\n",
    "        for j in range(0, nb_files):\n",
    "            freq_all.append(all_data_dict[j]['frequency'][i])\n",
    "            mes_cnt_all.append(all_data_dict[j]['message_count'][i])\n",
    "            nb_connections_all.append(all_data_dict[j]['connections'][i])\n",
    "\n",
    "        current.update({'Mean frequency (Hz)': float(np.round(statistics.mean(freq_all),4)), \\\n",
    "                        'Std frequency (Hz)': float(np.round(statistics.stdev(freq_all),4)), \\\n",
    "                        'CV frequency (Hz)': float(np.round(statistics.stdev(freq_all)/statistics.mean(freq_all),4)), \\\n",
    "                        'Mean message_count':  float(np.round(statistics.mean(mes_cnt_all),4)), \\\n",
    "                        'Std message_count': float(np.round(statistics.stdev(mes_cnt_all),4)),\\\n",
    "                        'CV message_count': float(np.round(statistics.stdev(mes_cnt_all)/statistics.mean(mes_cnt_all),4)), \\\n",
    "                        'Mean connections':  float(np.round(statistics.mean(nb_connections_all),4)),\\\n",
    "                        'Std connections': float(np.round(statistics.stdev(nb_connections_all),4)),\\\n",
    "                        'CV connections': float(np.round(statistics.stdev(nb_connections_all)/statistics.mean(nb_connections_all),4))})\n",
    "\n",
    "        all_nodes.update({all_data_dict[0]['node'][i]: current})\n",
    "#  float(np.round(overall_swap_median,4))   \n",
    "else: \n",
    "    #if only one file is uploaded, the one measurement is taken as mean value\n",
    "    for i in range(0,nb_nodes):\n",
    "        current ={}\n",
    "        current.update({'Mean frequency (Hz)': float(np.round(all_data_dict[0]['frequency'][i],4)), \\\n",
    "                            'Std frequency (Hz)': 'N/A as only one file uploaded', \\\n",
    "                            'CV frequency (Hz)': 'N/A as only one file uploaded', \\\n",
    "                            'Mean message_count':  float(np.round(all_data_dict[0]['message_count'][i],4)), \\\n",
    "                            'Std message_count': 'N/A as only one file uploaded',\\\n",
    "                            'CV message_count': 'N/A as only one file uploaded', \\\n",
    "                            'Mean connections':  float(np.round(all_data_dict[0]['connections'][i],4)),\\\n",
    "                            'Std connections': 'N/A as only one file uploaded',\\\n",
    "                            'CV connections': 'N/A as only one file uploaded'})\n",
    "        all_nodes.update({all_data_dict[0]['node'][i]: current})\n",
    "\n",
    "eng_data_all.update({'Node Info': {'Nb of files': nb_files, 'Nb of nodes': nb_nodes, 'Measurements': all_nodes}}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next cell such that the upload button appears. Then click on the button and select the BAGNAME_duckiebot_constant.json file that you should find within the folder data/BenchmarkXY/json. Make sure that after selecting the file, you click into the cell below the upload button before continuing to run.\n",
    "\n",
    "Please only select one file. It does not matter out of which experiment as this data is static and does not change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constants = FileUpload(accept='.json',\n",
    "    multiple=False)\n",
    "constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you click into the cell below after you selected the correct .json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert constants.data, 'File missing, please upload in above cell'\n",
    "data = json.loads(constants.data[0].decode('utf-8'))\n",
    "\n",
    "\n",
    "# Extracts all the constances name and value\n",
    "# Constances are the values set on the duckiebot as example the gain, trim etc.\n",
    "# These values don't change over the tests run for the same benchmark (hence static) and are reported in the final\n",
    "# benchmark report.\n",
    "for cnst_name in data:\n",
    "    dict_total_cnst[cnst_name] = data[cnst_name]\n",
    "    \n",
    "# Constances added to the static_things dictionary\n",
    "static_things.update({'Constants': dict_total_cnst})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not diag_toolbox_data:\n",
    "    print(\"saving the data as apparently there is no data available from the diagnostic toolbox\")\n",
    "    save_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can stop running this notebook if you do not have any data from the diagnostic toolbox colected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostic toolbox analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next cell such that the upload button appears. Then click on the button and select the .json file you downloaded from the dashboard webside (created by the diagnostic toolbox) and placed within the folder data/BenchmarkXY/json. Make sure that after selecting the file, you click into the cell below the upload button before continuing to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload = FileUpload(accept='.json',\n",
    "    multiple=True)\n",
    "upload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you click into the cell below after you selected the correct .json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "assert upload.data, 'File missing, please upload in above cell'\n",
    "\n",
    "all_data_dict = {}\n",
    "dict_total = {}\n",
    "overall_cpu = []\n",
    "overall_swap = []\n",
    "overall_memory = []\n",
    "\n",
    "#checks how many files that were uploaded\n",
    "nb_files = len(upload.value)\n",
    "\n",
    "for i in range(0, len(upload.value)):\n",
    "    data = json.loads(upload.data[i].decode('utf-8'))\n",
    "    all_data_dict.update({i: data})\n",
    "    total_current = {}\n",
    "    meas_name = ['Memory', 'Swap', 'CPU']\n",
    "    bm_data = np.array([[],[],[],[]])\n",
    "    t0= data['resources_stats'][0]['time']\n",
    "    all_data = []\n",
    "    # Extracts the information about the overall Memory, Swap and CPU usage over time\n",
    "    for j,meas in enumerate(data['resources_stats']):\n",
    "        # Only saves/analyses the data that is collected before the crash occured (if a crash occured)\n",
    "        if float(meas['time'])-float(t0) > float(runtime[i]):\n",
    "            last_indx = j-1\n",
    "            dat = np.array([[meas['time']-t0, meas['memory']['used']/meas['memory']['total']*100, \\\n",
    "                             meas['swap']['used']/meas['swap']['total']*100, meas['cpu']['pcpu']]])\n",
    "            tot_mem_max = meas['memory']['total']\n",
    "            tot_swap_max = meas['swap']['total']\n",
    "            all_data.append(dat.T)\n",
    "            bm_data = np.append(bm_data, dat.T, axis=1)\n",
    "            break\n",
    "        dat = np.array([[meas['time']-t0, meas['memory']['used']/meas['memory']['total']*100, \\\n",
    "                         meas['swap']['used']/meas['swap']['total']*100, meas['cpu']['pcpu']]])\n",
    "        tot_mem_max = meas['memory']['total']\n",
    "        tot_swap_max = meas['swap']['total']\n",
    "        all_data.append(dat.T)\n",
    "        bm_data = np.append(bm_data, dat.T, axis=1)\n",
    "\n",
    "    cpu_total = bm_data[3]\n",
    "    swap_total = bm_data[2]\n",
    "    memory_total = bm_data[1]\n",
    "    overall_cpu.append(bm_data[3])\n",
    "    overall_swap.append(bm_data[2])\n",
    "    overall_memory.append(bm_data[1])\n",
    "\n",
    "    total_cpu_mean = np.mean(cpu_total)\n",
    "    total_cpu_median = np.median(cpu_total)\n",
    "    total_cpu_std = np.std(cpu_total)\n",
    "    if total_cpu_mean != 0:\n",
    "        total_cpu_cv = total_cpu_std/total_cpu_mean\n",
    "    else:\n",
    "        if total_cpu_std == 0:\n",
    "            total_cpu_cv = 0\n",
    "        else:\n",
    "            total_cpu_cv = 10\n",
    "    total_cpu_cv = total_cpu_std/total_cpu_mean\n",
    "    total_swap_mean = np.mean(swap_total)\n",
    "    total_swap_median = np.median(swap_total)\n",
    "    total_swap_std = np.std(swap_total)\n",
    "    if total_swap_mean != 0:\n",
    "        total_swap_cv = total_swap_std/total_swap_mean\n",
    "    else:\n",
    "        if total_swap_std == 0:\n",
    "            total_swap_cv = 0\n",
    "        else:\n",
    "            total_swap_cv = 10\n",
    "    total_memory_mean = np.mean(memory_total)\n",
    "    total_memory_median = np.median(memory_total)\n",
    "    total_memory_std = np.std(memory_total)\n",
    "    if total_memory_mean != 0:\n",
    "        total_memory_cv = total_memory_std/total_memory_mean\n",
    "    else:\n",
    "        if total_memory_std == 0:\n",
    "            total_memory_cv = 0\n",
    "        else:\n",
    "            total_memory_cv = 10\n",
    "    \n",
    "\n",
    "    total_current = {'CPU (CPU used in %)': {'Mean' : float(np.round(total_cpu_mean,4)), 'Median' : float(np.round(total_cpu_median,4)),\\\n",
    "                           'Standard Deviation' : float(np.round(total_cpu_std,4)), 'coefficient of variation': float(np.round(total_cpu_cv,4))}, \\\n",
    "                   'Swaps': {'Mean' : float(np.round(total_swap_mean,4)), 'Median' : float(np.round(total_swap_median,4)),\\\n",
    "                           'Standard Deviation' : float(np.round(total_swap_std,4)), 'coefficient of variation': float(np.round(total_swap_cv,4)),\\\n",
    "                            'Swaps max': float(np.round(tot_swap_max,4))}, \\\n",
    "                   'Mem (memory used in %)': {'Mean' : float(np.round(total_memory_mean,4)), 'Median' : float(np.round(total_memory_median,4)),\\\n",
    "                           'Standard Deviation' : float(np.round(total_memory_std,4)), 'coefficient of variation': \\\n",
    "                           float(np.round(total_memory_cv,4)), 'Memory Max (bytes)': float(np.round(tot_mem_max,4))}}\n",
    "\n",
    "    dict_total.update({'Engineering data of file ' + str(i): total_current})\n",
    "    \n",
    "    \n",
    "    #Below there is the code that plots all the data so you can have a look at the behaviour of the total CPU, \n",
    "    #NThreads and Memory over the time in which the diagnostic toolbox (and therefore the Benchmark) was running. \n",
    "    #If you run the cell, you will see a plot an interpolated line of the total CPU usage, NThreads and Memory \n",
    "    #usage.\n",
    "    time_ip = np.linspace(bm_data[0][0], bm_data[0][-1], 100)\n",
    "    bm_ip = np.array([time_ip])\n",
    "    fig, axes= plt.subplots(3, 1, figsize=(9, 8))\n",
    "    fig.text(0.5, 0.04, 'time', ha='center', va='center')\n",
    "    fig.text(0.03, 0.5, 'Performance', ha='center', va='center', rotation='vertical')\n",
    "    \n",
    "    for k in range(len(bm_data)-1):\n",
    "        tck = interpolate.splrep(bm_data[0], bm_data[k+1], s=0)\n",
    "\n",
    "        ip = np.array([interpolate.splev(bm_ip[0], tck, der=0)])\n",
    "        bm_ip = np.append(bm_ip, ip, axis=0)\n",
    "\n",
    "        axes[k].plot(bm_data[0], bm_data[k+1], bm_ip[0], bm_ip[k+1])\n",
    "        axes[k].legend(['Measurement', 'IP Measurement'])\n",
    "        axes[k].set_title(meas_name[k])\n",
    "        axes[k].set_ylim(0, 100)\n",
    "        axes[k].set_ylabel('%')\n",
    "\n",
    "        fig.suptitle('Diagnostics', fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "current = {}\n",
    "if nb_files > 1:\n",
    "    #if more than one file is uploaded we calculate the overall mean,stv and cv\n",
    "    all_cpu = []\n",
    "    for i in range(0,nb_files):\n",
    "        for j in range (0, len(overall_cpu[i])):\n",
    "            all_cpu.append(overall_cpu[i][j])\n",
    "    overall_cpu_mean = np.mean(all_cpu)\n",
    "    overall_cpu_median = np.median(all_cpu)\n",
    "    overall_cpu_std = np.std(all_cpu)\n",
    "    if overall_cpu_mean != 0:\n",
    "        overall_cpu_cv = overall_cpu_std/overall_cpu_mean\n",
    "    else:\n",
    "        if overall_cpu_std == 0:\n",
    "            overall_cpu_cv = 0\n",
    "        else:\n",
    "            overall_cpu_cv = 10\n",
    "    all_swap = []\n",
    "    for i in range(0,nb_files):\n",
    "        for j in range (0, len(overall_swap[i])):\n",
    "            all_swap.append(overall_swap[i][j])\n",
    "    overall_swap_mean = np.mean(all_swap)\n",
    "    overall_swap_median = np.median(all_swap)\n",
    "    overall_swap_std = np.std(all_swap)\n",
    "    if overall_swap_mean != 0:\n",
    "        overall_swap_cv = overall_swap_std/overall_swap_mean\n",
    "    else:\n",
    "        if overall_swap_std == 0:\n",
    "            overall_swap_cv = 0\n",
    "        else:\n",
    "            overall_swap_cv = 10\n",
    "    all_mem = []\n",
    "    for i in range(0,nb_files):\n",
    "        for j in range (0, len(overall_memory[i])):\n",
    "            all_mem.append(overall_memory[i][j])\n",
    "    overall_memory_mean = np.mean(all_mem)\n",
    "    overall_memory_median = np.median(all_mem)\n",
    "    overall_memory_std = np.std(all_mem)\n",
    "    if overall_memory_mean != 0:\n",
    "        overall_memory_cv = overall_memory_std/overall_memory_mean\n",
    "    else:\n",
    "        if overall_memory_std == 0:\n",
    "            overall_memory_cv = 0\n",
    "        else:\n",
    "            overall_memory_cv = 10\n",
    "\n",
    "    current = {'Overall CPU (CPU used in %)': {'Mean' : float(np.round(overall_cpu_mean,4)), \\\n",
    "                                                     'Median' : float(np.round(overall_cpu_median,4)),\\\n",
    "                                                     'Standard Deviation' : float(np.round(overall_cpu_std,4)),\\\n",
    "                                                     'coefficient of variation': float(np.round(overall_cpu_cv,4))},\\\n",
    "                   'Overall Swaps (Swaps used in %)': {'Mean' : float(np.round(overall_swap_mean,4)),\\\n",
    "                                     'Median' : float(np.round(overall_swap_median,4)),\\\n",
    "                                     'Standard Deviation' : float(np.round(overall_swap_std,4)),\\\n",
    "                                     'coefficient of variation': float(np.round(overall_swap_cv,4)),\\\n",
    "                                     'Swaps max': float(np.round(tot_swap_max,4))},\\\n",
    "                   'Overall Mem (memory used in %)': {'Mean' : float(np.round(overall_memory_mean,4)),\\\n",
    "                                                      'Median' : float(np.round(overall_memory_median,4)),\\\n",
    "                                                      'Standard Deviation' : float(np.round(overall_memory_std,4)),\\\n",
    "                                                      'coefficient of variation':float(np.round(overall_memory_cv,4)),\\\n",
    "                                                      'Memory Max (bytes)': float(np.round(tot_mem_max,4))}}\n",
    "\n",
    "\n",
    "    dict_total.update({'Overall engineering data container': current})\n",
    "    \n",
    "else:\n",
    "    #if only one file was uploaded this one measurement is taken as mean\n",
    "    overall_cpu_mean = np.mean(overall_cpu)\n",
    "    overall_cpu_median = np.median(overall_cpu)\n",
    "    overall_cpu_std = 'N/A as only one file uploaded'\n",
    "    overall_cpu_cv = 'N/A as only one file uploaded'\n",
    "    overall_swap_mean = np.mean(overall_swap)\n",
    "    overall_swap_median = np.median(overall_swap)\n",
    "    overall_swap_std = 'N/A as only one file uploaded'\n",
    "    overall_swap_cv = 'N/A as only one file uploaded'\n",
    "    overall_memory_mean = np.mean(overall_memory)\n",
    "    overall_memory_median = np.median(overall_memory)\n",
    "    overall_memory_std = 'N/A as only one file uploaded'\n",
    "    overall_memory_cv = 'N/A as only one file uploaded'\n",
    "\n",
    "    current = {'Overall CPU (CPU used in %)': {'Mean' : float(np.round(overall_cpu_mean,4)),\\\n",
    "                                                     'Median' : float(np.round(overall_cpu_median,4)),\\\n",
    "                                                     'Standard Deviation' : overall_cpu_std,\\\n",
    "                                                     'coefficient of variation': overall_cpu_cv},\\\n",
    "                   'Overall Swaps': {'Mean' : float(np.round(overall_swap_mean,4)),\\\n",
    "                                     'Median' : float(np.round(overall_swap_median,4)),\\\n",
    "                                     'Standard Deviation' : overall_swap_std,\\\n",
    "                                     'coefficient of variation': overall_swap_cv,\\\n",
    "                                     'Swaps max': float(np.round(tot_swap_max,4))},\\\n",
    "                   'Overall Mem (memory used in %)': {'Mean' : float(np.round(overall_memory_mean,4)),\\\n",
    "                                                      'Median' : float(np.round(overall_memory_median,4)),\\\n",
    "                                                      'Standard Deviation' : overall_memory_std,\\\n",
    "                                                      'coefficient of variation':overall_memory_cv,\\\n",
    "                                                      'Memory Max (bytes)': float(np.round(tot_mem_max,4))}}\n",
    "    dict_total.update({'Overall engineering data container': current})\n",
    "\n",
    "# Add all the data to the eng_data_all dictionary\n",
    "eng_data_all.update({'Total engineering data container': {'Nb of files': nb_files, 'Measurements': dict_total}})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please change the variable `dt_core_name` below to the name, under which you ran the dt-core\n",
    "If you followed carefully the instructions, this should be: 'behaviour_benchmarking'.\n",
    "If you ran the normal lane_following demo you should change the variable to 'lane_following'.\n",
    "\n",
    "(If you have not given a name, it will be a random name generated by docker, please run the cell below to see all the differen names, the one with the very random name will be it as all other containers should have meaningful and proper names.\n",
    "If the name of the container is different for each experiment, just use the list version of the variable called `dt_core_name_list`, note that this list should have as many entries as files uploaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(upload.value)):\n",
    "    data = json.loads(upload.data[i].decode('utf-8'))\n",
    "    container_id = data['containers']\n",
    "    for x in container_id:\n",
    "        print(container_id[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if the name for dt-core is the same for all experiments, please change 'dt_core_name' below and comment out the\n",
    "#line at the very end of this cell\n",
    "#uncoment line below if all have same name otw comment it out\n",
    "dt_core_name = 'behaviour_benchmarking'\n",
    "\n",
    "#make a list out of it with the length of files uploaded\n",
    "#uncoment lines below if all have same name otw comment it out\n",
    "dt_core_name_list=[]\n",
    "for i in range(0, len(upload.value)):\n",
    "    dt_core_name_list.append(dt_core_name)\n",
    "\n",
    "#if the name for dt-core is different in each experiment, please comment the lines above and uncoment the line below\n",
    "#note the list below should have as many entries as you uploaded dashboard files\n",
    "# dt_core_name_list = ['','','','','']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_dict = {}\n",
    "dict_total = {}\n",
    "overall_cpu_processes_fl = {}\n",
    "overall_nthreads_fl = {}\n",
    "overall_mem_fl = {}\n",
    "\n",
    "for i in range(0, len(upload.value)):\n",
    "    data = json.loads(upload.data[i].decode('utf-8'))\n",
    "\n",
    "    cpu_processes = np.array([[],[],[]])\n",
    "    nthreads_processes = np.array([[],[],[]])\n",
    "    memory = np.array([[],[],[]])\n",
    "\n",
    "    t0= data['process_stats'][0]['time']\n",
    "    memory_max = float(np.round(data['resources_stats'][0]['memory']['total'],4))\n",
    "\n",
    "    container_id = data['containers']\n",
    "\n",
    "    # At the moment the containers considered are:\n",
    "    # 'behaviour_benchmarking', ('dts-run-diagnostics-system-monitor'), 'acquisition-bridge', 'demo_all_drivers', \n",
    "    # 'demo_all'\n",
    "\n",
    "    # dictionary where all container names and their corresponding keys are safed\n",
    "    all_keys = {}\n",
    "\n",
    "    # saves the key corresponding to the containers considered\n",
    "    for x in container_id:\n",
    "        if container_id[x] == dt_core_name_list[i]:\n",
    "            behaviour_benchmarking_key = x\n",
    "            all_keys.update({container_id[x]: x})\n",
    "        if container_id[x] == 'dts-run-diagnostics-system-monitor': \n",
    "            diagnostics_system_monitor_key = x\n",
    "            all_keys.update({container_id[x]: x})\n",
    "        if container_id[x] == 'acquisition-bridge': \n",
    "            acquisition_bridge_key = x\n",
    "            all_keys.update({container_id[x]: x})\n",
    "        if container_id[x] == 'demo_all_drivers': \n",
    "            demo_all_drivers_key = x\n",
    "            all_keys.update({container_id[x]: x})\n",
    "        if container_id[x] == 'demo_all': \n",
    "            demo_all_key = x\n",
    "            all_keys.update({container_id[x]: x})\n",
    "\n",
    "    process_stats = data['process_stats']\n",
    "    container_stats = data['container_stats']\n",
    "\n",
    "    # saves the cpu usage in % (compared to the total available cpu), the number of threads as well as the memory \n",
    "    # used in % (compared to the total available memory) of the single containers \n",
    "    for x in process_stats:\n",
    "        if x['container'] == behaviour_benchmarking_key:\n",
    "            dat = np.array([[x['time']-t0,float(x['pcpu']),x['command']]])\n",
    "            cpu_processes = np.append(cpu_processes,dat.T, axis=1)\n",
    "            dat = np.array([[x['time']-t0,float(x['nthreads']),x['command']]])\n",
    "            nthreads_processes = np.append(nthreads_processes,dat.T, axis=1)\n",
    "            #place pmem instead of mem if wanted\n",
    "            dat = np.array([[x['time']-t0,float(x['pmem']),x['command']]])\n",
    "            memory = np.append(memory,dat.T, axis=1)\n",
    "\n",
    "    command = cpu_processes[2]\n",
    "    container_names = []\n",
    "\n",
    "\n",
    "    length = cpu_processes.shape[1]\n",
    "    occurrences = np.count_nonzero(cpu_processes == cpu_processes[0][2])\n",
    "    # Commented out as same number of occurrences as cpu\n",
    "    # occurrences_nthreads = np.count_nonzero(nthreads_processes == nthreads_processes[0][2])\n",
    "    time_ip = np.linspace(float(cpu_processes[0][0]), float(cpu_processes[0][-1]), 100)\n",
    "    bm_ip = np.array([time_ip])\n",
    "\n",
    "    # dictionary that sumerizes the cpu, nthreads and memory used by the different containers\n",
    "    summary = {'Node': {'Engineering Data Performance': 'Value'}}\n",
    "\n",
    "    for j in range(0,occurrences-1):\n",
    "        pos = np.char.find(command[j],hostname_minus)\n",
    "        if pos == -1:\n",
    "            pos = np.char.find(command[j],hostname_underline)\n",
    "        # splits the found values (cpu, nthreads and memory) into lists of the different containers                  \n",
    "        current_container_name = command[j][pos:]\n",
    "        container_names.append(current_container_name)\n",
    "        cpu_processes_int = cpu_processes[:,j:length-1:occurrences]\n",
    "        nthreads_processes_int = nthreads_processes[:,j:length-1:occurrences]\n",
    "        mem_processes_int = memory[:,j:length-1:occurrences]\n",
    "\n",
    "        # calculates the mean, median and std of the cpu, nthreads and memory used by the different containers\n",
    "        # Also the cv is calculated, this is the coefficient of variation calculation:\n",
    "        # if lower than 1, the std can be considered small enough and one can stop running tests\n",
    "        # https://www.researchgate.net/post/What_do_you_consider_a_good_standard_deviation\n",
    "        cpu_processes_fl = cpu_processes_int[1].astype(np.float)\n",
    "        if i == 0:\n",
    "            overall_cpu_processes_fl[j] = cpu_processes_fl\n",
    "        else:\n",
    "            overall_cpu_processes_fl[j] = np.append(overall_cpu_processes_fl[j],cpu_processes_fl)\n",
    "        cpu_processes_mean = np.mean(cpu_processes_fl)\n",
    "        cpu_processes_median = np.median(cpu_processes_fl)\n",
    "        cpu_processes_std = np.std(cpu_processes_fl)\n",
    "        if cpu_processes_mean != 0:\n",
    "            cpu_cv = float(np.round(cpu_processes_std/cpu_processes_mean,4))\n",
    "        else:\n",
    "            if cpu_processes_std == 0:\n",
    "                cpu_cv = 0\n",
    "            else:\n",
    "                cpu_cv = 10\n",
    "        nthreads_fl = nthreads_processes_int[1].astype(np.float)\n",
    "        if i == 0:\n",
    "            overall_nthreads_fl[j] = nthreads_fl\n",
    "        else:\n",
    "            overall_nthreads_fl[j] = np.append(overall_nthreads_fl[j],nthreads_fl)\n",
    "        nthreads_mean = np.mean(nthreads_fl)\n",
    "        nthreads_median = np.median(nthreads_fl)\n",
    "        nthreads_std = np.std(nthreads_fl)\n",
    "        if nthreads_mean != 0:\n",
    "            nthreads_cv = float(np.round(nthreads_std/nthreads_mean,4))\n",
    "        else:\n",
    "            if nthreads_std == 0:\n",
    "                nthreads_cv = 0\n",
    "            else:\n",
    "                nthreads_cv = 10\n",
    "        mem_fl = mem_processes_int[1].astype(np.float)\n",
    "        if i == 0:\n",
    "            overall_mem_fl[j] = mem_fl\n",
    "        else:\n",
    "            overall_mem_fl[j] = np.append(overall_mem_fl[j],mem_fl)\n",
    "        mem_mean = np.mean(mem_fl)\n",
    "        mem_median = np.median(mem_fl)\n",
    "        mem_std = np.std(mem_fl)\n",
    "        if mem_mean != 0:\n",
    "            mem_cv = float(np.round(mem_std/mem_mean,4))\n",
    "        else:\n",
    "            if mem_std == 0:\n",
    "                mem_cv = 0\n",
    "            else:\n",
    "                mem_cv = 10\n",
    "\n",
    "        # save the data in the dictionary \n",
    "        current = {'CPU (CPU used in %)': {'Mean' : float(np.round(cpu_processes_mean,4)), 'Median' : float(np.round(cpu_processes_median,4)),\\\n",
    "                           'Standard Deviation' : float(np.round(cpu_processes_std,4)), 'coefficient of variation': cpu_cv}, \\\n",
    "                   'NThreads': {'Mean' : float(np.round(nthreads_mean,4)), 'Median' : float(np.round(nthreads_median,4)),\\\n",
    "                           'Standard Deviation' : float(np.round(nthreads_std,4)), 'coefficient of variation': nthreads_cv}, \\\n",
    "                   'PMem (memory used in %)': {'Mean' : float(np.round(mem_mean,4)), 'Median' : float(np.round(mem_median,4)),\\\n",
    "                           'Standard Deviation' : float(np.round(mem_std,4)), 'coefficient of variation': mem_cv, \\\n",
    "                           'Memory Max (bytes)': float(np.round(memory_max,4))}}\n",
    "\n",
    "        summary.update( {current_container_name: current} )\n",
    "\n",
    "    # Add all the data to the eng_data_all dictionary\n",
    "    dict_total.update({'Engineering data node of file ' + str(i): summary})\n",
    "\n",
    "node_cur = {}\n",
    "\n",
    "if nb_files > 1:\n",
    "    #if there was more then one file uploaded the mean, std and cv over all the data is calculated\n",
    "    for j in range(0,occurrences-1):\n",
    "        cpu_processes_mean = np.mean(overall_cpu_processes_fl[j])\n",
    "        cpu_processes_median = np.median(overall_cpu_processes_fl[j])\n",
    "        cpu_processes_std = np.std(overall_cpu_processes_fl[j])\n",
    "        cpu_cv = float(np.round(cpu_processes_std/cpu_processes_mean,4))\n",
    "        nthreads_mean = np.mean(overall_nthreads_fl[j])\n",
    "        nthreads_median = np.median(overall_nthreads_fl[j])\n",
    "        nthreads_std = np.std(overall_nthreads_fl[j])\n",
    "        nthreads_cv = float(np.round(nthreads_std/nthreads_mean,4))\n",
    "        mem_mean = np.mean(overall_mem_fl[j])\n",
    "        mem_median = np.median(overall_mem_fl[j])\n",
    "        mem_std = np.std(overall_mem_fl[j])\n",
    "        mem_cv = float(np.round(mem_std/mem_mean,4))\n",
    "        current = {'Overall CPU (CPU used in %)': {'Mean' : float(np.round(cpu_processes_mean,4)), 'Median' : float(np.round(cpu_processes_median,4)),\\\n",
    "                           'Standard Deviation' : float(np.round(cpu_processes_std,4)), 'coefficient of variation': float(np.round(cpu_cv,4))}, \\\n",
    "                   'Overall NThreads': {'Mean' : float(np.round(nthreads_mean,4)), 'Median' : float(np.round(nthreads_median,4)),\\\n",
    "                           'Standard Deviation' : float(np.round(nthreads_std,4)), 'coefficient of variation': float(np.round(nthreads_cv,4))}, \\\n",
    "                   'Overall PMem (memory used in %)': {'Mean' : float(np.round(mem_mean,4)), 'Median' : float(np.round(mem_median,4)),\\\n",
    "                           'Standard Deviation' : float(np.round(mem_std,4)), 'coefficient of variation': float(np.round(mem_cv,4)), \\\n",
    "                           'Memory Max (bytes)': float(np.round(memory_max,4))}}\n",
    "        \n",
    "        node_cur.update({container_names[j]: current})\n",
    "        \n",
    "    \n",
    "else:\n",
    "    #if only one file was uploaded the only measurement is taken as mean \n",
    "    for j in range(0,occurrences-1):\n",
    "        cpu_processes_mean = np.mean(overall_cpu_processes_fl[j])\n",
    "        cpu_processes_median = np.median(overall_cpu_processes_fl[j])\n",
    "        cpu_processes_std = 'N/A as only one file uploaded'\n",
    "        cpu_cv = 'N/A as only one file uploaded'\n",
    "        nthreads_mean = np.mean(overall_nthreads_fl[j])\n",
    "        nthreads_median = np.median(overall_nthreads_fl[j])\n",
    "        nthreads_std = 'N/A as only one file uploaded'\n",
    "        nthreads_cv = 'N/A as only one file uploaded'\n",
    "        mem_mean = np.mean(overall_mem_fl[j])\n",
    "        mem_median = np.median(overall_mem_fl[j])\n",
    "        mem_std = 'N/A as only one file uploaded'\n",
    "        mem_cv = 'N/A as only one file uploaded'\n",
    "        \n",
    "        current = {'Overall CPU (CPU used in %)': {'Mean' : float(np.round(cpu_processes_mean,4)), 'Median' : float(np.round(cpu_processes_median,4)),\\\n",
    "                           'Standard Deviation' : cpu_processes_std, 'coefficient of variation': cpu_cv}, \\\n",
    "                   'Overall NThreads': {'Mean' : float(np.round(nthreads_mean,4)), 'Median' : float(np.round(nthreads_median,4)),\\\n",
    "                           'Standard Deviation' : nthreads_std, 'coefficient of variation': nthreads_cv}, \\\n",
    "                   'Overall PMem (memory used in %)': {'Mean' : float(np.round(mem_mean,4)), 'Median' : float(np.round(mem_median,4)),\\\n",
    "                           'Standard Deviation' : mem_std, 'coefficient of variation': mem_cv, \\\n",
    "                           'Memory Max (bytes)': float(np.round(memory_max,4))}}\n",
    "        \n",
    "\n",
    "        node_cur.update({container_names[j]: current})\n",
    "\n",
    "dict_total.update({'Overall engineering data node': node_cur})\n",
    "eng_data_all.update({'Total engineering data node': {'Nb of files': nb_files, 'Measurements': dict_total}})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container_info = {'Container Name': {'Type': 'Information'}}\n",
    "\n",
    "# Dictionary which will hold all the \"static\" information about the software\n",
    "# This means, the container names, their tags, version etc (see below) and also the duckiebot name of the duckiebot\n",
    "# used for this benchmark\n",
    "# This will be used in the final benchmark report\n",
    "\n",
    "\n",
    "for i, cur_key in enumerate(all_keys):\n",
    "\n",
    "    # Not very usefull as always different\n",
    "    # name = data['container_config'][demo_all_drivers_key]['Args'][1]\n",
    "    image_name = data['container_config'][all_keys[cur_key]]['Config']['Image']\n",
    "    image_tag = data['container_config'][all_keys[cur_key]]['Image']\n",
    "    cont_name = data['container_config'][all_keys[cur_key]]['Name']\n",
    "    autobot_name = data['container_config'][all_keys[cur_key]]['Config']['Hostname']\n",
    "    dt_label_arch = data['container_config'][all_keys[cur_key]]['Config']['Labels']['org.duckietown.label.architecture']\n",
    "    dt_label_base_img = data['container_config'][all_keys[cur_key]]['Config']['Labels']['org.duckietown.label.base.image']\n",
    "    dt_code_branch = data['container_config'][all_keys[cur_key]]['Config']['Labels']['org.duckietown.label.code.branch']\n",
    "    dt_code_repository = data['container_config'][all_keys[cur_key]]['Config']['Labels']['org.duckietown.label.code.repository']\n",
    "    dt_code_version_major = data['container_config'][all_keys[cur_key]]['Config']['Labels']['org.duckietown.label.code.version.major']\n",
    "    dt_module_type = data['container_config'][all_keys[cur_key]]['Config']['Labels']['org.duckietown.label.module.type']\n",
    "    dt_template_name = data['container_config'][all_keys[cur_key]]['Config']['Labels']['org.duckietown.label.template.name']\n",
    "\n",
    "    curr = {'Image Name': image_name, 'Image Tag': image_tag, 'Image Name and Tag': image_name + '@' + image_tag, \\\n",
    "            'Container Name': cont_name, 'Autobot Name': autobot_name, 'DT label architecture': dt_label_arch, \\\n",
    "            'DT label base image': dt_label_base_img, 'DT label code branch': dt_code_branch,\\\n",
    "            'DT label code repository': dt_code_repository, 'DT label code version major': dt_code_version_major,\\\n",
    "            'DT label module type': dt_module_type, 'DT label template name': dt_template_name}\n",
    "    \n",
    "    container_info.update({cur_key: curr})\n",
    "\n",
    "    \n",
    "static_things.update({'Dashboard Info': container_info})\n",
    "\n",
    "save_data() \n",
    "\n",
    "dashboard_info = path.join(outdir, name + '_dashboard_info.yaml')\n",
    "\n",
    "    \n",
    "with open(dashboard_info, 'w') as yaml_file:\n",
    "    yaml.dump(container_info, yaml_file, default_flow_style=False)\n",
    "# print((data['container_config'][diagnostics_system_monitor_key]['Config']['Labels']['org.duckietown.label.base.image']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below there is a cell you can use to have a look at the behaviour of the CPU, NThreads and Memory of all the seperate containers over the time in which the diagnostic toolbox (and therefore the Benchmark) was running. If you run the cell, you will see a plot an interpolated line of CPU usage, NThreads and Memory usage for each container of the last file you uploaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes= plt.subplots(occurrences, 1, figsize=(15, 40))\n",
    "fig.text(0.5, 0.04, 'time', ha='center', va='center')\n",
    "fig.text(0.03, 0.5, 'Performance', ha='center', va='center', rotation='vertical')\n",
    "\n",
    "# Plots of the CPU usage behaviour over time for all containers\n",
    "for i in range(0,occurrences-1):\n",
    "    pos = np.char.find(command[i],hostname_minus)\n",
    "    if pos == -1:\n",
    "        pos = np.char.find(command[i],hostname_underline)\n",
    "                       \n",
    "    current_container_name = command[i][pos:]\n",
    "    container_names.append(current_container_name)\n",
    "    cpu_processes_int = cpu_processes[:,i:length-1:occurrences]\n",
    "    \n",
    "    for j in range(0,len(cpu_processes)-2):\n",
    "        tck = interpolate.splrep((cpu_processes_int[0]), (cpu_processes_int[j+1]), s=0)\n",
    "        ip = np.array([interpolate.splev(bm_ip[0], tck, der=0)])\n",
    "        time_ip = np.linspace(float(cpu_processes_int[0][0]), float(cpu_processes_int[0][-1]), 100)\n",
    "        cpu_processes_int_x = np.array([time_ip])\n",
    "        value_ip = np.linspace(float(cpu_processes_int[1][0]), float(cpu_processes_int[1][-1]), 100)\n",
    "        cpu_processes_int_y = np.array([value_ip])\n",
    "        \n",
    "        \n",
    "        axes[i].plot((cpu_processes_int_x[0]), cpu_processes_int_y[0])\n",
    "        axes[i].legend(['Measurement', 'IP Measurement'])\n",
    "        axes[i].set_title(current_container_name)\n",
    "        axes[i].set_ylabel('%')\n",
    "         # ToDo add units to axes\n",
    "    \n",
    "fig.suptitle('pcpu', fontsize=16)\n",
    "\n",
    "\n",
    "fig, axes= plt.subplots(occurrences, 1, figsize=(15, 40))\n",
    "fig.text(0.5, 0.04, 'time', ha='center', va='center')\n",
    "fig.text(0.03, 0.5, 'Performance', ha='center', va='center', rotation='vertical')\n",
    "\n",
    "# Plots of the nthreads used over time for all containers\n",
    "for i in range(0,occurrences-1):\n",
    "    nthreads_processes_int = nthreads_processes[:,i:length-1:occurrences]\n",
    "    for j in range(0,len(cpu_processes)-2):\n",
    "        tck = interpolate.splrep((nthreads_processes_int[0]), (nthreads_processes_int[j+1]), s=0)\n",
    "        ip = np.array([interpolate.splev(bm_ip[0], tck, der=0)])\n",
    "        time_ip = np.linspace(float(nthreads_processes_int[0][0]), float(nthreads_processes_int[0][-1]), 100)\n",
    "        nthreads_processes_int_x = np.array([time_ip])\n",
    "        value_ip = np.linspace(float(nthreads_processes_int[1][0]), float(nthreads_processes_int[1][-1]), 100)\n",
    "        nthreads_processes_int_y = np.array([value_ip])\n",
    "        \n",
    "        \n",
    "        axes[i].plot((nthreads_processes_int_x[0]), nthreads_processes_int_y[0])\n",
    "        axes[i].legend(['Measurement', 'IP Measurement'])\n",
    "        axes[i].set_title('Nthreads')\n",
    "        axes[i].set_ylabel('amount')\n",
    "         # ToDo add units to axes\n",
    "    \n",
    "fig.suptitle('nthreads', fontsize=16)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "fig, axes= plt.subplots(occurrences, 1, figsize=(15, 40))\n",
    "fig.text(0.5, 0.04, 'time', ha='center', va='center')\n",
    "fig.text(0.03, 0.5, 'Performance', ha='center', va='center', rotation='vertical')\n",
    "\n",
    "# Plots of the Memory usage behaviour over time for all containers\n",
    "for i in range(0,occurrences-1):\n",
    "    mem_processes_int = memory[:,i:length-1:occurrences]\n",
    "    for j in range(0,len(cpu_processes)-2):\n",
    "        tck = interpolate.splrep((mem_processes_int[0]), (mem_processes_int[j+1]), s=0)\n",
    "        ip = np.array([interpolate.splev(bm_ip[0], tck, der=0)])\n",
    "        time_ip = np.linspace(float(mem_processes_int[0][0]), float(mem_processes_int[0][-1]), 100)\n",
    "        mem_processes_int_x = np.array([time_ip])\n",
    "        value_ip = np.linspace(float(mem_processes_int[1][0]), float(mem_processes_int[1][-1]), 100)\n",
    "        mem_processes_int_y = np.array([value_ip])\n",
    "        \n",
    "        \n",
    "        axes[i].plot((mem_processes_int_x[0]), mem_processes_int_y[0])\n",
    "        axes[i].legend(['Measurement', 'IP Measurement'])\n",
    "        axes[i].set_title('mem')\n",
    "        axes[i].set_ylabel('%')\n",
    "        # ToDo add units to axes\n",
    "    \n",
    "fig.suptitle('mem', fontsize=16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
